{
  "best_global_step": 6000,
  "best_metric": 0.8616991836169918,
  "best_model_checkpoint": "output/dictabert_hebnli/checkpoint-6000",
  "epoch": 20.409468969929623,
  "eval_steps": 500,
  "global_step": 6000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.06824482832160375,
      "grad_norm": 5.46275520324707,
      "learning_rate": 2.3750000000000003e-07,
      "loss": 1.0752,
      "step": 20
    },
    {
      "epoch": 0.1364896566432075,
      "grad_norm": 5.280899524688721,
      "learning_rate": 4.875000000000001e-07,
      "loss": 1.0576,
      "step": 40
    },
    {
      "epoch": 0.20473448496481125,
      "grad_norm": 4.561397075653076,
      "learning_rate": 7.375e-07,
      "loss": 1.0133,
      "step": 60
    },
    {
      "epoch": 0.272979313286415,
      "grad_norm": 3.426286220550537,
      "learning_rate": 9.875e-07,
      "loss": 0.9272,
      "step": 80
    },
    {
      "epoch": 0.3412241416080188,
      "grad_norm": 2.0848183631896973,
      "learning_rate": 1.2375e-06,
      "loss": 0.818,
      "step": 100
    },
    {
      "epoch": 0.4094689699296225,
      "grad_norm": 0.9918751120567322,
      "learning_rate": 1.4875000000000002e-06,
      "loss": 0.7244,
      "step": 120
    },
    {
      "epoch": 0.4777137982512263,
      "grad_norm": 0.6126210689544678,
      "learning_rate": 1.7375e-06,
      "loss": 0.6771,
      "step": 140
    },
    {
      "epoch": 0.54595862657283,
      "grad_norm": 0.3549734055995941,
      "learning_rate": 1.9875000000000005e-06,
      "loss": 0.6587,
      "step": 160
    },
    {
      "epoch": 0.6142034548944337,
      "grad_norm": 0.32712769508361816,
      "learning_rate": 2.2375e-06,
      "loss": 0.65,
      "step": 180
    },
    {
      "epoch": 0.6824482832160376,
      "grad_norm": 0.42703139781951904,
      "learning_rate": 2.4875000000000003e-06,
      "loss": 0.6416,
      "step": 200
    },
    {
      "epoch": 0.7506931115376413,
      "grad_norm": 0.691750705242157,
      "learning_rate": 2.7375e-06,
      "loss": 0.6049,
      "step": 220
    },
    {
      "epoch": 0.818937939859245,
      "grad_norm": 0.8466253280639648,
      "learning_rate": 2.9875e-06,
      "loss": 0.5463,
      "step": 240
    },
    {
      "epoch": 0.8871827681808488,
      "grad_norm": 0.7401958107948303,
      "learning_rate": 3.2375e-06,
      "loss": 0.5062,
      "step": 260
    },
    {
      "epoch": 0.9554275965024526,
      "grad_norm": 0.830376148223877,
      "learning_rate": 3.4875000000000005e-06,
      "loss": 0.4822,
      "step": 280
    },
    {
      "epoch": 1.020473448496481,
      "grad_norm": 1.5260552167892456,
      "learning_rate": 3.7375000000000006e-06,
      "loss": 0.4694,
      "step": 300
    },
    {
      "epoch": 1.088718276818085,
      "grad_norm": 2.7848758697509766,
      "learning_rate": 3.9875e-06,
      "loss": 0.4469,
      "step": 320
    },
    {
      "epoch": 1.1569631051396887,
      "grad_norm": 3.2679901123046875,
      "learning_rate": 4.2375000000000005e-06,
      "loss": 0.4334,
      "step": 340
    },
    {
      "epoch": 1.2252079334612924,
      "grad_norm": 1.5942604541778564,
      "learning_rate": 4.4875e-06,
      "loss": 0.423,
      "step": 360
    },
    {
      "epoch": 1.2934527617828961,
      "grad_norm": 3.7128195762634277,
      "learning_rate": 4.737500000000001e-06,
      "loss": 0.4257,
      "step": 380
    },
    {
      "epoch": 1.3616975901044999,
      "grad_norm": 1.3691977262496948,
      "learning_rate": 4.987500000000001e-06,
      "loss": 0.4075,
      "step": 400
    },
    {
      "epoch": 1.4299424184261036,
      "grad_norm": 1.086165189743042,
      "learning_rate": 5.237500000000001e-06,
      "loss": 0.3936,
      "step": 420
    },
    {
      "epoch": 1.4981872467477073,
      "grad_norm": 1.8368732929229736,
      "learning_rate": 5.4875e-06,
      "loss": 0.3844,
      "step": 440
    },
    {
      "epoch": 1.566432075069311,
      "grad_norm": 3.7879927158355713,
      "learning_rate": 5.7375000000000005e-06,
      "loss": 0.3802,
      "step": 460
    },
    {
      "epoch": 1.6346769033909148,
      "grad_norm": 1.1639909744262695,
      "learning_rate": 5.987500000000001e-06,
      "loss": 0.3804,
      "step": 480
    },
    {
      "epoch": 1.7029217317125187,
      "grad_norm": 1.2495887279510498,
      "learning_rate": 6.237500000000001e-06,
      "loss": 0.3748,
      "step": 500
    },
    {
      "epoch": 1.7029217317125187,
      "eval_accuracy": 0.8394197098549274,
      "eval_loss": 0.37220239639282227,
      "eval_macro_f1": 0.815786549997201,
      "eval_macro_precision": 0.820092077003088,
      "eval_macro_recall": 0.8120339936531269,
      "eval_runtime": 0.7614,
      "eval_samples_per_second": 2625.556,
      "eval_steps_per_second": 82.746,
      "step": 500
    },
    {
      "epoch": 1.7711665600341224,
      "grad_norm": 1.830322265625,
      "learning_rate": 6.487500000000001e-06,
      "loss": 0.3751,
      "step": 520
    },
    {
      "epoch": 1.8394113883557262,
      "grad_norm": 1.328730821609497,
      "learning_rate": 6.7375e-06,
      "loss": 0.3657,
      "step": 540
    },
    {
      "epoch": 1.9076562166773299,
      "grad_norm": 2.3902993202209473,
      "learning_rate": 6.9875000000000004e-06,
      "loss": 0.3663,
      "step": 560
    },
    {
      "epoch": 1.9759010449989338,
      "grad_norm": 1.485105037689209,
      "learning_rate": 7.237500000000001e-06,
      "loss": 0.3617,
      "step": 580
    },
    {
      "epoch": 2.040946896992962,
      "grad_norm": 3.2052114009857178,
      "learning_rate": 7.487500000000001e-06,
      "loss": 0.355,
      "step": 600
    },
    {
      "epoch": 2.109191725314566,
      "grad_norm": 1.5490623712539673,
      "learning_rate": 7.737500000000002e-06,
      "loss": 0.3387,
      "step": 620
    },
    {
      "epoch": 2.17743655363617,
      "grad_norm": 2.360523223876953,
      "learning_rate": 7.987500000000001e-06,
      "loss": 0.346,
      "step": 640
    },
    {
      "epoch": 2.2456813819577737,
      "grad_norm": 1.1823707818984985,
      "learning_rate": 8.2375e-06,
      "loss": 0.3347,
      "step": 660
    },
    {
      "epoch": 2.3139262102793774,
      "grad_norm": 1.2765713930130005,
      "learning_rate": 8.487500000000001e-06,
      "loss": 0.3447,
      "step": 680
    },
    {
      "epoch": 2.382171038600981,
      "grad_norm": 1.48305344581604,
      "learning_rate": 8.7375e-06,
      "loss": 0.3363,
      "step": 700
    },
    {
      "epoch": 2.450415866922585,
      "grad_norm": 1.3454457521438599,
      "learning_rate": 8.987500000000002e-06,
      "loss": 0.3375,
      "step": 720
    },
    {
      "epoch": 2.5186606952441886,
      "grad_norm": 1.4334810972213745,
      "learning_rate": 9.237500000000001e-06,
      "loss": 0.328,
      "step": 740
    },
    {
      "epoch": 2.5869055235657923,
      "grad_norm": 1.7937854528427124,
      "learning_rate": 9.4875e-06,
      "loss": 0.3332,
      "step": 760
    },
    {
      "epoch": 2.655150351887396,
      "grad_norm": 1.1983451843261719,
      "learning_rate": 9.737500000000001e-06,
      "loss": 0.3315,
      "step": 780
    },
    {
      "epoch": 2.7233951802089997,
      "grad_norm": 1.4359734058380127,
      "learning_rate": 9.9875e-06,
      "loss": 0.3238,
      "step": 800
    },
    {
      "epoch": 2.7916400085306035,
      "grad_norm": 1.4743584394454956,
      "learning_rate": 9.973611111111113e-06,
      "loss": 0.3227,
      "step": 820
    },
    {
      "epoch": 2.859884836852207,
      "grad_norm": 1.596145749092102,
      "learning_rate": 9.945833333333334e-06,
      "loss": 0.3275,
      "step": 840
    },
    {
      "epoch": 2.928129665173811,
      "grad_norm": 1.1056408882141113,
      "learning_rate": 9.918055555555557e-06,
      "loss": 0.3224,
      "step": 860
    },
    {
      "epoch": 2.9963744934954146,
      "grad_norm": 1.5103168487548828,
      "learning_rate": 9.890277777777778e-06,
      "loss": 0.3158,
      "step": 880
    },
    {
      "epoch": 3.0614203454894433,
      "grad_norm": 1.1481752395629883,
      "learning_rate": 9.862500000000001e-06,
      "loss": 0.292,
      "step": 900
    },
    {
      "epoch": 3.129665173811047,
      "grad_norm": 1.7815098762512207,
      "learning_rate": 9.834722222222222e-06,
      "loss": 0.3012,
      "step": 920
    },
    {
      "epoch": 3.1979100021326508,
      "grad_norm": 2.194721221923828,
      "learning_rate": 9.806944444444445e-06,
      "loss": 0.2986,
      "step": 940
    },
    {
      "epoch": 3.2661548304542545,
      "grad_norm": 1.2074517011642456,
      "learning_rate": 9.779166666666668e-06,
      "loss": 0.3,
      "step": 960
    },
    {
      "epoch": 3.334399658775858,
      "grad_norm": 1.3622708320617676,
      "learning_rate": 9.751388888888889e-06,
      "loss": 0.3015,
      "step": 980
    },
    {
      "epoch": 3.4026444870974624,
      "grad_norm": 1.5902178287506104,
      "learning_rate": 9.723611111111112e-06,
      "loss": 0.2966,
      "step": 1000
    },
    {
      "epoch": 3.4026444870974624,
      "eval_accuracy": 0.8569284642321161,
      "eval_loss": 0.3323439061641693,
      "eval_macro_f1": 0.8382686084142394,
      "eval_macro_precision": 0.8374992097253458,
      "eval_macro_recall": 0.8390586756856687,
      "eval_runtime": 0.8006,
      "eval_samples_per_second": 2496.903,
      "eval_steps_per_second": 78.692,
      "step": 1000
    },
    {
      "epoch": 3.470889315419066,
      "grad_norm": 1.509990930557251,
      "learning_rate": 9.695833333333335e-06,
      "loss": 0.2942,
      "step": 1020
    },
    {
      "epoch": 3.53913414374067,
      "grad_norm": 1.6489698886871338,
      "learning_rate": 9.668055555555556e-06,
      "loss": 0.2936,
      "step": 1040
    },
    {
      "epoch": 3.6073789720622735,
      "grad_norm": 1.2774540185928345,
      "learning_rate": 9.640277777777779e-06,
      "loss": 0.2954,
      "step": 1060
    },
    {
      "epoch": 3.6756238003838773,
      "grad_norm": 1.4352965354919434,
      "learning_rate": 9.612500000000002e-06,
      "loss": 0.2978,
      "step": 1080
    },
    {
      "epoch": 3.743868628705481,
      "grad_norm": 1.235145926475525,
      "learning_rate": 9.584722222222223e-06,
      "loss": 0.2894,
      "step": 1100
    },
    {
      "epoch": 3.8121134570270847,
      "grad_norm": 1.2267825603485107,
      "learning_rate": 9.556944444444446e-06,
      "loss": 0.2867,
      "step": 1120
    },
    {
      "epoch": 3.8803582853486884,
      "grad_norm": 2.0049006938934326,
      "learning_rate": 9.529166666666667e-06,
      "loss": 0.2883,
      "step": 1140
    },
    {
      "epoch": 3.948603113670292,
      "grad_norm": 1.1187989711761475,
      "learning_rate": 9.50138888888889e-06,
      "loss": 0.2905,
      "step": 1160
    },
    {
      "epoch": 4.013648965664321,
      "grad_norm": 1.617362141609192,
      "learning_rate": 9.473611111111112e-06,
      "loss": 0.2816,
      "step": 1180
    },
    {
      "epoch": 4.081893793985924,
      "grad_norm": 2.204773187637329,
      "learning_rate": 9.445833333333334e-06,
      "loss": 0.2796,
      "step": 1200
    },
    {
      "epoch": 4.150138622307528,
      "grad_norm": 2.2430853843688965,
      "learning_rate": 9.418055555555557e-06,
      "loss": 0.2669,
      "step": 1220
    },
    {
      "epoch": 4.218383450629132,
      "grad_norm": 1.7991706132888794,
      "learning_rate": 9.39027777777778e-06,
      "loss": 0.2711,
      "step": 1240
    },
    {
      "epoch": 4.286628278950736,
      "grad_norm": 2.161635160446167,
      "learning_rate": 9.3625e-06,
      "loss": 0.271,
      "step": 1260
    },
    {
      "epoch": 4.35487310727234,
      "grad_norm": 1.7587343454360962,
      "learning_rate": 9.334722222222223e-06,
      "loss": 0.2581,
      "step": 1280
    },
    {
      "epoch": 4.423117935593943,
      "grad_norm": 1.7882136106491089,
      "learning_rate": 9.306944444444446e-06,
      "loss": 0.2657,
      "step": 1300
    },
    {
      "epoch": 4.491362763915547,
      "grad_norm": 1.4175416231155396,
      "learning_rate": 9.279166666666667e-06,
      "loss": 0.2662,
      "step": 1320
    },
    {
      "epoch": 4.559607592237151,
      "grad_norm": 1.3771487474441528,
      "learning_rate": 9.251388888888889e-06,
      "loss": 0.2594,
      "step": 1340
    },
    {
      "epoch": 4.627852420558755,
      "grad_norm": 1.4702954292297363,
      "learning_rate": 9.223611111111112e-06,
      "loss": 0.2629,
      "step": 1360
    },
    {
      "epoch": 4.696097248880358,
      "grad_norm": 1.2863059043884277,
      "learning_rate": 9.195833333333334e-06,
      "loss": 0.2557,
      "step": 1380
    },
    {
      "epoch": 4.764342077201962,
      "grad_norm": 1.5014511346817017,
      "learning_rate": 9.168055555555556e-06,
      "loss": 0.2656,
      "step": 1400
    },
    {
      "epoch": 4.8325869055235655,
      "grad_norm": 1.350797414779663,
      "learning_rate": 9.140277777777778e-06,
      "loss": 0.2625,
      "step": 1420
    },
    {
      "epoch": 4.90083173384517,
      "grad_norm": 1.6145776510238647,
      "learning_rate": 9.112500000000001e-06,
      "loss": 0.2686,
      "step": 1440
    },
    {
      "epoch": 4.969076562166773,
      "grad_norm": 1.3295093774795532,
      "learning_rate": 9.084722222222222e-06,
      "loss": 0.2612,
      "step": 1460
    },
    {
      "epoch": 5.034122414160802,
      "grad_norm": 1.7995762825012207,
      "learning_rate": 9.056944444444445e-06,
      "loss": 0.2526,
      "step": 1480
    },
    {
      "epoch": 5.102367242482406,
      "grad_norm": 1.7513810396194458,
      "learning_rate": 9.029166666666668e-06,
      "loss": 0.2473,
      "step": 1500
    },
    {
      "epoch": 5.102367242482406,
      "eval_accuracy": 0.8729364682341171,
      "eval_loss": 0.31359001994132996,
      "eval_macro_f1": 0.8545340468932641,
      "eval_macro_precision": 0.8584976042854098,
      "eval_macro_recall": 0.8509811794114511,
      "eval_runtime": 0.7586,
      "eval_samples_per_second": 2635.149,
      "eval_steps_per_second": 83.049,
      "step": 1500
    },
    {
      "epoch": 5.170612070804009,
      "grad_norm": 1.3031858205795288,
      "learning_rate": 9.00138888888889e-06,
      "loss": 0.2381,
      "step": 1520
    },
    {
      "epoch": 5.238856899125613,
      "grad_norm": 1.184763789176941,
      "learning_rate": 8.973611111111112e-06,
      "loss": 0.2389,
      "step": 1540
    },
    {
      "epoch": 5.3071017274472165,
      "grad_norm": 1.3185213804244995,
      "learning_rate": 8.945833333333333e-06,
      "loss": 0.243,
      "step": 1560
    },
    {
      "epoch": 5.375346555768821,
      "grad_norm": 1.3767590522766113,
      "learning_rate": 8.918055555555556e-06,
      "loss": 0.2385,
      "step": 1580
    },
    {
      "epoch": 5.443591384090425,
      "grad_norm": 2.454850435256958,
      "learning_rate": 8.890277777777777e-06,
      "loss": 0.2445,
      "step": 1600
    },
    {
      "epoch": 5.511836212412028,
      "grad_norm": 2.297668695449829,
      "learning_rate": 8.8625e-06,
      "loss": 0.2437,
      "step": 1620
    },
    {
      "epoch": 5.580081040733631,
      "grad_norm": 1.548923373222351,
      "learning_rate": 8.834722222222223e-06,
      "loss": 0.2504,
      "step": 1640
    },
    {
      "epoch": 5.648325869055236,
      "grad_norm": 1.4516316652297974,
      "learning_rate": 8.806944444444446e-06,
      "loss": 0.2458,
      "step": 1660
    },
    {
      "epoch": 5.71657069737684,
      "grad_norm": 1.2415392398834229,
      "learning_rate": 8.779166666666667e-06,
      "loss": 0.2444,
      "step": 1680
    },
    {
      "epoch": 5.784815525698443,
      "grad_norm": 1.6814930438995361,
      "learning_rate": 8.75138888888889e-06,
      "loss": 0.2385,
      "step": 1700
    },
    {
      "epoch": 5.853060354020047,
      "grad_norm": 1.6364890336990356,
      "learning_rate": 8.723611111111113e-06,
      "loss": 0.2342,
      "step": 1720
    },
    {
      "epoch": 5.9213051823416505,
      "grad_norm": 1.6328494548797607,
      "learning_rate": 8.695833333333334e-06,
      "loss": 0.2455,
      "step": 1740
    },
    {
      "epoch": 5.989550010663255,
      "grad_norm": 2.307600498199463,
      "learning_rate": 8.668055555555557e-06,
      "loss": 0.2377,
      "step": 1760
    },
    {
      "epoch": 6.054595862657283,
      "grad_norm": 1.5939315557479858,
      "learning_rate": 8.640277777777778e-06,
      "loss": 0.2168,
      "step": 1780
    },
    {
      "epoch": 6.122840690978887,
      "grad_norm": 1.8022063970565796,
      "learning_rate": 8.612500000000001e-06,
      "loss": 0.2244,
      "step": 1800
    },
    {
      "epoch": 6.191085519300491,
      "grad_norm": 1.505889892578125,
      "learning_rate": 8.584722222222222e-06,
      "loss": 0.2213,
      "step": 1820
    },
    {
      "epoch": 6.259330347622094,
      "grad_norm": 1.3430383205413818,
      "learning_rate": 8.556944444444445e-06,
      "loss": 0.2191,
      "step": 1840
    },
    {
      "epoch": 6.327575175943698,
      "grad_norm": 1.3859537839889526,
      "learning_rate": 8.529166666666668e-06,
      "loss": 0.2227,
      "step": 1860
    },
    {
      "epoch": 6.3958200042653015,
      "grad_norm": 1.5031168460845947,
      "learning_rate": 8.501388888888889e-06,
      "loss": 0.2203,
      "step": 1880
    },
    {
      "epoch": 6.464064832586906,
      "grad_norm": 1.5275505781173706,
      "learning_rate": 8.473611111111112e-06,
      "loss": 0.2157,
      "step": 1900
    },
    {
      "epoch": 6.532309660908509,
      "grad_norm": 1.5134201049804688,
      "learning_rate": 8.445833333333335e-06,
      "loss": 0.2257,
      "step": 1920
    },
    {
      "epoch": 6.600554489230113,
      "grad_norm": 1.7249444723129272,
      "learning_rate": 8.418055555555556e-06,
      "loss": 0.2153,
      "step": 1940
    },
    {
      "epoch": 6.668799317551716,
      "grad_norm": 1.7126336097717285,
      "learning_rate": 8.390277777777779e-06,
      "loss": 0.2287,
      "step": 1960
    },
    {
      "epoch": 6.737044145873321,
      "grad_norm": 1.5203657150268555,
      "learning_rate": 8.362500000000002e-06,
      "loss": 0.2258,
      "step": 1980
    },
    {
      "epoch": 6.805288974194925,
      "grad_norm": 1.5289170742034912,
      "learning_rate": 8.334722222222223e-06,
      "loss": 0.2138,
      "step": 2000
    },
    {
      "epoch": 6.805288974194925,
      "eval_accuracy": 0.870935467733867,
      "eval_loss": 0.3188134729862213,
      "eval_macro_f1": 0.8521225863279154,
      "eval_macro_precision": 0.8563952746219814,
      "eval_macro_recall": 0.8483254961471893,
      "eval_runtime": 0.7505,
      "eval_samples_per_second": 2663.526,
      "eval_steps_per_second": 83.943,
      "step": 2000
    },
    {
      "epoch": 6.873533802516528,
      "grad_norm": 1.650875210762024,
      "learning_rate": 8.306944444444444e-06,
      "loss": 0.2164,
      "step": 2020
    },
    {
      "epoch": 6.941778630838132,
      "grad_norm": 1.7216562032699585,
      "learning_rate": 8.279166666666667e-06,
      "loss": 0.2294,
      "step": 2040
    },
    {
      "epoch": 7.00682448283216,
      "grad_norm": 1.3719396591186523,
      "learning_rate": 8.25138888888889e-06,
      "loss": 0.2188,
      "step": 2060
    },
    {
      "epoch": 7.075069311153764,
      "grad_norm": 1.5852384567260742,
      "learning_rate": 8.223611111111111e-06,
      "loss": 0.2004,
      "step": 2080
    },
    {
      "epoch": 7.143314139475368,
      "grad_norm": 1.9619814157485962,
      "learning_rate": 8.195833333333334e-06,
      "loss": 0.1965,
      "step": 2100
    },
    {
      "epoch": 7.211558967796972,
      "grad_norm": 1.4082950353622437,
      "learning_rate": 8.168055555555557e-06,
      "loss": 0.1964,
      "step": 2120
    },
    {
      "epoch": 7.279803796118576,
      "grad_norm": 1.6102043390274048,
      "learning_rate": 8.140277777777778e-06,
      "loss": 0.2057,
      "step": 2140
    },
    {
      "epoch": 7.348048624440179,
      "grad_norm": 1.6108965873718262,
      "learning_rate": 8.1125e-06,
      "loss": 0.2006,
      "step": 2160
    },
    {
      "epoch": 7.416293452761783,
      "grad_norm": 2.047959089279175,
      "learning_rate": 8.084722222222224e-06,
      "loss": 0.2001,
      "step": 2180
    },
    {
      "epoch": 7.4845382810833865,
      "grad_norm": 1.61355459690094,
      "learning_rate": 8.056944444444446e-06,
      "loss": 0.2041,
      "step": 2200
    },
    {
      "epoch": 7.552783109404991,
      "grad_norm": 1.75156831741333,
      "learning_rate": 8.029166666666668e-06,
      "loss": 0.2025,
      "step": 2220
    },
    {
      "epoch": 7.621027937726594,
      "grad_norm": 2.583676338195801,
      "learning_rate": 8.001388888888889e-06,
      "loss": 0.2027,
      "step": 2240
    },
    {
      "epoch": 7.689272766048198,
      "grad_norm": 1.479519248008728,
      "learning_rate": 7.973611111111112e-06,
      "loss": 0.2062,
      "step": 2260
    },
    {
      "epoch": 7.757517594369801,
      "grad_norm": 1.6042001247406006,
      "learning_rate": 7.945833333333334e-06,
      "loss": 0.204,
      "step": 2280
    },
    {
      "epoch": 7.8257624226914055,
      "grad_norm": 1.8064298629760742,
      "learning_rate": 7.918055555555556e-06,
      "loss": 0.2101,
      "step": 2300
    },
    {
      "epoch": 7.894007251013009,
      "grad_norm": 1.8129231929779053,
      "learning_rate": 7.890277777777779e-06,
      "loss": 0.2031,
      "step": 2320
    },
    {
      "epoch": 7.962252079334613,
      "grad_norm": 2.6623618602752686,
      "learning_rate": 7.862500000000001e-06,
      "loss": 0.2091,
      "step": 2340
    },
    {
      "epoch": 8.027297931328642,
      "grad_norm": 2.204676628112793,
      "learning_rate": 7.834722222222223e-06,
      "loss": 0.1983,
      "step": 2360
    },
    {
      "epoch": 8.095542759650245,
      "grad_norm": 1.7524410486221313,
      "learning_rate": 7.806944444444445e-06,
      "loss": 0.1831,
      "step": 2380
    },
    {
      "epoch": 8.163787587971848,
      "grad_norm": 1.5748860836029053,
      "learning_rate": 7.779166666666668e-06,
      "loss": 0.1748,
      "step": 2400
    },
    {
      "epoch": 8.232032416293453,
      "grad_norm": 1.538834571838379,
      "learning_rate": 7.75138888888889e-06,
      "loss": 0.1892,
      "step": 2420
    },
    {
      "epoch": 8.300277244615057,
      "grad_norm": 1.872692346572876,
      "learning_rate": 7.723611111111112e-06,
      "loss": 0.1986,
      "step": 2440
    },
    {
      "epoch": 8.36852207293666,
      "grad_norm": 1.6170392036437988,
      "learning_rate": 7.695833333333333e-06,
      "loss": 0.1842,
      "step": 2460
    },
    {
      "epoch": 8.436766901258263,
      "grad_norm": 1.789853572845459,
      "learning_rate": 7.668055555555556e-06,
      "loss": 0.1877,
      "step": 2480
    },
    {
      "epoch": 8.505011729579868,
      "grad_norm": 1.4999449253082275,
      "learning_rate": 7.640277777777778e-06,
      "loss": 0.1847,
      "step": 2500
    },
    {
      "epoch": 8.505011729579868,
      "eval_accuracy": 0.8739369684842421,
      "eval_loss": 0.3353341817855835,
      "eval_macro_f1": 0.8577134180098385,
      "eval_macro_precision": 0.8563786800293547,
      "eval_macro_recall": 0.8591070144517259,
      "eval_runtime": 0.7502,
      "eval_samples_per_second": 2664.535,
      "eval_steps_per_second": 83.975,
      "step": 2500
    },
    {
      "epoch": 8.573256557901471,
      "grad_norm": 2.0117862224578857,
      "learning_rate": 7.6125e-06,
      "loss": 0.1814,
      "step": 2520
    },
    {
      "epoch": 8.641501386223075,
      "grad_norm": 3.782045602798462,
      "learning_rate": 7.584722222222222e-06,
      "loss": 0.1873,
      "step": 2540
    },
    {
      "epoch": 8.70974621454468,
      "grad_norm": 2.5659573078155518,
      "learning_rate": 7.556944444444445e-06,
      "loss": 0.1852,
      "step": 2560
    },
    {
      "epoch": 8.777991042866283,
      "grad_norm": 1.8110466003417969,
      "learning_rate": 7.529166666666667e-06,
      "loss": 0.1905,
      "step": 2580
    },
    {
      "epoch": 8.846235871187886,
      "grad_norm": 1.4897222518920898,
      "learning_rate": 7.501388888888889e-06,
      "loss": 0.1905,
      "step": 2600
    },
    {
      "epoch": 8.91448069950949,
      "grad_norm": 1.4558017253875732,
      "learning_rate": 7.473611111111112e-06,
      "loss": 0.1876,
      "step": 2620
    },
    {
      "epoch": 8.982725527831095,
      "grad_norm": 1.6601684093475342,
      "learning_rate": 7.445833333333334e-06,
      "loss": 0.1923,
      "step": 2640
    },
    {
      "epoch": 9.047771379825123,
      "grad_norm": 1.547455072402954,
      "learning_rate": 7.418055555555557e-06,
      "loss": 0.1783,
      "step": 2660
    },
    {
      "epoch": 9.116016208146727,
      "grad_norm": 1.8921984434127808,
      "learning_rate": 7.390277777777778e-06,
      "loss": 0.175,
      "step": 2680
    },
    {
      "epoch": 9.18426103646833,
      "grad_norm": 1.6409608125686646,
      "learning_rate": 7.3625e-06,
      "loss": 0.1587,
      "step": 2700
    },
    {
      "epoch": 9.252505864789933,
      "grad_norm": 1.675615906715393,
      "learning_rate": 7.334722222222222e-06,
      "loss": 0.168,
      "step": 2720
    },
    {
      "epoch": 9.320750693111538,
      "grad_norm": 2.9220681190490723,
      "learning_rate": 7.306944444444445e-06,
      "loss": 0.1691,
      "step": 2740
    },
    {
      "epoch": 9.388995521433142,
      "grad_norm": 1.633306860923767,
      "learning_rate": 7.279166666666667e-06,
      "loss": 0.171,
      "step": 2760
    },
    {
      "epoch": 9.457240349754745,
      "grad_norm": 1.7951016426086426,
      "learning_rate": 7.251388888888889e-06,
      "loss": 0.1734,
      "step": 2780
    },
    {
      "epoch": 9.525485178076348,
      "grad_norm": 3.023272752761841,
      "learning_rate": 7.223611111111112e-06,
      "loss": 0.1735,
      "step": 2800
    },
    {
      "epoch": 9.593730006397953,
      "grad_norm": 2.21646785736084,
      "learning_rate": 7.195833333333334e-06,
      "loss": 0.1694,
      "step": 2820
    },
    {
      "epoch": 9.661974834719556,
      "grad_norm": 1.977999210357666,
      "learning_rate": 7.168055555555556e-06,
      "loss": 0.1686,
      "step": 2840
    },
    {
      "epoch": 9.73021966304116,
      "grad_norm": 1.9488747119903564,
      "learning_rate": 7.140277777777779e-06,
      "loss": 0.1697,
      "step": 2860
    },
    {
      "epoch": 9.798464491362765,
      "grad_norm": 1.89760422706604,
      "learning_rate": 7.112500000000001e-06,
      "loss": 0.1749,
      "step": 2880
    },
    {
      "epoch": 9.866709319684368,
      "grad_norm": 3.255558490753174,
      "learning_rate": 7.084722222222222e-06,
      "loss": 0.1747,
      "step": 2900
    },
    {
      "epoch": 9.934954148005971,
      "grad_norm": 1.7002118825912476,
      "learning_rate": 7.056944444444445e-06,
      "loss": 0.1736,
      "step": 2920
    },
    {
      "epoch": 10.0,
      "grad_norm": 7.084474563598633,
      "learning_rate": 7.029166666666667e-06,
      "loss": 0.1631,
      "step": 2940
    },
    {
      "epoch": 10.068244828321603,
      "grad_norm": 1.8775780200958252,
      "learning_rate": 7.001388888888889e-06,
      "loss": 0.1569,
      "step": 2960
    },
    {
      "epoch": 10.136489656643208,
      "grad_norm": 1.539544701576233,
      "learning_rate": 6.973611111111112e-06,
      "loss": 0.1548,
      "step": 2980
    },
    {
      "epoch": 10.204734484964812,
      "grad_norm": 1.5896787643432617,
      "learning_rate": 6.945833333333334e-06,
      "loss": 0.1572,
      "step": 3000
    },
    {
      "epoch": 10.204734484964812,
      "eval_accuracy": 0.8699349674837419,
      "eval_loss": 0.35384443402290344,
      "eval_macro_f1": 0.8520513661351515,
      "eval_macro_precision": 0.8534742736160522,
      "eval_macro_recall": 0.8506879937937651,
      "eval_runtime": 0.7475,
      "eval_samples_per_second": 2674.115,
      "eval_steps_per_second": 84.277,
      "step": 3000
    },
    {
      "epoch": 10.272979313286415,
      "grad_norm": 1.7855536937713623,
      "learning_rate": 6.918055555555556e-06,
      "loss": 0.1542,
      "step": 3020
    },
    {
      "epoch": 10.341224141608018,
      "grad_norm": 1.4966278076171875,
      "learning_rate": 6.890277777777779e-06,
      "loss": 0.1629,
      "step": 3040
    },
    {
      "epoch": 10.409468969929623,
      "grad_norm": 1.6138337850570679,
      "learning_rate": 6.862500000000001e-06,
      "loss": 0.1548,
      "step": 3060
    },
    {
      "epoch": 10.477713798251227,
      "grad_norm": 1.7328606843948364,
      "learning_rate": 6.834722222222223e-06,
      "loss": 0.1543,
      "step": 3080
    },
    {
      "epoch": 10.54595862657283,
      "grad_norm": 1.6013966798782349,
      "learning_rate": 6.806944444444446e-06,
      "loss": 0.1564,
      "step": 3100
    },
    {
      "epoch": 10.614203454894433,
      "grad_norm": 1.9068264961242676,
      "learning_rate": 6.779166666666667e-06,
      "loss": 0.1618,
      "step": 3120
    },
    {
      "epoch": 10.682448283216038,
      "grad_norm": 1.8498235940933228,
      "learning_rate": 6.751388888888889e-06,
      "loss": 0.1548,
      "step": 3140
    },
    {
      "epoch": 10.750693111537641,
      "grad_norm": 1.755973219871521,
      "learning_rate": 6.723611111111112e-06,
      "loss": 0.1592,
      "step": 3160
    },
    {
      "epoch": 10.818937939859245,
      "grad_norm": 1.8026460409164429,
      "learning_rate": 6.695833333333334e-06,
      "loss": 0.1553,
      "step": 3180
    },
    {
      "epoch": 10.88718276818085,
      "grad_norm": 1.515430212020874,
      "learning_rate": 6.668055555555556e-06,
      "loss": 0.1613,
      "step": 3200
    },
    {
      "epoch": 10.955427596502453,
      "grad_norm": 1.6015512943267822,
      "learning_rate": 6.640277777777779e-06,
      "loss": 0.1542,
      "step": 3220
    },
    {
      "epoch": 11.020473448496482,
      "grad_norm": 1.7186561822891235,
      "learning_rate": 6.612500000000001e-06,
      "loss": 0.1512,
      "step": 3240
    },
    {
      "epoch": 11.088718276818085,
      "grad_norm": 1.7746094465255737,
      "learning_rate": 6.584722222222223e-06,
      "loss": 0.1436,
      "step": 3260
    },
    {
      "epoch": 11.156963105139688,
      "grad_norm": 1.7998631000518799,
      "learning_rate": 6.5569444444444455e-06,
      "loss": 0.1392,
      "step": 3280
    },
    {
      "epoch": 11.225207933461292,
      "grad_norm": 1.894147515296936,
      "learning_rate": 6.5291666666666675e-06,
      "loss": 0.142,
      "step": 3300
    },
    {
      "epoch": 11.293452761782897,
      "grad_norm": 1.8066705465316772,
      "learning_rate": 6.5013888888888896e-06,
      "loss": 0.147,
      "step": 3320
    },
    {
      "epoch": 11.3616975901045,
      "grad_norm": 1.9624675512313843,
      "learning_rate": 6.473611111111112e-06,
      "loss": 0.1412,
      "step": 3340
    },
    {
      "epoch": 11.429942418426103,
      "grad_norm": 1.7693685293197632,
      "learning_rate": 6.445833333333334e-06,
      "loss": 0.1497,
      "step": 3360
    },
    {
      "epoch": 11.498187246747708,
      "grad_norm": 1.927676796913147,
      "learning_rate": 6.418055555555556e-06,
      "loss": 0.1414,
      "step": 3380
    },
    {
      "epoch": 11.566432075069311,
      "grad_norm": 2.0395712852478027,
      "learning_rate": 6.390277777777778e-06,
      "loss": 0.1451,
      "step": 3400
    },
    {
      "epoch": 11.634676903390915,
      "grad_norm": 2.52909255027771,
      "learning_rate": 6.3625000000000005e-06,
      "loss": 0.1422,
      "step": 3420
    },
    {
      "epoch": 11.702921731712518,
      "grad_norm": 1.8666776418685913,
      "learning_rate": 6.3347222222222225e-06,
      "loss": 0.1467,
      "step": 3440
    },
    {
      "epoch": 11.771166560034123,
      "grad_norm": 2.5246448516845703,
      "learning_rate": 6.306944444444445e-06,
      "loss": 0.1459,
      "step": 3460
    },
    {
      "epoch": 11.839411388355726,
      "grad_norm": 1.8723194599151611,
      "learning_rate": 6.279166666666667e-06,
      "loss": 0.148,
      "step": 3480
    },
    {
      "epoch": 11.90765621667733,
      "grad_norm": 2.0397865772247314,
      "learning_rate": 6.251388888888889e-06,
      "loss": 0.1471,
      "step": 3500
    },
    {
      "epoch": 11.90765621667733,
      "eval_accuracy": 0.8759379689844923,
      "eval_loss": 0.36008766293525696,
      "eval_macro_f1": 0.8594319571189097,
      "eval_macro_precision": 0.8594319571189097,
      "eval_macro_recall": 0.8594319571189097,
      "eval_runtime": 0.7483,
      "eval_samples_per_second": 2671.289,
      "eval_steps_per_second": 84.188,
      "step": 3500
    },
    {
      "epoch": 11.975901044998933,
      "grad_norm": 1.6292169094085693,
      "learning_rate": 6.223611111111112e-06,
      "loss": 0.1429,
      "step": 3520
    },
    {
      "epoch": 12.040946896992962,
      "grad_norm": 1.8416550159454346,
      "learning_rate": 6.195833333333334e-06,
      "loss": 0.1389,
      "step": 3540
    },
    {
      "epoch": 12.109191725314567,
      "grad_norm": 1.9260268211364746,
      "learning_rate": 6.168055555555556e-06,
      "loss": 0.1349,
      "step": 3560
    },
    {
      "epoch": 12.17743655363617,
      "grad_norm": 2.002027750015259,
      "learning_rate": 6.1402777777777775e-06,
      "loss": 0.1388,
      "step": 3580
    },
    {
      "epoch": 12.245681381957773,
      "grad_norm": 2.590033769607544,
      "learning_rate": 6.1125e-06,
      "loss": 0.1351,
      "step": 3600
    },
    {
      "epoch": 12.313926210279377,
      "grad_norm": 2.3013901710510254,
      "learning_rate": 6.084722222222222e-06,
      "loss": 0.1338,
      "step": 3620
    },
    {
      "epoch": 12.382171038600982,
      "grad_norm": 1.9365898370742798,
      "learning_rate": 6.056944444444444e-06,
      "loss": 0.1362,
      "step": 3640
    },
    {
      "epoch": 12.450415866922585,
      "grad_norm": 2.1939778327941895,
      "learning_rate": 6.029166666666667e-06,
      "loss": 0.1337,
      "step": 3660
    },
    {
      "epoch": 12.518660695244188,
      "grad_norm": 1.8743609189987183,
      "learning_rate": 6.001388888888889e-06,
      "loss": 0.1339,
      "step": 3680
    },
    {
      "epoch": 12.586905523565793,
      "grad_norm": 1.9297038316726685,
      "learning_rate": 5.973611111111111e-06,
      "loss": 0.1331,
      "step": 3700
    },
    {
      "epoch": 12.655150351887396,
      "grad_norm": 1.857985019683838,
      "learning_rate": 5.945833333333334e-06,
      "loss": 0.1332,
      "step": 3720
    },
    {
      "epoch": 12.723395180209,
      "grad_norm": 2.0677571296691895,
      "learning_rate": 5.918055555555556e-06,
      "loss": 0.133,
      "step": 3740
    },
    {
      "epoch": 12.791640008530603,
      "grad_norm": 1.797634482383728,
      "learning_rate": 5.890277777777779e-06,
      "loss": 0.1333,
      "step": 3760
    },
    {
      "epoch": 12.859884836852208,
      "grad_norm": 1.923053503036499,
      "learning_rate": 5.862500000000001e-06,
      "loss": 0.1367,
      "step": 3780
    },
    {
      "epoch": 12.928129665173811,
      "grad_norm": 1.8176095485687256,
      "learning_rate": 5.834722222222222e-06,
      "loss": 0.1332,
      "step": 3800
    },
    {
      "epoch": 12.996374493495415,
      "grad_norm": 2.383370876312256,
      "learning_rate": 5.806944444444444e-06,
      "loss": 0.1291,
      "step": 3820
    },
    {
      "epoch": 13.061420345489443,
      "grad_norm": 2.2397828102111816,
      "learning_rate": 5.779166666666667e-06,
      "loss": 0.1213,
      "step": 3840
    },
    {
      "epoch": 13.129665173811047,
      "grad_norm": 2.269753932952881,
      "learning_rate": 5.751388888888889e-06,
      "loss": 0.1274,
      "step": 3860
    },
    {
      "epoch": 13.197910002132652,
      "grad_norm": 2.141780376434326,
      "learning_rate": 5.723611111111111e-06,
      "loss": 0.1249,
      "step": 3880
    },
    {
      "epoch": 13.266154830454255,
      "grad_norm": 1.9455034732818604,
      "learning_rate": 5.695833333333334e-06,
      "loss": 0.1254,
      "step": 3900
    },
    {
      "epoch": 13.334399658775858,
      "grad_norm": 1.9371626377105713,
      "learning_rate": 5.668055555555556e-06,
      "loss": 0.1245,
      "step": 3920
    },
    {
      "epoch": 13.402644487097461,
      "grad_norm": 2.133942127227783,
      "learning_rate": 5.640277777777778e-06,
      "loss": 0.1226,
      "step": 3940
    },
    {
      "epoch": 13.470889315419067,
      "grad_norm": 2.1155529022216797,
      "learning_rate": 5.612500000000001e-06,
      "loss": 0.1229,
      "step": 3960
    },
    {
      "epoch": 13.53913414374067,
      "grad_norm": 2.0490198135375977,
      "learning_rate": 5.584722222222223e-06,
      "loss": 0.1221,
      "step": 3980
    },
    {
      "epoch": 13.607378972062273,
      "grad_norm": 2.1419801712036133,
      "learning_rate": 5.556944444444446e-06,
      "loss": 0.1278,
      "step": 4000
    },
    {
      "epoch": 13.607378972062273,
      "eval_accuracy": 0.8764382191095548,
      "eval_loss": 0.3880080282688141,
      "eval_macro_f1": 0.8606955671412253,
      "eval_macro_precision": 0.8589670159361207,
      "eval_macro_recall": 0.862523732723598,
      "eval_runtime": 0.7511,
      "eval_samples_per_second": 2661.333,
      "eval_steps_per_second": 83.874,
      "step": 4000
    },
    {
      "epoch": 13.675623800383876,
      "grad_norm": 2.210322856903076,
      "learning_rate": 5.529166666666667e-06,
      "loss": 0.1232,
      "step": 4020
    },
    {
      "epoch": 13.743868628705481,
      "grad_norm": 1.9720704555511475,
      "learning_rate": 5.501388888888889e-06,
      "loss": 0.1232,
      "step": 4040
    },
    {
      "epoch": 13.812113457027085,
      "grad_norm": 2.7294435501098633,
      "learning_rate": 5.473611111111111e-06,
      "loss": 0.122,
      "step": 4060
    },
    {
      "epoch": 13.880358285348688,
      "grad_norm": 1.784522533416748,
      "learning_rate": 5.445833333333334e-06,
      "loss": 0.1248,
      "step": 4080
    },
    {
      "epoch": 13.948603113670291,
      "grad_norm": 1.821851134300232,
      "learning_rate": 5.418055555555556e-06,
      "loss": 0.1281,
      "step": 4100
    },
    {
      "epoch": 14.01364896566432,
      "grad_norm": 2.639425754547119,
      "learning_rate": 5.390277777777778e-06,
      "loss": 0.1178,
      "step": 4120
    },
    {
      "epoch": 14.081893793985925,
      "grad_norm": 2.3840620517730713,
      "learning_rate": 5.362500000000001e-06,
      "loss": 0.1185,
      "step": 4140
    },
    {
      "epoch": 14.150138622307528,
      "grad_norm": 2.3026957511901855,
      "learning_rate": 5.334722222222223e-06,
      "loss": 0.1145,
      "step": 4160
    },
    {
      "epoch": 14.218383450629132,
      "grad_norm": 2.1457393169403076,
      "learning_rate": 5.306944444444445e-06,
      "loss": 0.1162,
      "step": 4180
    },
    {
      "epoch": 14.286628278950737,
      "grad_norm": 1.9156731367111206,
      "learning_rate": 5.279166666666668e-06,
      "loss": 0.1186,
      "step": 4200
    },
    {
      "epoch": 14.35487310727234,
      "grad_norm": 2.2884116172790527,
      "learning_rate": 5.25138888888889e-06,
      "loss": 0.1156,
      "step": 4220
    },
    {
      "epoch": 14.423117935593943,
      "grad_norm": 1.7458127737045288,
      "learning_rate": 5.223611111111112e-06,
      "loss": 0.1112,
      "step": 4240
    },
    {
      "epoch": 14.491362763915546,
      "grad_norm": 1.9924979209899902,
      "learning_rate": 5.195833333333334e-06,
      "loss": 0.1092,
      "step": 4260
    },
    {
      "epoch": 14.559607592237152,
      "grad_norm": 2.3514342308044434,
      "learning_rate": 5.168055555555556e-06,
      "loss": 0.1144,
      "step": 4280
    },
    {
      "epoch": 14.627852420558755,
      "grad_norm": 1.6678751707077026,
      "learning_rate": 5.140277777777778e-06,
      "loss": 0.1174,
      "step": 4300
    },
    {
      "epoch": 14.696097248880358,
      "grad_norm": 2.112272262573242,
      "learning_rate": 5.112500000000001e-06,
      "loss": 0.1166,
      "step": 4320
    },
    {
      "epoch": 14.764342077201961,
      "grad_norm": 1.5633127689361572,
      "learning_rate": 5.084722222222223e-06,
      "loss": 0.1208,
      "step": 4340
    },
    {
      "epoch": 14.832586905523566,
      "grad_norm": 2.1657955646514893,
      "learning_rate": 5.056944444444445e-06,
      "loss": 0.1102,
      "step": 4360
    },
    {
      "epoch": 14.90083173384517,
      "grad_norm": 1.8965367078781128,
      "learning_rate": 5.0291666666666675e-06,
      "loss": 0.1213,
      "step": 4380
    },
    {
      "epoch": 14.969076562166773,
      "grad_norm": 2.2705657482147217,
      "learning_rate": 5.0013888888888895e-06,
      "loss": 0.1176,
      "step": 4400
    },
    {
      "epoch": 15.034122414160802,
      "grad_norm": 2.240562915802002,
      "learning_rate": 4.9736111111111115e-06,
      "loss": 0.1092,
      "step": 4420
    },
    {
      "epoch": 15.102367242482405,
      "grad_norm": 2.3520233631134033,
      "learning_rate": 4.9458333333333336e-06,
      "loss": 0.1086,
      "step": 4440
    },
    {
      "epoch": 15.17061207080401,
      "grad_norm": 2.598005533218384,
      "learning_rate": 4.9180555555555556e-06,
      "loss": 0.1059,
      "step": 4460
    },
    {
      "epoch": 15.238856899125613,
      "grad_norm": 2.022289514541626,
      "learning_rate": 4.8902777777777784e-06,
      "loss": 0.1143,
      "step": 4480
    },
    {
      "epoch": 15.307101727447217,
      "grad_norm": 1.983299732208252,
      "learning_rate": 4.8625000000000005e-06,
      "loss": 0.1071,
      "step": 4500
    },
    {
      "epoch": 15.307101727447217,
      "eval_accuracy": 0.8729364682341171,
      "eval_loss": 0.408562570810318,
      "eval_macro_f1": 0.8573378513548445,
      "eval_macro_precision": 0.854317492574296,
      "eval_macro_recall": 0.8606925985659424,
      "eval_runtime": 0.7618,
      "eval_samples_per_second": 2624.061,
      "eval_steps_per_second": 82.699,
      "step": 4500
    },
    {
      "epoch": 15.37534655576882,
      "grad_norm": 2.3602728843688965,
      "learning_rate": 4.8347222222222225e-06,
      "loss": 0.1084,
      "step": 4520
    },
    {
      "epoch": 15.443591384090425,
      "grad_norm": 2.3666749000549316,
      "learning_rate": 4.8069444444444445e-06,
      "loss": 0.1051,
      "step": 4540
    },
    {
      "epoch": 15.511836212412028,
      "grad_norm": 2.6120266914367676,
      "learning_rate": 4.779166666666667e-06,
      "loss": 0.1074,
      "step": 4560
    },
    {
      "epoch": 15.580081040733631,
      "grad_norm": 1.8546059131622314,
      "learning_rate": 4.751388888888889e-06,
      "loss": 0.1069,
      "step": 4580
    },
    {
      "epoch": 15.648325869055236,
      "grad_norm": 2.579831600189209,
      "learning_rate": 4.723611111111111e-06,
      "loss": 0.107,
      "step": 4600
    },
    {
      "epoch": 15.71657069737684,
      "grad_norm": 2.2919204235076904,
      "learning_rate": 4.695833333333334e-06,
      "loss": 0.1061,
      "step": 4620
    },
    {
      "epoch": 15.784815525698443,
      "grad_norm": 2.0177371501922607,
      "learning_rate": 4.6680555555555554e-06,
      "loss": 0.1113,
      "step": 4640
    },
    {
      "epoch": 15.853060354020046,
      "grad_norm": 2.6864967346191406,
      "learning_rate": 4.640277777777778e-06,
      "loss": 0.1082,
      "step": 4660
    },
    {
      "epoch": 15.921305182341651,
      "grad_norm": 2.858505964279175,
      "learning_rate": 4.6125e-06,
      "loss": 0.1104,
      "step": 4680
    },
    {
      "epoch": 15.989550010663255,
      "grad_norm": 1.9770798683166504,
      "learning_rate": 4.584722222222222e-06,
      "loss": 0.1074,
      "step": 4700
    },
    {
      "epoch": 16.054595862657283,
      "grad_norm": 2.500070333480835,
      "learning_rate": 4.556944444444445e-06,
      "loss": 0.0982,
      "step": 4720
    },
    {
      "epoch": 16.122840690978887,
      "grad_norm": 2.178569793701172,
      "learning_rate": 4.529166666666667e-06,
      "loss": 0.1048,
      "step": 4740
    },
    {
      "epoch": 16.19108551930049,
      "grad_norm": 1.846859335899353,
      "learning_rate": 4.501388888888889e-06,
      "loss": 0.0985,
      "step": 4760
    },
    {
      "epoch": 16.259330347622093,
      "grad_norm": 1.9573537111282349,
      "learning_rate": 4.473611111111111e-06,
      "loss": 0.0979,
      "step": 4780
    },
    {
      "epoch": 16.327575175943696,
      "grad_norm": 1.8706817626953125,
      "learning_rate": 4.445833333333333e-06,
      "loss": 0.1014,
      "step": 4800
    },
    {
      "epoch": 16.395820004265303,
      "grad_norm": 1.8451213836669922,
      "learning_rate": 4.418055555555556e-06,
      "loss": 0.1038,
      "step": 4820
    },
    {
      "epoch": 16.464064832586907,
      "grad_norm": 2.3150486946105957,
      "learning_rate": 4.390277777777778e-06,
      "loss": 0.0999,
      "step": 4840
    },
    {
      "epoch": 16.53230966090851,
      "grad_norm": 1.8817607164382935,
      "learning_rate": 4.362500000000001e-06,
      "loss": 0.1068,
      "step": 4860
    },
    {
      "epoch": 16.600554489230113,
      "grad_norm": 1.6327528953552246,
      "learning_rate": 4.334722222222222e-06,
      "loss": 0.0995,
      "step": 4880
    },
    {
      "epoch": 16.668799317551716,
      "grad_norm": 1.7777513265609741,
      "learning_rate": 4.306944444444445e-06,
      "loss": 0.105,
      "step": 4900
    },
    {
      "epoch": 16.73704414587332,
      "grad_norm": 2.762895107269287,
      "learning_rate": 4.279166666666667e-06,
      "loss": 0.0989,
      "step": 4920
    },
    {
      "epoch": 16.805288974194923,
      "grad_norm": 2.275545597076416,
      "learning_rate": 4.251388888888889e-06,
      "loss": 0.1019,
      "step": 4940
    },
    {
      "epoch": 16.873533802516526,
      "grad_norm": 2.445847988128662,
      "learning_rate": 4.223611111111112e-06,
      "loss": 0.1025,
      "step": 4960
    },
    {
      "epoch": 16.941778630838133,
      "grad_norm": 2.0141477584838867,
      "learning_rate": 4.195833333333333e-06,
      "loss": 0.0973,
      "step": 4980
    },
    {
      "epoch": 17.00682448283216,
      "grad_norm": 2.6140198707580566,
      "learning_rate": 4.168055555555556e-06,
      "loss": 0.1012,
      "step": 5000
    },
    {
      "epoch": 17.00682448283216,
      "eval_accuracy": 0.8714357178589295,
      "eval_loss": 0.40913522243499756,
      "eval_macro_f1": 0.8532324301469696,
      "eval_macro_precision": 0.8560417957476408,
      "eval_macro_recall": 0.8506403582195183,
      "eval_runtime": 0.7459,
      "eval_samples_per_second": 2680.052,
      "eval_steps_per_second": 84.464,
      "step": 5000
    },
    {
      "epoch": 17.075069311153765,
      "grad_norm": 1.877256989479065,
      "learning_rate": 4.140277777777778e-06,
      "loss": 0.0965,
      "step": 5020
    },
    {
      "epoch": 17.14331413947537,
      "grad_norm": 2.006678342819214,
      "learning_rate": 4.1125e-06,
      "loss": 0.0905,
      "step": 5040
    },
    {
      "epoch": 17.21155896779697,
      "grad_norm": 2.6349031925201416,
      "learning_rate": 4.084722222222223e-06,
      "loss": 0.0922,
      "step": 5060
    },
    {
      "epoch": 17.279803796118575,
      "grad_norm": 2.304844379425049,
      "learning_rate": 4.056944444444445e-06,
      "loss": 0.0953,
      "step": 5080
    },
    {
      "epoch": 17.348048624440178,
      "grad_norm": 2.0974647998809814,
      "learning_rate": 4.029166666666667e-06,
      "loss": 0.0972,
      "step": 5100
    },
    {
      "epoch": 17.41629345276178,
      "grad_norm": 2.3231983184814453,
      "learning_rate": 4.001388888888889e-06,
      "loss": 0.0972,
      "step": 5120
    },
    {
      "epoch": 17.48453828108339,
      "grad_norm": 2.1525094509124756,
      "learning_rate": 3.973611111111112e-06,
      "loss": 0.095,
      "step": 5140
    },
    {
      "epoch": 17.55278310940499,
      "grad_norm": 2.0929923057556152,
      "learning_rate": 3.945833333333334e-06,
      "loss": 0.0933,
      "step": 5160
    },
    {
      "epoch": 17.621027937726595,
      "grad_norm": 2.4601786136627197,
      "learning_rate": 3.918055555555556e-06,
      "loss": 0.0969,
      "step": 5180
    },
    {
      "epoch": 17.689272766048198,
      "grad_norm": 2.108586549758911,
      "learning_rate": 3.890277777777778e-06,
      "loss": 0.0954,
      "step": 5200
    },
    {
      "epoch": 17.7575175943698,
      "grad_norm": 3.584789991378784,
      "learning_rate": 3.8625e-06,
      "loss": 0.0992,
      "step": 5220
    },
    {
      "epoch": 17.825762422691405,
      "grad_norm": 2.3476953506469727,
      "learning_rate": 3.834722222222223e-06,
      "loss": 0.0966,
      "step": 5240
    },
    {
      "epoch": 17.894007251013008,
      "grad_norm": 2.185981273651123,
      "learning_rate": 3.8069444444444447e-06,
      "loss": 0.0949,
      "step": 5260
    },
    {
      "epoch": 17.96225207933461,
      "grad_norm": 2.114521026611328,
      "learning_rate": 3.779166666666667e-06,
      "loss": 0.0952,
      "step": 5280
    },
    {
      "epoch": 18.02729793132864,
      "grad_norm": 2.3519794940948486,
      "learning_rate": 3.751388888888889e-06,
      "loss": 0.104,
      "step": 5300
    },
    {
      "epoch": 18.095542759650247,
      "grad_norm": 1.976405143737793,
      "learning_rate": 3.7236111111111112e-06,
      "loss": 0.0901,
      "step": 5320
    },
    {
      "epoch": 18.16378758797185,
      "grad_norm": 2.1933436393737793,
      "learning_rate": 3.6958333333333337e-06,
      "loss": 0.0872,
      "step": 5340
    },
    {
      "epoch": 18.232032416293453,
      "grad_norm": 2.2043490409851074,
      "learning_rate": 3.6680555555555557e-06,
      "loss": 0.0914,
      "step": 5360
    },
    {
      "epoch": 18.300277244615057,
      "grad_norm": 1.7206165790557861,
      "learning_rate": 3.640277777777778e-06,
      "loss": 0.0934,
      "step": 5380
    },
    {
      "epoch": 18.36852207293666,
      "grad_norm": 2.916863203048706,
      "learning_rate": 3.6125000000000006e-06,
      "loss": 0.0894,
      "step": 5400
    },
    {
      "epoch": 18.436766901258263,
      "grad_norm": 2.5308637619018555,
      "learning_rate": 3.5847222222222226e-06,
      "loss": 0.0876,
      "step": 5420
    },
    {
      "epoch": 18.505011729579866,
      "grad_norm": 2.3941245079040527,
      "learning_rate": 3.5569444444444446e-06,
      "loss": 0.0894,
      "step": 5440
    },
    {
      "epoch": 18.573256557901473,
      "grad_norm": 2.0861077308654785,
      "learning_rate": 3.529166666666667e-06,
      "loss": 0.0916,
      "step": 5460
    },
    {
      "epoch": 18.641501386223077,
      "grad_norm": 2.3388617038726807,
      "learning_rate": 3.501388888888889e-06,
      "loss": 0.0911,
      "step": 5480
    },
    {
      "epoch": 18.70974621454468,
      "grad_norm": 2.1121814250946045,
      "learning_rate": 3.4736111111111115e-06,
      "loss": 0.0912,
      "step": 5500
    },
    {
      "epoch": 18.70974621454468,
      "eval_accuracy": 0.8749374687343672,
      "eval_loss": 0.4303681254386902,
      "eval_macro_f1": 0.8582983438698686,
      "eval_macro_precision": 0.8582983438698686,
      "eval_macro_recall": 0.8582983438698686,
      "eval_runtime": 0.7746,
      "eval_samples_per_second": 2580.736,
      "eval_steps_per_second": 81.334,
      "step": 5500
    },
    {
      "epoch": 18.777991042866283,
      "grad_norm": 2.3054583072662354,
      "learning_rate": 3.445833333333334e-06,
      "loss": 0.0909,
      "step": 5520
    },
    {
      "epoch": 18.846235871187886,
      "grad_norm": 2.0465333461761475,
      "learning_rate": 3.4180555555555555e-06,
      "loss": 0.0919,
      "step": 5540
    },
    {
      "epoch": 18.91448069950949,
      "grad_norm": 1.6204373836517334,
      "learning_rate": 3.390277777777778e-06,
      "loss": 0.0967,
      "step": 5560
    },
    {
      "epoch": 18.982725527831093,
      "grad_norm": 2.4373621940612793,
      "learning_rate": 3.3625000000000004e-06,
      "loss": 0.0883,
      "step": 5580
    },
    {
      "epoch": 19.04777137982512,
      "grad_norm": 1.8789167404174805,
      "learning_rate": 3.3347222222222224e-06,
      "loss": 0.0855,
      "step": 5600
    },
    {
      "epoch": 19.116016208146725,
      "grad_norm": 2.7606985569000244,
      "learning_rate": 3.306944444444445e-06,
      "loss": 0.0846,
      "step": 5620
    },
    {
      "epoch": 19.18426103646833,
      "grad_norm": 2.5193989276885986,
      "learning_rate": 3.2791666666666673e-06,
      "loss": 0.0853,
      "step": 5640
    },
    {
      "epoch": 19.252505864789935,
      "grad_norm": 2.345078706741333,
      "learning_rate": 3.251388888888889e-06,
      "loss": 0.0829,
      "step": 5660
    },
    {
      "epoch": 19.32075069311154,
      "grad_norm": 1.7261122465133667,
      "learning_rate": 3.2236111111111114e-06,
      "loss": 0.0845,
      "step": 5680
    },
    {
      "epoch": 19.38899552143314,
      "grad_norm": 1.9967937469482422,
      "learning_rate": 3.1958333333333334e-06,
      "loss": 0.0847,
      "step": 5700
    },
    {
      "epoch": 19.457240349754745,
      "grad_norm": 2.2386457920074463,
      "learning_rate": 3.168055555555556e-06,
      "loss": 0.0855,
      "step": 5720
    },
    {
      "epoch": 19.525485178076348,
      "grad_norm": 2.04972505569458,
      "learning_rate": 3.1402777777777783e-06,
      "loss": 0.0834,
      "step": 5740
    },
    {
      "epoch": 19.59373000639795,
      "grad_norm": 2.728586196899414,
      "learning_rate": 3.1125000000000007e-06,
      "loss": 0.0874,
      "step": 5760
    },
    {
      "epoch": 19.661974834719558,
      "grad_norm": 2.307673931121826,
      "learning_rate": 3.0847222222222223e-06,
      "loss": 0.0818,
      "step": 5780
    },
    {
      "epoch": 19.73021966304116,
      "grad_norm": 2.2115602493286133,
      "learning_rate": 3.0569444444444447e-06,
      "loss": 0.0881,
      "step": 5800
    },
    {
      "epoch": 19.798464491362765,
      "grad_norm": 1.7153595685958862,
      "learning_rate": 3.0291666666666667e-06,
      "loss": 0.0885,
      "step": 5820
    },
    {
      "epoch": 19.866709319684368,
      "grad_norm": 2.201068878173828,
      "learning_rate": 3.001388888888889e-06,
      "loss": 0.0905,
      "step": 5840
    },
    {
      "epoch": 19.93495414800597,
      "grad_norm": 2.249911308288574,
      "learning_rate": 2.9736111111111116e-06,
      "loss": 0.0878,
      "step": 5860
    },
    {
      "epoch": 20.0,
      "grad_norm": 7.2117204666137695,
      "learning_rate": 2.9458333333333332e-06,
      "loss": 0.0825,
      "step": 5880
    },
    {
      "epoch": 20.068244828321603,
      "grad_norm": 2.2923543453216553,
      "learning_rate": 2.9180555555555557e-06,
      "loss": 0.0796,
      "step": 5900
    },
    {
      "epoch": 20.136489656643207,
      "grad_norm": 2.0793557167053223,
      "learning_rate": 2.890277777777778e-06,
      "loss": 0.082,
      "step": 5920
    },
    {
      "epoch": 20.20473448496481,
      "grad_norm": 1.706006646156311,
      "learning_rate": 2.8625e-06,
      "loss": 0.0824,
      "step": 5940
    },
    {
      "epoch": 20.272979313286417,
      "grad_norm": 1.8527004718780518,
      "learning_rate": 2.8347222222222226e-06,
      "loss": 0.0775,
      "step": 5960
    },
    {
      "epoch": 20.34122414160802,
      "grad_norm": 2.893237352371216,
      "learning_rate": 2.806944444444445e-06,
      "loss": 0.0828,
      "step": 5980
    },
    {
      "epoch": 20.409468969929623,
      "grad_norm": 2.0376296043395996,
      "learning_rate": 2.7791666666666666e-06,
      "loss": 0.0783,
      "step": 6000
    },
    {
      "epoch": 20.409468969929623,
      "eval_accuracy": 0.8779389694847424,
      "eval_loss": 0.45369774103164673,
      "eval_macro_f1": 0.8616991836169918,
      "eval_macro_precision": 0.8616991836169918,
      "eval_macro_recall": 0.8616991836169918,
      "eval_runtime": 0.7669,
      "eval_samples_per_second": 2606.655,
      "eval_steps_per_second": 82.151,
      "step": 6000
    }
  ],
  "logging_steps": 20,
  "max_steps": 8000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 28,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.4920353685472323e+17,
  "train_batch_size": 64,
  "trial_name": null,
  "trial_params": null
}
