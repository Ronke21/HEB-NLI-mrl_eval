{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.18662498708621741,
  "eval_steps": 30946,
  "global_step": 3500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.001066428497635528,
      "grad_norm": 30.40918731689453,
      "learning_rate": 2.026018340797612e-08,
      "loss": 0.9037,
      "step": 20
    },
    {
      "epoch": 0.002132856995271056,
      "grad_norm": 34.81815719604492,
      "learning_rate": 4.1586692258477293e-08,
      "loss": 0.8018,
      "step": 40
    },
    {
      "epoch": 0.003199285492906584,
      "grad_norm": 109.52640533447266,
      "learning_rate": 6.291320110897847e-08,
      "loss": 0.8566,
      "step": 60
    },
    {
      "epoch": 0.004265713990542112,
      "grad_norm": 208.7018585205078,
      "learning_rate": 8.423970995947965e-08,
      "loss": 0.562,
      "step": 80
    },
    {
      "epoch": 0.0053321424881776405,
      "grad_norm": 1.0278514623641968,
      "learning_rate": 1.0556621880998082e-07,
      "loss": 0.9315,
      "step": 100
    },
    {
      "epoch": 0.006398570985813168,
      "grad_norm": 0.9893989562988281,
      "learning_rate": 1.26892727660482e-07,
      "loss": 0.6243,
      "step": 120
    },
    {
      "epoch": 0.007464999483448697,
      "grad_norm": 21.48494529724121,
      "learning_rate": 1.4821923651098315e-07,
      "loss": 0.5805,
      "step": 140
    },
    {
      "epoch": 0.008531427981084224,
      "grad_norm": 71.69548797607422,
      "learning_rate": 1.6954574536148434e-07,
      "loss": 0.5529,
      "step": 160
    },
    {
      "epoch": 0.009597856478719753,
      "grad_norm": 5.374662399291992,
      "learning_rate": 1.9087225421198552e-07,
      "loss": 0.4533,
      "step": 180
    },
    {
      "epoch": 0.010664284976355281,
      "grad_norm": 106.09730529785156,
      "learning_rate": 2.121987630624867e-07,
      "loss": 0.7033,
      "step": 200
    },
    {
      "epoch": 0.011730713473990809,
      "grad_norm": 218.14666748046875,
      "learning_rate": 2.3352527191298785e-07,
      "loss": 1.4538,
      "step": 220
    },
    {
      "epoch": 0.012797141971626336,
      "grad_norm": 8.822221755981445,
      "learning_rate": 2.54851780763489e-07,
      "loss": 0.5462,
      "step": 240
    },
    {
      "epoch": 0.013863570469261864,
      "grad_norm": 4.810640811920166,
      "learning_rate": 2.761782896139902e-07,
      "loss": 0.9446,
      "step": 260
    },
    {
      "epoch": 0.014929998966897394,
      "grad_norm": 124.28128051757812,
      "learning_rate": 2.9750479846449137e-07,
      "loss": 0.5685,
      "step": 280
    },
    {
      "epoch": 0.01599642746453292,
      "grad_norm": 355.0894775390625,
      "learning_rate": 3.1883130731499253e-07,
      "loss": 0.942,
      "step": 300
    },
    {
      "epoch": 0.017062855962168447,
      "grad_norm": 104.49898529052734,
      "learning_rate": 3.4015781616549374e-07,
      "loss": 0.9466,
      "step": 320
    },
    {
      "epoch": 0.01812928445980398,
      "grad_norm": 63.741050720214844,
      "learning_rate": 3.614843250159949e-07,
      "loss": 0.9225,
      "step": 340
    },
    {
      "epoch": 0.019195712957439506,
      "grad_norm": 126.21836853027344,
      "learning_rate": 3.828108338664961e-07,
      "loss": 0.7998,
      "step": 360
    },
    {
      "epoch": 0.020262141455075034,
      "grad_norm": 56.54940414428711,
      "learning_rate": 4.041373427169973e-07,
      "loss": 0.9987,
      "step": 380
    },
    {
      "epoch": 0.021328569952710562,
      "grad_norm": 0.9375487565994263,
      "learning_rate": 4.2546385156749846e-07,
      "loss": 0.7058,
      "step": 400
    },
    {
      "epoch": 0.02239499845034609,
      "grad_norm": 72.61161041259766,
      "learning_rate": 4.467903604179996e-07,
      "loss": 0.95,
      "step": 420
    },
    {
      "epoch": 0.023461426947981617,
      "grad_norm": 54.07810974121094,
      "learning_rate": 4.6811686926850077e-07,
      "loss": 0.6742,
      "step": 440
    },
    {
      "epoch": 0.024527855445617145,
      "grad_norm": 95.14642333984375,
      "learning_rate": 4.894433781190019e-07,
      "loss": 0.8159,
      "step": 460
    },
    {
      "epoch": 0.025594283943252673,
      "grad_norm": 0.8117004036903381,
      "learning_rate": 5.107698869695031e-07,
      "loss": 0.3574,
      "step": 480
    },
    {
      "epoch": 0.0266607124408882,
      "grad_norm": 0.6489806175231934,
      "learning_rate": 5.320963958200042e-07,
      "loss": 0.3377,
      "step": 500
    },
    {
      "epoch": 0.02772714093852373,
      "grad_norm": 46.145809173583984,
      "learning_rate": 5.534229046705055e-07,
      "loss": 0.4489,
      "step": 520
    },
    {
      "epoch": 0.028793569436159256,
      "grad_norm": 0.5945598483085632,
      "learning_rate": 5.747494135210067e-07,
      "loss": 0.603,
      "step": 540
    },
    {
      "epoch": 0.029859997933794787,
      "grad_norm": 1.5459638833999634,
      "learning_rate": 5.960759223715078e-07,
      "loss": 0.5942,
      "step": 560
    },
    {
      "epoch": 0.030926426431430315,
      "grad_norm": 145.26951599121094,
      "learning_rate": 6.17402431222009e-07,
      "loss": 0.622,
      "step": 580
    },
    {
      "epoch": 0.03199285492906584,
      "grad_norm": 0.2774520814418793,
      "learning_rate": 6.387289400725102e-07,
      "loss": 0.4477,
      "step": 600
    },
    {
      "epoch": 0.03305928342670137,
      "grad_norm": 0.2679499685764313,
      "learning_rate": 6.600554489230114e-07,
      "loss": 0.3415,
      "step": 620
    },
    {
      "epoch": 0.034125711924336895,
      "grad_norm": 0.11070234328508377,
      "learning_rate": 6.813819577735125e-07,
      "loss": 1.1531,
      "step": 640
    },
    {
      "epoch": 0.035192140421972426,
      "grad_norm": 0.07349901646375656,
      "learning_rate": 7.027084666240137e-07,
      "loss": 0.8486,
      "step": 660
    },
    {
      "epoch": 0.03625856891960796,
      "grad_norm": 241.1537628173828,
      "learning_rate": 7.24034975474515e-07,
      "loss": 0.963,
      "step": 680
    },
    {
      "epoch": 0.03732499741724348,
      "grad_norm": 0.14107745885849,
      "learning_rate": 7.453614843250161e-07,
      "loss": 1.0172,
      "step": 700
    },
    {
      "epoch": 0.03839142591487901,
      "grad_norm": 1.9457520246505737,
      "learning_rate": 7.666879931755173e-07,
      "loss": 1.2768,
      "step": 720
    },
    {
      "epoch": 0.03945785441251454,
      "grad_norm": 0.9561519622802734,
      "learning_rate": 7.880145020260184e-07,
      "loss": 1.6991,
      "step": 740
    },
    {
      "epoch": 0.04052428291015007,
      "grad_norm": 279.87274169921875,
      "learning_rate": 8.093410108765196e-07,
      "loss": 0.6359,
      "step": 760
    },
    {
      "epoch": 0.04159071140778559,
      "grad_norm": 2.1532881259918213,
      "learning_rate": 8.306675197270207e-07,
      "loss": 1.0906,
      "step": 780
    },
    {
      "epoch": 0.042657139905421124,
      "grad_norm": 36.82831954956055,
      "learning_rate": 8.519940285775219e-07,
      "loss": 1.1579,
      "step": 800
    },
    {
      "epoch": 0.04372356840305665,
      "grad_norm": 27.987106323242188,
      "learning_rate": 8.73320537428023e-07,
      "loss": 0.8015,
      "step": 820
    },
    {
      "epoch": 0.04478999690069218,
      "grad_norm": 97.41328430175781,
      "learning_rate": 8.946470462785242e-07,
      "loss": 1.0214,
      "step": 840
    },
    {
      "epoch": 0.0458564253983277,
      "grad_norm": 7.345329761505127,
      "learning_rate": 9.159735551290253e-07,
      "loss": 0.8155,
      "step": 860
    },
    {
      "epoch": 0.046922853895963235,
      "grad_norm": 0.3862553834915161,
      "learning_rate": 9.373000639795267e-07,
      "loss": 0.6191,
      "step": 880
    },
    {
      "epoch": 0.047989282393598766,
      "grad_norm": 2.4141669273376465,
      "learning_rate": 9.586265728300278e-07,
      "loss": 0.9117,
      "step": 900
    },
    {
      "epoch": 0.04905571089123429,
      "grad_norm": 46.78615188598633,
      "learning_rate": 9.79953081680529e-07,
      "loss": 0.8924,
      "step": 920
    },
    {
      "epoch": 0.05012213938886982,
      "grad_norm": 9.07925796508789,
      "learning_rate": 1.0012795905310303e-06,
      "loss": 0.9311,
      "step": 940
    },
    {
      "epoch": 0.051188567886505346,
      "grad_norm": 222.67840576171875,
      "learning_rate": 1.0226060993815313e-06,
      "loss": 1.1955,
      "step": 960
    },
    {
      "epoch": 0.05225499638414088,
      "grad_norm": 4.368932723999023,
      "learning_rate": 1.0439326082320326e-06,
      "loss": 0.8325,
      "step": 980
    },
    {
      "epoch": 0.0533214248817764,
      "grad_norm": 128.329833984375,
      "learning_rate": 1.0652591170825336e-06,
      "loss": 1.0103,
      "step": 1000
    },
    {
      "epoch": 0.05438785337941193,
      "grad_norm": 56.9404296875,
      "learning_rate": 1.086585625933035e-06,
      "loss": 0.6048,
      "step": 1020
    },
    {
      "epoch": 0.05545428187704746,
      "grad_norm": 64.66481018066406,
      "learning_rate": 1.107912134783536e-06,
      "loss": 0.8561,
      "step": 1040
    },
    {
      "epoch": 0.05652071037468299,
      "grad_norm": 58.42427062988281,
      "learning_rate": 1.1292386436340372e-06,
      "loss": 0.8371,
      "step": 1060
    },
    {
      "epoch": 0.05758713887231851,
      "grad_norm": 0.09673850238323212,
      "learning_rate": 1.1505651524845383e-06,
      "loss": 0.703,
      "step": 1080
    },
    {
      "epoch": 0.05865356736995404,
      "grad_norm": 0.33486610651016235,
      "learning_rate": 1.1718916613350395e-06,
      "loss": 1.0851,
      "step": 1100
    },
    {
      "epoch": 0.059719995867589575,
      "grad_norm": 174.9239501953125,
      "learning_rate": 1.1932181701855406e-06,
      "loss": 0.923,
      "step": 1120
    },
    {
      "epoch": 0.0607864243652251,
      "grad_norm": 0.1902531534433365,
      "learning_rate": 1.2145446790360418e-06,
      "loss": 0.6862,
      "step": 1140
    },
    {
      "epoch": 0.06185285286286063,
      "grad_norm": 0.19022899866104126,
      "learning_rate": 1.235871187886543e-06,
      "loss": 0.5048,
      "step": 1160
    },
    {
      "epoch": 0.06291928136049615,
      "grad_norm": 71.72462463378906,
      "learning_rate": 1.2571976967370441e-06,
      "loss": 0.6125,
      "step": 1180
    },
    {
      "epoch": 0.06398570985813168,
      "grad_norm": 45.488792419433594,
      "learning_rate": 1.2785242055875454e-06,
      "loss": 1.182,
      "step": 1200
    },
    {
      "epoch": 0.06505213835576722,
      "grad_norm": 255.39642333984375,
      "learning_rate": 1.2998507144380465e-06,
      "loss": 1.2173,
      "step": 1220
    },
    {
      "epoch": 0.06611856685340274,
      "grad_norm": 6.017702579498291,
      "learning_rate": 1.3211772232885477e-06,
      "loss": 0.6693,
      "step": 1240
    },
    {
      "epoch": 0.06718499535103827,
      "grad_norm": 49.11395263671875,
      "learning_rate": 1.3425037321390488e-06,
      "loss": 1.3625,
      "step": 1260
    },
    {
      "epoch": 0.06825142384867379,
      "grad_norm": 36.44204330444336,
      "learning_rate": 1.3638302409895502e-06,
      "loss": 1.6365,
      "step": 1280
    },
    {
      "epoch": 0.06931785234630933,
      "grad_norm": 1.0080350637435913,
      "learning_rate": 1.385156749840051e-06,
      "loss": 0.549,
      "step": 1300
    },
    {
      "epoch": 0.07038428084394485,
      "grad_norm": 1.9425398111343384,
      "learning_rate": 1.4064832586905525e-06,
      "loss": 0.9946,
      "step": 1320
    },
    {
      "epoch": 0.07145070934158038,
      "grad_norm": 0.5348049402236938,
      "learning_rate": 1.4278097675410538e-06,
      "loss": 1.2629,
      "step": 1340
    },
    {
      "epoch": 0.07251713783921591,
      "grad_norm": 51.698822021484375,
      "learning_rate": 1.4491362763915549e-06,
      "loss": 0.943,
      "step": 1360
    },
    {
      "epoch": 0.07358356633685144,
      "grad_norm": 214.35865783691406,
      "learning_rate": 1.4704627852420561e-06,
      "loss": 0.8472,
      "step": 1380
    },
    {
      "epoch": 0.07464999483448696,
      "grad_norm": 79.52757263183594,
      "learning_rate": 1.4917892940925572e-06,
      "loss": 0.9166,
      "step": 1400
    },
    {
      "epoch": 0.07571642333212249,
      "grad_norm": 0.13550186157226562,
      "learning_rate": 1.5131158029430584e-06,
      "loss": 0.8156,
      "step": 1420
    },
    {
      "epoch": 0.07678285182975803,
      "grad_norm": 0.4582098126411438,
      "learning_rate": 1.5344423117935595e-06,
      "loss": 0.7049,
      "step": 1440
    },
    {
      "epoch": 0.07784928032739355,
      "grad_norm": 0.18081125617027283,
      "learning_rate": 1.5557688206440607e-06,
      "loss": 1.5018,
      "step": 1460
    },
    {
      "epoch": 0.07891570882502907,
      "grad_norm": 326.43310546875,
      "learning_rate": 1.5770953294945618e-06,
      "loss": 0.8153,
      "step": 1480
    },
    {
      "epoch": 0.0799821373226646,
      "grad_norm": 49.778995513916016,
      "learning_rate": 1.598421838345063e-06,
      "loss": 0.7269,
      "step": 1500
    },
    {
      "epoch": 0.08104856582030014,
      "grad_norm": 41.65342330932617,
      "learning_rate": 1.619748347195564e-06,
      "loss": 0.7927,
      "step": 1520
    },
    {
      "epoch": 0.08211499431793566,
      "grad_norm": 16.264455795288086,
      "learning_rate": 1.6410748560460654e-06,
      "loss": 0.9105,
      "step": 1540
    },
    {
      "epoch": 0.08318142281557118,
      "grad_norm": 404.638916015625,
      "learning_rate": 1.6624013648965664e-06,
      "loss": 0.9903,
      "step": 1560
    },
    {
      "epoch": 0.08424785131320672,
      "grad_norm": 247.65115356445312,
      "learning_rate": 1.6837278737470677e-06,
      "loss": 0.955,
      "step": 1580
    },
    {
      "epoch": 0.08531427981084225,
      "grad_norm": 0.78867506980896,
      "learning_rate": 1.7050543825975687e-06,
      "loss": 0.8926,
      "step": 1600
    },
    {
      "epoch": 0.08638070830847777,
      "grad_norm": 0.28775957226753235,
      "learning_rate": 1.72638089144807e-06,
      "loss": 0.5711,
      "step": 1620
    },
    {
      "epoch": 0.0874471368061133,
      "grad_norm": 0.15704679489135742,
      "learning_rate": 1.7477074002985715e-06,
      "loss": 0.5368,
      "step": 1640
    },
    {
      "epoch": 0.08851356530374883,
      "grad_norm": 130.4899139404297,
      "learning_rate": 1.7690339091490725e-06,
      "loss": 0.776,
      "step": 1660
    },
    {
      "epoch": 0.08957999380138436,
      "grad_norm": 286.6835021972656,
      "learning_rate": 1.7903604179995738e-06,
      "loss": 0.5385,
      "step": 1680
    },
    {
      "epoch": 0.09064642229901988,
      "grad_norm": 0.35978347063064575,
      "learning_rate": 1.8116869268500748e-06,
      "loss": 1.1085,
      "step": 1700
    },
    {
      "epoch": 0.0917128507966554,
      "grad_norm": 38.315711975097656,
      "learning_rate": 1.833013435700576e-06,
      "loss": 0.9938,
      "step": 1720
    },
    {
      "epoch": 0.09277927929429095,
      "grad_norm": 315.253662109375,
      "learning_rate": 1.8543399445510771e-06,
      "loss": 1.2743,
      "step": 1740
    },
    {
      "epoch": 0.09384570779192647,
      "grad_norm": 355.28570556640625,
      "learning_rate": 1.8756664534015784e-06,
      "loss": 0.7451,
      "step": 1760
    },
    {
      "epoch": 0.094912136289562,
      "grad_norm": 56.41270446777344,
      "learning_rate": 1.8969929622520794e-06,
      "loss": 0.4261,
      "step": 1780
    },
    {
      "epoch": 0.09597856478719753,
      "grad_norm": 0.05293414369225502,
      "learning_rate": 1.9183194711025807e-06,
      "loss": 0.7719,
      "step": 1800
    },
    {
      "epoch": 0.09704499328483306,
      "grad_norm": 67.33733367919922,
      "learning_rate": 1.9396459799530815e-06,
      "loss": 0.8852,
      "step": 1820
    },
    {
      "epoch": 0.09811142178246858,
      "grad_norm": 37.68648147583008,
      "learning_rate": 1.9609724888035832e-06,
      "loss": 1.5134,
      "step": 1840
    },
    {
      "epoch": 0.0991778502801041,
      "grad_norm": 287.6117858886719,
      "learning_rate": 1.982298997654084e-06,
      "loss": 0.3327,
      "step": 1860
    },
    {
      "epoch": 0.10024427877773964,
      "grad_norm": 0.36448565125465393,
      "learning_rate": 2.0036255065045853e-06,
      "loss": 0.4681,
      "step": 1880
    },
    {
      "epoch": 0.10131070727537517,
      "grad_norm": 591.4154663085938,
      "learning_rate": 2.0249520153550866e-06,
      "loss": 0.5941,
      "step": 1900
    },
    {
      "epoch": 0.10237713577301069,
      "grad_norm": 122.05184936523438,
      "learning_rate": 2.046278524205588e-06,
      "loss": 1.3267,
      "step": 1920
    },
    {
      "epoch": 0.10344356427064622,
      "grad_norm": 66.90589141845703,
      "learning_rate": 2.067605033056089e-06,
      "loss": 0.7964,
      "step": 1940
    },
    {
      "epoch": 0.10450999276828175,
      "grad_norm": 0.49185705184936523,
      "learning_rate": 2.08893154190659e-06,
      "loss": 0.6923,
      "step": 1960
    },
    {
      "epoch": 0.10557642126591728,
      "grad_norm": 1.1245324611663818,
      "learning_rate": 2.110258050757091e-06,
      "loss": 0.4054,
      "step": 1980
    },
    {
      "epoch": 0.1066428497635528,
      "grad_norm": 287.3852844238281,
      "learning_rate": 2.1315845596075925e-06,
      "loss": 0.8309,
      "step": 2000
    },
    {
      "epoch": 0.10770927826118834,
      "grad_norm": 0.29090890288352966,
      "learning_rate": 2.1529110684580937e-06,
      "loss": 0.7729,
      "step": 2020
    },
    {
      "epoch": 0.10877570675882386,
      "grad_norm": 14.066157341003418,
      "learning_rate": 2.1742375773085946e-06,
      "loss": 0.6652,
      "step": 2040
    },
    {
      "epoch": 0.10984213525645939,
      "grad_norm": 10.301706314086914,
      "learning_rate": 2.195564086159096e-06,
      "loss": 0.5725,
      "step": 2060
    },
    {
      "epoch": 0.11090856375409491,
      "grad_norm": 47.83635711669922,
      "learning_rate": 2.216890595009597e-06,
      "loss": 0.9766,
      "step": 2080
    },
    {
      "epoch": 0.11197499225173045,
      "grad_norm": 375.29949951171875,
      "learning_rate": 2.2382171038600983e-06,
      "loss": 0.7419,
      "step": 2100
    },
    {
      "epoch": 0.11304142074936598,
      "grad_norm": 316.3367004394531,
      "learning_rate": 2.259543612710599e-06,
      "loss": 0.8365,
      "step": 2120
    },
    {
      "epoch": 0.1141078492470015,
      "grad_norm": 0.1669401079416275,
      "learning_rate": 2.280870121561101e-06,
      "loss": 0.6815,
      "step": 2140
    },
    {
      "epoch": 0.11517427774463702,
      "grad_norm": 203.5036163330078,
      "learning_rate": 2.3021966304116017e-06,
      "loss": 0.8191,
      "step": 2160
    },
    {
      "epoch": 0.11624070624227256,
      "grad_norm": 299.71759033203125,
      "learning_rate": 2.323523139262103e-06,
      "loss": 0.8303,
      "step": 2180
    },
    {
      "epoch": 0.11730713473990809,
      "grad_norm": 1.356423020362854,
      "learning_rate": 2.3448496481126042e-06,
      "loss": 0.857,
      "step": 2200
    },
    {
      "epoch": 0.11837356323754361,
      "grad_norm": 0.2641802132129669,
      "learning_rate": 2.3661761569631055e-06,
      "loss": 0.914,
      "step": 2220
    },
    {
      "epoch": 0.11943999173517915,
      "grad_norm": 0.5557299256324768,
      "learning_rate": 2.3875026658136067e-06,
      "loss": 1.1113,
      "step": 2240
    },
    {
      "epoch": 0.12050642023281467,
      "grad_norm": 117.99295043945312,
      "learning_rate": 2.4088291746641076e-06,
      "loss": 0.51,
      "step": 2260
    },
    {
      "epoch": 0.1215728487304502,
      "grad_norm": 428.1340637207031,
      "learning_rate": 2.430155683514609e-06,
      "loss": 0.4537,
      "step": 2280
    },
    {
      "epoch": 0.12263927722808572,
      "grad_norm": 0.19119508564472198,
      "learning_rate": 2.45148219236511e-06,
      "loss": 0.3617,
      "step": 2300
    },
    {
      "epoch": 0.12370570572572126,
      "grad_norm": 76.41500091552734,
      "learning_rate": 2.4728087012156114e-06,
      "loss": 0.4826,
      "step": 2320
    },
    {
      "epoch": 0.12477213422335678,
      "grad_norm": 0.12022749334573746,
      "learning_rate": 2.494135210066112e-06,
      "loss": 0.7917,
      "step": 2340
    },
    {
      "epoch": 0.1258385627209923,
      "grad_norm": 0.05397162586450577,
      "learning_rate": 2.5154617189166135e-06,
      "loss": 0.4843,
      "step": 2360
    },
    {
      "epoch": 0.12690499121862783,
      "grad_norm": 0.9511826038360596,
      "learning_rate": 2.5367882277671147e-06,
      "loss": 0.5135,
      "step": 2380
    },
    {
      "epoch": 0.12797141971626336,
      "grad_norm": 0.21982428431510925,
      "learning_rate": 2.5581147366176156e-06,
      "loss": 0.6033,
      "step": 2400
    },
    {
      "epoch": 0.1290378482138989,
      "grad_norm": 198.4531707763672,
      "learning_rate": 2.5794412454681172e-06,
      "loss": 1.1036,
      "step": 2420
    },
    {
      "epoch": 0.13010427671153443,
      "grad_norm": 588.6286010742188,
      "learning_rate": 2.600767754318618e-06,
      "loss": 0.6165,
      "step": 2440
    },
    {
      "epoch": 0.13117070520916996,
      "grad_norm": 110.68282318115234,
      "learning_rate": 2.6220942631691193e-06,
      "loss": 0.655,
      "step": 2460
    },
    {
      "epoch": 0.13223713370680548,
      "grad_norm": 0.12141268700361252,
      "learning_rate": 2.64342077201962e-06,
      "loss": 0.5913,
      "step": 2480
    },
    {
      "epoch": 0.133303562204441,
      "grad_norm": 232.1306610107422,
      "learning_rate": 2.664747280870122e-06,
      "loss": 0.5912,
      "step": 2500
    },
    {
      "epoch": 0.13436999070207653,
      "grad_norm": 15.6192045211792,
      "learning_rate": 2.686073789720623e-06,
      "loss": 0.7468,
      "step": 2520
    },
    {
      "epoch": 0.13543641919971205,
      "grad_norm": 0.306551456451416,
      "learning_rate": 2.707400298571124e-06,
      "loss": 0.9921,
      "step": 2540
    },
    {
      "epoch": 0.13650284769734758,
      "grad_norm": 1.6982955932617188,
      "learning_rate": 2.7287268074216256e-06,
      "loss": 0.7877,
      "step": 2560
    },
    {
      "epoch": 0.13756927619498313,
      "grad_norm": 194.15554809570312,
      "learning_rate": 2.7500533162721265e-06,
      "loss": 1.1813,
      "step": 2580
    },
    {
      "epoch": 0.13863570469261866,
      "grad_norm": 0.15230998396873474,
      "learning_rate": 2.7713798251226277e-06,
      "loss": 0.4334,
      "step": 2600
    },
    {
      "epoch": 0.13970213319025418,
      "grad_norm": 0.15226925909519196,
      "learning_rate": 2.7927063339731286e-06,
      "loss": 0.7646,
      "step": 2620
    },
    {
      "epoch": 0.1407685616878897,
      "grad_norm": 0.49922624230384827,
      "learning_rate": 2.8140328428236303e-06,
      "loss": 0.713,
      "step": 2640
    },
    {
      "epoch": 0.14183499018552523,
      "grad_norm": 32.9724006652832,
      "learning_rate": 2.835359351674131e-06,
      "loss": 0.2488,
      "step": 2660
    },
    {
      "epoch": 0.14290141868316075,
      "grad_norm": 1.2052640914916992,
      "learning_rate": 2.8566858605246324e-06,
      "loss": 0.8299,
      "step": 2680
    },
    {
      "epoch": 0.14396784718079628,
      "grad_norm": 1.2993528842926025,
      "learning_rate": 2.878012369375133e-06,
      "loss": 0.6298,
      "step": 2700
    },
    {
      "epoch": 0.14503427567843183,
      "grad_norm": 48.506874084472656,
      "learning_rate": 2.899338878225635e-06,
      "loss": 0.6645,
      "step": 2720
    },
    {
      "epoch": 0.14610070417606735,
      "grad_norm": 111.55780029296875,
      "learning_rate": 2.9206653870761357e-06,
      "loss": 0.6656,
      "step": 2740
    },
    {
      "epoch": 0.14716713267370288,
      "grad_norm": 0.07626975327730179,
      "learning_rate": 2.941991895926637e-06,
      "loss": 0.4356,
      "step": 2760
    },
    {
      "epoch": 0.1482335611713384,
      "grad_norm": 618.4431762695312,
      "learning_rate": 2.9633184047771387e-06,
      "loss": 1.0232,
      "step": 2780
    },
    {
      "epoch": 0.14929998966897393,
      "grad_norm": 0.170592799782753,
      "learning_rate": 2.9846449136276395e-06,
      "loss": 0.5706,
      "step": 2800
    },
    {
      "epoch": 0.15036641816660945,
      "grad_norm": 48.28886032104492,
      "learning_rate": 3.0059714224781403e-06,
      "loss": 0.182,
      "step": 2820
    },
    {
      "epoch": 0.15143284666424497,
      "grad_norm": 1.1452323198318481,
      "learning_rate": 3.0272979313286416e-06,
      "loss": 0.7051,
      "step": 2840
    },
    {
      "epoch": 0.15249927516188053,
      "grad_norm": 1.2629674673080444,
      "learning_rate": 3.0486244401791433e-06,
      "loss": 0.3522,
      "step": 2860
    },
    {
      "epoch": 0.15356570365951605,
      "grad_norm": 242.2732391357422,
      "learning_rate": 3.069950949029644e-06,
      "loss": 0.2901,
      "step": 2880
    },
    {
      "epoch": 0.15463213215715158,
      "grad_norm": 611.2373657226562,
      "learning_rate": 3.0912774578801454e-06,
      "loss": 0.7173,
      "step": 2900
    },
    {
      "epoch": 0.1556985606547871,
      "grad_norm": 316.9176940917969,
      "learning_rate": 3.1126039667306462e-06,
      "loss": 0.6943,
      "step": 2920
    },
    {
      "epoch": 0.15676498915242262,
      "grad_norm": 0.03620743006467819,
      "learning_rate": 3.133930475581148e-06,
      "loss": 0.9898,
      "step": 2940
    },
    {
      "epoch": 0.15783141765005815,
      "grad_norm": 0.2525152266025543,
      "learning_rate": 3.1552569844316487e-06,
      "loss": 0.996,
      "step": 2960
    },
    {
      "epoch": 0.15889784614769367,
      "grad_norm": 35.182193756103516,
      "learning_rate": 3.17658349328215e-06,
      "loss": 0.8455,
      "step": 2980
    },
    {
      "epoch": 0.1599642746453292,
      "grad_norm": 0.07487733662128448,
      "learning_rate": 3.197910002132651e-06,
      "loss": 0.7248,
      "step": 3000
    },
    {
      "epoch": 0.16103070314296475,
      "grad_norm": 0.4951907992362976,
      "learning_rate": 3.2192365109831525e-06,
      "loss": 0.399,
      "step": 3020
    },
    {
      "epoch": 0.16209713164060027,
      "grad_norm": 213.32322692871094,
      "learning_rate": 3.2405630198336534e-06,
      "loss": 0.215,
      "step": 3040
    },
    {
      "epoch": 0.1631635601382358,
      "grad_norm": 0.028627492487430573,
      "learning_rate": 3.2618895286841546e-06,
      "loss": 0.8983,
      "step": 3060
    },
    {
      "epoch": 0.16422998863587132,
      "grad_norm": 0.06668464094400406,
      "learning_rate": 3.2832160375346563e-06,
      "loss": 0.7152,
      "step": 3080
    },
    {
      "epoch": 0.16529641713350685,
      "grad_norm": 0.06735475361347198,
      "learning_rate": 3.304542546385157e-06,
      "loss": 0.6553,
      "step": 3100
    },
    {
      "epoch": 0.16636284563114237,
      "grad_norm": 0.08225227147340775,
      "learning_rate": 3.325869055235658e-06,
      "loss": 0.9584,
      "step": 3120
    },
    {
      "epoch": 0.1674292741287779,
      "grad_norm": 63.381839752197266,
      "learning_rate": 3.3471955640861592e-06,
      "loss": 0.7423,
      "step": 3140
    },
    {
      "epoch": 0.16849570262641345,
      "grad_norm": 0.4321822226047516,
      "learning_rate": 3.368522072936661e-06,
      "loss": 0.4306,
      "step": 3160
    },
    {
      "epoch": 0.16956213112404897,
      "grad_norm": 1.0414413213729858,
      "learning_rate": 3.3898485817871618e-06,
      "loss": 0.5236,
      "step": 3180
    },
    {
      "epoch": 0.1706285596216845,
      "grad_norm": 0.3700566589832306,
      "learning_rate": 3.411175090637663e-06,
      "loss": 0.4519,
      "step": 3200
    },
    {
      "epoch": 0.17169498811932002,
      "grad_norm": 0.1953248232603073,
      "learning_rate": 3.432501599488164e-06,
      "loss": 0.228,
      "step": 3220
    },
    {
      "epoch": 0.17276141661695554,
      "grad_norm": 0.24766013026237488,
      "learning_rate": 3.4538281083386656e-06,
      "loss": 0.5313,
      "step": 3240
    },
    {
      "epoch": 0.17382784511459107,
      "grad_norm": 26.711702346801758,
      "learning_rate": 3.4751546171891664e-06,
      "loss": 0.5642,
      "step": 3260
    },
    {
      "epoch": 0.1748942736122266,
      "grad_norm": 0.18033580482006073,
      "learning_rate": 3.4964811260396677e-06,
      "loss": 0.6141,
      "step": 3280
    },
    {
      "epoch": 0.17596070210986214,
      "grad_norm": 0.15596109628677368,
      "learning_rate": 3.5178076348901685e-06,
      "loss": 0.715,
      "step": 3300
    },
    {
      "epoch": 0.17702713060749767,
      "grad_norm": 57.363731384277344,
      "learning_rate": 3.53913414374067e-06,
      "loss": 0.4072,
      "step": 3320
    },
    {
      "epoch": 0.1780935591051332,
      "grad_norm": 0.13092608749866486,
      "learning_rate": 3.560460652591171e-06,
      "loss": 0.5204,
      "step": 3340
    },
    {
      "epoch": 0.17915998760276872,
      "grad_norm": 262.6496276855469,
      "learning_rate": 3.5817871614416723e-06,
      "loss": 0.7654,
      "step": 3360
    },
    {
      "epoch": 0.18022641610040424,
      "grad_norm": 101.399169921875,
      "learning_rate": 3.6031136702921735e-06,
      "loss": 0.9608,
      "step": 3380
    },
    {
      "epoch": 0.18129284459803977,
      "grad_norm": 480.7981262207031,
      "learning_rate": 3.624440179142675e-06,
      "loss": 0.1544,
      "step": 3400
    },
    {
      "epoch": 0.1823592730956753,
      "grad_norm": 0.02776523120701313,
      "learning_rate": 3.6457666879931756e-06,
      "loss": 0.9029,
      "step": 3420
    },
    {
      "epoch": 0.1834257015933108,
      "grad_norm": 0.21173541247844696,
      "learning_rate": 3.667093196843677e-06,
      "loss": 0.4461,
      "step": 3440
    },
    {
      "epoch": 0.18449213009094637,
      "grad_norm": 0.8473488092422485,
      "learning_rate": 3.6884197056941786e-06,
      "loss": 0.8201,
      "step": 3460
    },
    {
      "epoch": 0.1855585585885819,
      "grad_norm": 1.475902795791626,
      "learning_rate": 3.7097462145446794e-06,
      "loss": 0.0572,
      "step": 3480
    },
    {
      "epoch": 0.18662498708621741,
      "grad_norm": 265.4759521484375,
      "learning_rate": 3.7310727233951803e-06,
      "loss": 0.5558,
      "step": 3500
    }
  ],
  "logging_steps": 20,
  "max_steps": 93775,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.1945015167014336e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
