{
  "best_global_step": 2500,
  "best_metric": 0.8356690351361296,
  "best_model_checkpoint": "output/alephbert-base_hebnli/checkpoint-2500",
  "epoch": 8.505011729579868,
  "eval_steps": 500,
  "global_step": 2500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.06824482832160375,
      "grad_norm": 9.729863166809082,
      "learning_rate": 2.3750000000000003e-07,
      "loss": 1.284,
      "step": 20
    },
    {
      "epoch": 0.1364896566432075,
      "grad_norm": 8.49509334564209,
      "learning_rate": 4.875000000000001e-07,
      "loss": 1.2007,
      "step": 40
    },
    {
      "epoch": 0.20473448496481125,
      "grad_norm": 6.0995774269104,
      "learning_rate": 7.375e-07,
      "loss": 1.0544,
      "step": 60
    },
    {
      "epoch": 0.272979313286415,
      "grad_norm": 3.3033695220947266,
      "learning_rate": 9.875e-07,
      "loss": 0.8842,
      "step": 80
    },
    {
      "epoch": 0.3412241416080188,
      "grad_norm": 1.460514783859253,
      "learning_rate": 1.2375e-06,
      "loss": 0.7482,
      "step": 100
    },
    {
      "epoch": 0.4094689699296225,
      "grad_norm": 0.8455040454864502,
      "learning_rate": 1.4875000000000002e-06,
      "loss": 0.6739,
      "step": 120
    },
    {
      "epoch": 0.4777137982512263,
      "grad_norm": 0.625079333782196,
      "learning_rate": 1.7375e-06,
      "loss": 0.6518,
      "step": 140
    },
    {
      "epoch": 0.54595862657283,
      "grad_norm": 0.3774931728839874,
      "learning_rate": 1.9875000000000005e-06,
      "loss": 0.6415,
      "step": 160
    },
    {
      "epoch": 0.6142034548944337,
      "grad_norm": 0.6220096349716187,
      "learning_rate": 2.2375e-06,
      "loss": 0.6315,
      "step": 180
    },
    {
      "epoch": 0.6824482832160376,
      "grad_norm": 0.5839018821716309,
      "learning_rate": 2.4875000000000003e-06,
      "loss": 0.6022,
      "step": 200
    },
    {
      "epoch": 0.7506931115376413,
      "grad_norm": 0.9699823260307312,
      "learning_rate": 2.7375e-06,
      "loss": 0.571,
      "step": 220
    },
    {
      "epoch": 0.818937939859245,
      "grad_norm": 1.0332428216934204,
      "learning_rate": 2.9875e-06,
      "loss": 0.5416,
      "step": 240
    },
    {
      "epoch": 0.8871827681808488,
      "grad_norm": 0.6548230648040771,
      "learning_rate": 3.2375e-06,
      "loss": 0.5278,
      "step": 260
    },
    {
      "epoch": 0.9554275965024526,
      "grad_norm": 0.641913652420044,
      "learning_rate": 3.4875000000000005e-06,
      "loss": 0.5244,
      "step": 280
    },
    {
      "epoch": 1.020473448496481,
      "grad_norm": 0.7721569538116455,
      "learning_rate": 3.7375000000000006e-06,
      "loss": 0.5151,
      "step": 300
    },
    {
      "epoch": 1.088718276818085,
      "grad_norm": 1.0604685544967651,
      "learning_rate": 3.9875e-06,
      "loss": 0.5027,
      "step": 320
    },
    {
      "epoch": 1.1569631051396887,
      "grad_norm": 0.903929591178894,
      "learning_rate": 4.2375000000000005e-06,
      "loss": 0.4964,
      "step": 340
    },
    {
      "epoch": 1.2252079334612924,
      "grad_norm": 1.0833895206451416,
      "learning_rate": 4.4875e-06,
      "loss": 0.4906,
      "step": 360
    },
    {
      "epoch": 1.2934527617828961,
      "grad_norm": 1.6224653720855713,
      "learning_rate": 4.737500000000001e-06,
      "loss": 0.4915,
      "step": 380
    },
    {
      "epoch": 1.3616975901044999,
      "grad_norm": 0.8425558805465698,
      "learning_rate": 4.987500000000001e-06,
      "loss": 0.4791,
      "step": 400
    },
    {
      "epoch": 1.4299424184261036,
      "grad_norm": 0.9181768298149109,
      "learning_rate": 5.237500000000001e-06,
      "loss": 0.4676,
      "step": 420
    },
    {
      "epoch": 1.4981872467477073,
      "grad_norm": 0.9230828881263733,
      "learning_rate": 5.4875e-06,
      "loss": 0.4648,
      "step": 440
    },
    {
      "epoch": 1.566432075069311,
      "grad_norm": 1.1551607847213745,
      "learning_rate": 5.7375000000000005e-06,
      "loss": 0.467,
      "step": 460
    },
    {
      "epoch": 1.6346769033909148,
      "grad_norm": 1.032691478729248,
      "learning_rate": 5.987500000000001e-06,
      "loss": 0.4598,
      "step": 480
    },
    {
      "epoch": 1.7029217317125187,
      "grad_norm": 0.9711072444915771,
      "learning_rate": 6.237500000000001e-06,
      "loss": 0.4516,
      "step": 500
    },
    {
      "epoch": 1.7029217317125187,
      "eval_accuracy": 0.80040020010005,
      "eval_loss": 0.43356233835220337,
      "eval_macro_f1": 0.7597816714488406,
      "eval_macro_precision": 0.7834367035939044,
      "eval_macro_recall": 0.7472348683330045,
      "eval_runtime": 0.8447,
      "eval_samples_per_second": 2366.635,
      "eval_steps_per_second": 74.586,
      "step": 500
    },
    {
      "epoch": 1.7711665600341224,
      "grad_norm": 1.3767296075820923,
      "learning_rate": 6.487500000000001e-06,
      "loss": 0.4508,
      "step": 520
    },
    {
      "epoch": 1.8394113883557262,
      "grad_norm": 0.9155921936035156,
      "learning_rate": 6.7375e-06,
      "loss": 0.4496,
      "step": 540
    },
    {
      "epoch": 1.9076562166773299,
      "grad_norm": 1.8212809562683105,
      "learning_rate": 6.9875000000000004e-06,
      "loss": 0.4422,
      "step": 560
    },
    {
      "epoch": 1.9759010449989338,
      "grad_norm": 1.154582142829895,
      "learning_rate": 7.237500000000001e-06,
      "loss": 0.4429,
      "step": 580
    },
    {
      "epoch": 2.040946896992962,
      "grad_norm": 1.18726646900177,
      "learning_rate": 7.487500000000001e-06,
      "loss": 0.4338,
      "step": 600
    },
    {
      "epoch": 2.109191725314566,
      "grad_norm": 1.6257109642028809,
      "learning_rate": 7.737500000000002e-06,
      "loss": 0.4216,
      "step": 620
    },
    {
      "epoch": 2.17743655363617,
      "grad_norm": 1.1696633100509644,
      "learning_rate": 7.987500000000001e-06,
      "loss": 0.4246,
      "step": 640
    },
    {
      "epoch": 2.2456813819577737,
      "grad_norm": 1.0642712116241455,
      "learning_rate": 8.2375e-06,
      "loss": 0.4192,
      "step": 660
    },
    {
      "epoch": 2.3139262102793774,
      "grad_norm": 0.9566589593887329,
      "learning_rate": 8.487500000000001e-06,
      "loss": 0.4277,
      "step": 680
    },
    {
      "epoch": 2.382171038600981,
      "grad_norm": 1.4171574115753174,
      "learning_rate": 8.7375e-06,
      "loss": 0.4197,
      "step": 700
    },
    {
      "epoch": 2.450415866922585,
      "grad_norm": 1.9290822744369507,
      "learning_rate": 8.987500000000002e-06,
      "loss": 0.4194,
      "step": 720
    },
    {
      "epoch": 2.5186606952441886,
      "grad_norm": 0.9441655278205872,
      "learning_rate": 9.237500000000001e-06,
      "loss": 0.4096,
      "step": 740
    },
    {
      "epoch": 2.5869055235657923,
      "grad_norm": 1.1791447401046753,
      "learning_rate": 9.4875e-06,
      "loss": 0.4199,
      "step": 760
    },
    {
      "epoch": 2.655150351887396,
      "grad_norm": 0.9287567138671875,
      "learning_rate": 9.737500000000001e-06,
      "loss": 0.4123,
      "step": 780
    },
    {
      "epoch": 2.7233951802089997,
      "grad_norm": 0.9652462601661682,
      "learning_rate": 9.9875e-06,
      "loss": 0.4045,
      "step": 800
    },
    {
      "epoch": 2.7916400085306035,
      "grad_norm": 1.1288135051727295,
      "learning_rate": 9.973611111111113e-06,
      "loss": 0.4073,
      "step": 820
    },
    {
      "epoch": 2.859884836852207,
      "grad_norm": 1.0588809251785278,
      "learning_rate": 9.945833333333334e-06,
      "loss": 0.4032,
      "step": 840
    },
    {
      "epoch": 2.928129665173811,
      "grad_norm": 1.2118147611618042,
      "learning_rate": 9.918055555555557e-06,
      "loss": 0.4046,
      "step": 860
    },
    {
      "epoch": 2.9963744934954146,
      "grad_norm": 1.2255059480667114,
      "learning_rate": 9.890277777777778e-06,
      "loss": 0.3981,
      "step": 880
    },
    {
      "epoch": 3.0614203454894433,
      "grad_norm": 1.08984375,
      "learning_rate": 9.862500000000001e-06,
      "loss": 0.374,
      "step": 900
    },
    {
      "epoch": 3.129665173811047,
      "grad_norm": 1.9314740896224976,
      "learning_rate": 9.834722222222222e-06,
      "loss": 0.3825,
      "step": 920
    },
    {
      "epoch": 3.1979100021326508,
      "grad_norm": 1.804089903831482,
      "learning_rate": 9.806944444444445e-06,
      "loss": 0.384,
      "step": 940
    },
    {
      "epoch": 3.2661548304542545,
      "grad_norm": 1.1546952724456787,
      "learning_rate": 9.779166666666668e-06,
      "loss": 0.3779,
      "step": 960
    },
    {
      "epoch": 3.334399658775858,
      "grad_norm": 1.25486421585083,
      "learning_rate": 9.751388888888889e-06,
      "loss": 0.377,
      "step": 980
    },
    {
      "epoch": 3.4026444870974624,
      "grad_norm": 1.4006942510604858,
      "learning_rate": 9.723611111111112e-06,
      "loss": 0.38,
      "step": 1000
    },
    {
      "epoch": 3.4026444870974624,
      "eval_accuracy": 0.8309154577288644,
      "eval_loss": 0.37383902072906494,
      "eval_macro_f1": 0.8043165725989565,
      "eval_macro_precision": 0.811875570627364,
      "eval_macro_recall": 0.7983194849913915,
      "eval_runtime": 0.7865,
      "eval_samples_per_second": 2541.602,
      "eval_steps_per_second": 80.101,
      "step": 1000
    },
    {
      "epoch": 3.470889315419066,
      "grad_norm": 1.220974087715149,
      "learning_rate": 9.695833333333335e-06,
      "loss": 0.3748,
      "step": 1020
    },
    {
      "epoch": 3.53913414374067,
      "grad_norm": 1.4286847114562988,
      "learning_rate": 9.668055555555556e-06,
      "loss": 0.3743,
      "step": 1040
    },
    {
      "epoch": 3.6073789720622735,
      "grad_norm": 1.332173466682434,
      "learning_rate": 9.640277777777779e-06,
      "loss": 0.375,
      "step": 1060
    },
    {
      "epoch": 3.6756238003838773,
      "grad_norm": 1.1154954433441162,
      "learning_rate": 9.612500000000002e-06,
      "loss": 0.3797,
      "step": 1080
    },
    {
      "epoch": 3.743868628705481,
      "grad_norm": 1.346835732460022,
      "learning_rate": 9.584722222222223e-06,
      "loss": 0.368,
      "step": 1100
    },
    {
      "epoch": 3.8121134570270847,
      "grad_norm": 1.428462266921997,
      "learning_rate": 9.556944444444446e-06,
      "loss": 0.369,
      "step": 1120
    },
    {
      "epoch": 3.8803582853486884,
      "grad_norm": 1.2679704427719116,
      "learning_rate": 9.529166666666667e-06,
      "loss": 0.3711,
      "step": 1140
    },
    {
      "epoch": 3.948603113670292,
      "grad_norm": 1.102891206741333,
      "learning_rate": 9.50138888888889e-06,
      "loss": 0.3682,
      "step": 1160
    },
    {
      "epoch": 4.013648965664321,
      "grad_norm": 1.4517719745635986,
      "learning_rate": 9.473611111111112e-06,
      "loss": 0.3603,
      "step": 1180
    },
    {
      "epoch": 4.081893793985924,
      "grad_norm": 2.4173099994659424,
      "learning_rate": 9.445833333333334e-06,
      "loss": 0.3552,
      "step": 1200
    },
    {
      "epoch": 4.150138622307528,
      "grad_norm": 1.58118736743927,
      "learning_rate": 9.418055555555557e-06,
      "loss": 0.3446,
      "step": 1220
    },
    {
      "epoch": 4.218383450629132,
      "grad_norm": 1.201060175895691,
      "learning_rate": 9.39027777777778e-06,
      "loss": 0.3525,
      "step": 1240
    },
    {
      "epoch": 4.286628278950736,
      "grad_norm": 2.536003828048706,
      "learning_rate": 9.3625e-06,
      "loss": 0.3486,
      "step": 1260
    },
    {
      "epoch": 4.35487310727234,
      "grad_norm": 1.2804135084152222,
      "learning_rate": 9.334722222222223e-06,
      "loss": 0.3358,
      "step": 1280
    },
    {
      "epoch": 4.423117935593943,
      "grad_norm": 1.2510807514190674,
      "learning_rate": 9.306944444444446e-06,
      "loss": 0.3464,
      "step": 1300
    },
    {
      "epoch": 4.491362763915547,
      "grad_norm": 1.3935731649398804,
      "learning_rate": 9.279166666666667e-06,
      "loss": 0.3443,
      "step": 1320
    },
    {
      "epoch": 4.559607592237151,
      "grad_norm": 1.2277660369873047,
      "learning_rate": 9.251388888888889e-06,
      "loss": 0.3318,
      "step": 1340
    },
    {
      "epoch": 4.627852420558755,
      "grad_norm": 1.5337084531784058,
      "learning_rate": 9.223611111111112e-06,
      "loss": 0.3472,
      "step": 1360
    },
    {
      "epoch": 4.696097248880358,
      "grad_norm": 1.247621774673462,
      "learning_rate": 9.195833333333334e-06,
      "loss": 0.344,
      "step": 1380
    },
    {
      "epoch": 4.764342077201962,
      "grad_norm": 1.3515852689743042,
      "learning_rate": 9.168055555555556e-06,
      "loss": 0.3391,
      "step": 1400
    },
    {
      "epoch": 4.8325869055235655,
      "grad_norm": 1.6575453281402588,
      "learning_rate": 9.140277777777778e-06,
      "loss": 0.3392,
      "step": 1420
    },
    {
      "epoch": 4.90083173384517,
      "grad_norm": 1.1725900173187256,
      "learning_rate": 9.112500000000001e-06,
      "loss": 0.3447,
      "step": 1440
    },
    {
      "epoch": 4.969076562166773,
      "grad_norm": 1.221179723739624,
      "learning_rate": 9.084722222222222e-06,
      "loss": 0.3445,
      "step": 1460
    },
    {
      "epoch": 5.034122414160802,
      "grad_norm": 1.4883283376693726,
      "learning_rate": 9.056944444444445e-06,
      "loss": 0.3323,
      "step": 1480
    },
    {
      "epoch": 5.102367242482406,
      "grad_norm": 1.5658860206604004,
      "learning_rate": 9.029166666666668e-06,
      "loss": 0.3238,
      "step": 1500
    },
    {
      "epoch": 5.102367242482406,
      "eval_accuracy": 0.840920460230115,
      "eval_loss": 0.3666646182537079,
      "eval_macro_f1": 0.8178812083151887,
      "eval_macro_precision": 0.8214451209905376,
      "eval_macro_recall": 0.8147055554421375,
      "eval_runtime": 0.7808,
      "eval_samples_per_second": 2560.231,
      "eval_steps_per_second": 80.688,
      "step": 1500
    },
    {
      "epoch": 5.170612070804009,
      "grad_norm": 1.3509379625320435,
      "learning_rate": 9.00138888888889e-06,
      "loss": 0.3164,
      "step": 1520
    },
    {
      "epoch": 5.238856899125613,
      "grad_norm": 1.3602068424224854,
      "learning_rate": 8.973611111111112e-06,
      "loss": 0.3138,
      "step": 1540
    },
    {
      "epoch": 5.3071017274472165,
      "grad_norm": 1.4228326082229614,
      "learning_rate": 8.945833333333333e-06,
      "loss": 0.316,
      "step": 1560
    },
    {
      "epoch": 5.375346555768821,
      "grad_norm": 1.2462983131408691,
      "learning_rate": 8.918055555555556e-06,
      "loss": 0.3149,
      "step": 1580
    },
    {
      "epoch": 5.443591384090425,
      "grad_norm": 1.8749498128890991,
      "learning_rate": 8.890277777777777e-06,
      "loss": 0.321,
      "step": 1600
    },
    {
      "epoch": 5.511836212412028,
      "grad_norm": 1.7895240783691406,
      "learning_rate": 8.8625e-06,
      "loss": 0.3147,
      "step": 1620
    },
    {
      "epoch": 5.580081040733631,
      "grad_norm": 1.3252431154251099,
      "learning_rate": 8.834722222222223e-06,
      "loss": 0.3242,
      "step": 1640
    },
    {
      "epoch": 5.648325869055236,
      "grad_norm": 1.2714225053787231,
      "learning_rate": 8.806944444444446e-06,
      "loss": 0.3236,
      "step": 1660
    },
    {
      "epoch": 5.71657069737684,
      "grad_norm": 1.3227649927139282,
      "learning_rate": 8.779166666666667e-06,
      "loss": 0.3251,
      "step": 1680
    },
    {
      "epoch": 5.784815525698443,
      "grad_norm": 1.4013444185256958,
      "learning_rate": 8.75138888888889e-06,
      "loss": 0.3178,
      "step": 1700
    },
    {
      "epoch": 5.853060354020047,
      "grad_norm": 1.4292689561843872,
      "learning_rate": 8.723611111111113e-06,
      "loss": 0.3143,
      "step": 1720
    },
    {
      "epoch": 5.9213051823416505,
      "grad_norm": 1.349610447883606,
      "learning_rate": 8.695833333333334e-06,
      "loss": 0.3192,
      "step": 1740
    },
    {
      "epoch": 5.989550010663255,
      "grad_norm": 1.527152180671692,
      "learning_rate": 8.668055555555557e-06,
      "loss": 0.3142,
      "step": 1760
    },
    {
      "epoch": 6.054595862657283,
      "grad_norm": 2.2399158477783203,
      "learning_rate": 8.640277777777778e-06,
      "loss": 0.2891,
      "step": 1780
    },
    {
      "epoch": 6.122840690978887,
      "grad_norm": 1.4852321147918701,
      "learning_rate": 8.612500000000001e-06,
      "loss": 0.2977,
      "step": 1800
    },
    {
      "epoch": 6.191085519300491,
      "grad_norm": 1.4544851779937744,
      "learning_rate": 8.584722222222222e-06,
      "loss": 0.2949,
      "step": 1820
    },
    {
      "epoch": 6.259330347622094,
      "grad_norm": 1.6133416891098022,
      "learning_rate": 8.556944444444445e-06,
      "loss": 0.2933,
      "step": 1840
    },
    {
      "epoch": 6.327575175943698,
      "grad_norm": 1.6650798320770264,
      "learning_rate": 8.529166666666668e-06,
      "loss": 0.2948,
      "step": 1860
    },
    {
      "epoch": 6.3958200042653015,
      "grad_norm": 1.8387089967727661,
      "learning_rate": 8.501388888888889e-06,
      "loss": 0.297,
      "step": 1880
    },
    {
      "epoch": 6.464064832586906,
      "grad_norm": 1.4247080087661743,
      "learning_rate": 8.473611111111112e-06,
      "loss": 0.2896,
      "step": 1900
    },
    {
      "epoch": 6.532309660908509,
      "grad_norm": 1.7731865644454956,
      "learning_rate": 8.445833333333335e-06,
      "loss": 0.2979,
      "step": 1920
    },
    {
      "epoch": 6.600554489230113,
      "grad_norm": 1.6518689393997192,
      "learning_rate": 8.418055555555556e-06,
      "loss": 0.2913,
      "step": 1940
    },
    {
      "epoch": 6.668799317551716,
      "grad_norm": 1.7998627424240112,
      "learning_rate": 8.390277777777779e-06,
      "loss": 0.2965,
      "step": 1960
    },
    {
      "epoch": 6.737044145873321,
      "grad_norm": 1.766281008720398,
      "learning_rate": 8.362500000000002e-06,
      "loss": 0.2957,
      "step": 1980
    },
    {
      "epoch": 6.805288974194925,
      "grad_norm": 1.5310908555984497,
      "learning_rate": 8.334722222222223e-06,
      "loss": 0.2882,
      "step": 2000
    },
    {
      "epoch": 6.805288974194925,
      "eval_accuracy": 0.8499249624812406,
      "eval_loss": 0.36273959279060364,
      "eval_macro_f1": 0.8233774403777372,
      "eval_macro_precision": 0.8393208152286364,
      "eval_macro_recall": 0.8124774581657581,
      "eval_runtime": 0.7699,
      "eval_samples_per_second": 2596.335,
      "eval_steps_per_second": 81.825,
      "step": 2000
    },
    {
      "epoch": 6.873533802516528,
      "grad_norm": 1.6291877031326294,
      "learning_rate": 8.306944444444444e-06,
      "loss": 0.2949,
      "step": 2020
    },
    {
      "epoch": 6.941778630838132,
      "grad_norm": 1.8236439228057861,
      "learning_rate": 8.279166666666667e-06,
      "loss": 0.3011,
      "step": 2040
    },
    {
      "epoch": 7.00682448283216,
      "grad_norm": 1.537187099456787,
      "learning_rate": 8.25138888888889e-06,
      "loss": 0.2937,
      "step": 2060
    },
    {
      "epoch": 7.075069311153764,
      "grad_norm": 1.483410358428955,
      "learning_rate": 8.223611111111111e-06,
      "loss": 0.273,
      "step": 2080
    },
    {
      "epoch": 7.143314139475368,
      "grad_norm": 1.8637510538101196,
      "learning_rate": 8.195833333333334e-06,
      "loss": 0.2677,
      "step": 2100
    },
    {
      "epoch": 7.211558967796972,
      "grad_norm": 1.5103261470794678,
      "learning_rate": 8.168055555555557e-06,
      "loss": 0.2714,
      "step": 2120
    },
    {
      "epoch": 7.279803796118576,
      "grad_norm": 2.0435543060302734,
      "learning_rate": 8.140277777777778e-06,
      "loss": 0.2746,
      "step": 2140
    },
    {
      "epoch": 7.348048624440179,
      "grad_norm": 1.6879143714904785,
      "learning_rate": 8.1125e-06,
      "loss": 0.2736,
      "step": 2160
    },
    {
      "epoch": 7.416293452761783,
      "grad_norm": 1.9367362260818481,
      "learning_rate": 8.084722222222224e-06,
      "loss": 0.2755,
      "step": 2180
    },
    {
      "epoch": 7.4845382810833865,
      "grad_norm": 1.6891629695892334,
      "learning_rate": 8.056944444444446e-06,
      "loss": 0.2744,
      "step": 2200
    },
    {
      "epoch": 7.552783109404991,
      "grad_norm": 1.632580280303955,
      "learning_rate": 8.029166666666668e-06,
      "loss": 0.2707,
      "step": 2220
    },
    {
      "epoch": 7.621027937726594,
      "grad_norm": 2.245885133743286,
      "learning_rate": 8.001388888888889e-06,
      "loss": 0.275,
      "step": 2240
    },
    {
      "epoch": 7.689272766048198,
      "grad_norm": 2.4403223991394043,
      "learning_rate": 7.973611111111112e-06,
      "loss": 0.2818,
      "step": 2260
    },
    {
      "epoch": 7.757517594369801,
      "grad_norm": 2.0328357219696045,
      "learning_rate": 7.945833333333334e-06,
      "loss": 0.2771,
      "step": 2280
    },
    {
      "epoch": 7.8257624226914055,
      "grad_norm": 2.0353128910064697,
      "learning_rate": 7.918055555555556e-06,
      "loss": 0.2808,
      "step": 2300
    },
    {
      "epoch": 7.894007251013009,
      "grad_norm": 1.5806877613067627,
      "learning_rate": 7.890277777777779e-06,
      "loss": 0.2758,
      "step": 2320
    },
    {
      "epoch": 7.962252079334613,
      "grad_norm": 1.7613822221755981,
      "learning_rate": 7.862500000000001e-06,
      "loss": 0.2794,
      "step": 2340
    },
    {
      "epoch": 8.027297931328642,
      "grad_norm": 1.690705418586731,
      "learning_rate": 7.834722222222223e-06,
      "loss": 0.2719,
      "step": 2360
    },
    {
      "epoch": 8.095542759650245,
      "grad_norm": 1.7441704273223877,
      "learning_rate": 7.806944444444445e-06,
      "loss": 0.2537,
      "step": 2380
    },
    {
      "epoch": 8.163787587971848,
      "grad_norm": 1.7393347024917603,
      "learning_rate": 7.779166666666668e-06,
      "loss": 0.2491,
      "step": 2400
    },
    {
      "epoch": 8.232032416293453,
      "grad_norm": 2.2918641567230225,
      "learning_rate": 7.75138888888889e-06,
      "loss": 0.2545,
      "step": 2420
    },
    {
      "epoch": 8.300277244615057,
      "grad_norm": 1.7754545211791992,
      "learning_rate": 7.723611111111112e-06,
      "loss": 0.2605,
      "step": 2440
    },
    {
      "epoch": 8.36852207293666,
      "grad_norm": 1.9032988548278809,
      "learning_rate": 7.695833333333333e-06,
      "loss": 0.2521,
      "step": 2460
    },
    {
      "epoch": 8.436766901258263,
      "grad_norm": 1.6741739511489868,
      "learning_rate": 7.668055555555556e-06,
      "loss": 0.2572,
      "step": 2480
    },
    {
      "epoch": 8.505011729579868,
      "grad_norm": 1.6107820272445679,
      "learning_rate": 7.640277777777778e-06,
      "loss": 0.25,
      "step": 2500
    },
    {
      "epoch": 8.505011729579868,
      "eval_accuracy": 0.8569284642321161,
      "eval_loss": 0.37762048840522766,
      "eval_macro_f1": 0.8356690351361296,
      "eval_macro_precision": 0.8407368696106611,
      "eval_macro_recall": 0.8312895403620757,
      "eval_runtime": 0.7739,
      "eval_samples_per_second": 2582.881,
      "eval_steps_per_second": 81.401,
      "step": 2500
    }
  ],
  "logging_steps": 20,
  "max_steps": 8000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 28,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.1368437229502336e+17,
  "train_batch_size": 64,
  "trial_name": null,
  "trial_params": null
}
