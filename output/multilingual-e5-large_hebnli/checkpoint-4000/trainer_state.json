{
  "best_global_step": 4000,
  "best_metric": 0.9047061869486643,
  "best_model_checkpoint": "output/multilingual-e5-large_hebnli/checkpoint-4000",
  "epoch": 13.607378972062273,
  "eval_steps": 500,
  "global_step": 4000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.06824482832160375,
      "grad_norm": 6.941891670227051,
      "learning_rate": 2.3750000000000003e-07,
      "loss": 0.9012,
      "step": 20
    },
    {
      "epoch": 0.1364896566432075,
      "grad_norm": 5.658565998077393,
      "learning_rate": 4.875000000000001e-07,
      "loss": 0.859,
      "step": 40
    },
    {
      "epoch": 0.20473448496481125,
      "grad_norm": 3.3730971813201904,
      "learning_rate": 7.375e-07,
      "loss": 0.7912,
      "step": 60
    },
    {
      "epoch": 0.272979313286415,
      "grad_norm": 1.748124599456787,
      "learning_rate": 9.875e-07,
      "loss": 0.7117,
      "step": 80
    },
    {
      "epoch": 0.3412241416080188,
      "grad_norm": 1.544105887413025,
      "learning_rate": 1.2375e-06,
      "loss": 0.6606,
      "step": 100
    },
    {
      "epoch": 0.4094689699296225,
      "grad_norm": 2.935027837753296,
      "learning_rate": 1.4875000000000002e-06,
      "loss": 0.6302,
      "step": 120
    },
    {
      "epoch": 0.4777137982512263,
      "grad_norm": 3.2123990058898926,
      "learning_rate": 1.7375e-06,
      "loss": 0.5602,
      "step": 140
    },
    {
      "epoch": 0.54595862657283,
      "grad_norm": 4.759181976318359,
      "learning_rate": 1.9875000000000005e-06,
      "loss": 0.4738,
      "step": 160
    },
    {
      "epoch": 0.6142034548944337,
      "grad_norm": 3.711029291152954,
      "learning_rate": 2.2375e-06,
      "loss": 0.4164,
      "step": 180
    },
    {
      "epoch": 0.6824482832160376,
      "grad_norm": 5.181697368621826,
      "learning_rate": 2.4875000000000003e-06,
      "loss": 0.3977,
      "step": 200
    },
    {
      "epoch": 0.7506931115376413,
      "grad_norm": 4.576099872589111,
      "learning_rate": 2.7375e-06,
      "loss": 0.3815,
      "step": 220
    },
    {
      "epoch": 0.818937939859245,
      "grad_norm": 4.415471076965332,
      "learning_rate": 2.9875e-06,
      "loss": 0.3713,
      "step": 240
    },
    {
      "epoch": 0.8871827681808488,
      "grad_norm": 3.5210423469543457,
      "learning_rate": 3.2375e-06,
      "loss": 0.3581,
      "step": 260
    },
    {
      "epoch": 0.9554275965024526,
      "grad_norm": 3.7435457706451416,
      "learning_rate": 3.4875000000000005e-06,
      "loss": 0.3525,
      "step": 280
    },
    {
      "epoch": 1.020473448496481,
      "grad_norm": 9.272966384887695,
      "learning_rate": 3.7375000000000006e-06,
      "loss": 0.3432,
      "step": 300
    },
    {
      "epoch": 1.088718276818085,
      "grad_norm": 4.780267238616943,
      "learning_rate": 3.9875e-06,
      "loss": 0.3358,
      "step": 320
    },
    {
      "epoch": 1.1569631051396887,
      "grad_norm": 5.005377769470215,
      "learning_rate": 4.2375000000000005e-06,
      "loss": 0.3303,
      "step": 340
    },
    {
      "epoch": 1.2252079334612924,
      "grad_norm": 3.8665566444396973,
      "learning_rate": 4.4875e-06,
      "loss": 0.3288,
      "step": 360
    },
    {
      "epoch": 1.2934527617828961,
      "grad_norm": 4.225553512573242,
      "learning_rate": 4.737500000000001e-06,
      "loss": 0.3323,
      "step": 380
    },
    {
      "epoch": 1.3616975901044999,
      "grad_norm": 3.87408447265625,
      "learning_rate": 4.987500000000001e-06,
      "loss": 0.3216,
      "step": 400
    },
    {
      "epoch": 1.4299424184261036,
      "grad_norm": 6.529589653015137,
      "learning_rate": 5.237500000000001e-06,
      "loss": 0.3207,
      "step": 420
    },
    {
      "epoch": 1.4981872467477073,
      "grad_norm": 6.08759880065918,
      "learning_rate": 5.4875e-06,
      "loss": 0.3137,
      "step": 440
    },
    {
      "epoch": 1.566432075069311,
      "grad_norm": 7.073441982269287,
      "learning_rate": 5.7375000000000005e-06,
      "loss": 0.3048,
      "step": 460
    },
    {
      "epoch": 1.6346769033909148,
      "grad_norm": 4.307260990142822,
      "learning_rate": 5.987500000000001e-06,
      "loss": 0.3107,
      "step": 480
    },
    {
      "epoch": 1.7029217317125187,
      "grad_norm": 3.5452632904052734,
      "learning_rate": 6.237500000000001e-06,
      "loss": 0.3034,
      "step": 500
    },
    {
      "epoch": 1.7029217317125187,
      "eval_accuracy": 0.8969484742371185,
      "eval_loss": 0.2592848837375641,
      "eval_macro_f1": 0.8821195882427032,
      "eval_macro_precision": 0.8860263589451796,
      "eval_macro_recall": 0.878576354154616,
      "eval_runtime": 1.4086,
      "eval_samples_per_second": 1419.139,
      "eval_steps_per_second": 44.725,
      "step": 500
    },
    {
      "epoch": 1.7711665600341224,
      "grad_norm": 4.1004743576049805,
      "learning_rate": 6.487500000000001e-06,
      "loss": 0.3037,
      "step": 520
    },
    {
      "epoch": 1.8394113883557262,
      "grad_norm": 4.734645366668701,
      "learning_rate": 6.7375e-06,
      "loss": 0.298,
      "step": 540
    },
    {
      "epoch": 1.9076562166773299,
      "grad_norm": 4.460270404815674,
      "learning_rate": 6.9875000000000004e-06,
      "loss": 0.2948,
      "step": 560
    },
    {
      "epoch": 1.9759010449989338,
      "grad_norm": 5.878218173980713,
      "learning_rate": 7.237500000000001e-06,
      "loss": 0.2974,
      "step": 580
    },
    {
      "epoch": 2.040946896992962,
      "grad_norm": 6.798260688781738,
      "learning_rate": 7.487500000000001e-06,
      "loss": 0.2934,
      "step": 600
    },
    {
      "epoch": 2.109191725314566,
      "grad_norm": 4.232055187225342,
      "learning_rate": 7.737500000000002e-06,
      "loss": 0.2766,
      "step": 620
    },
    {
      "epoch": 2.17743655363617,
      "grad_norm": 3.5089972019195557,
      "learning_rate": 7.987500000000001e-06,
      "loss": 0.2788,
      "step": 640
    },
    {
      "epoch": 2.2456813819577737,
      "grad_norm": 3.6353535652160645,
      "learning_rate": 8.2375e-06,
      "loss": 0.2715,
      "step": 660
    },
    {
      "epoch": 2.3139262102793774,
      "grad_norm": 10.723335266113281,
      "learning_rate": 8.487500000000001e-06,
      "loss": 0.2778,
      "step": 680
    },
    {
      "epoch": 2.382171038600981,
      "grad_norm": 3.6378438472747803,
      "learning_rate": 8.7375e-06,
      "loss": 0.2756,
      "step": 700
    },
    {
      "epoch": 2.450415866922585,
      "grad_norm": 2.934678077697754,
      "learning_rate": 8.987500000000002e-06,
      "loss": 0.2729,
      "step": 720
    },
    {
      "epoch": 2.5186606952441886,
      "grad_norm": 4.00606632232666,
      "learning_rate": 9.237500000000001e-06,
      "loss": 0.2617,
      "step": 740
    },
    {
      "epoch": 2.5869055235657923,
      "grad_norm": 3.731455087661743,
      "learning_rate": 9.4875e-06,
      "loss": 0.2767,
      "step": 760
    },
    {
      "epoch": 2.655150351887396,
      "grad_norm": 3.8253583908081055,
      "learning_rate": 9.737500000000001e-06,
      "loss": 0.2747,
      "step": 780
    },
    {
      "epoch": 2.7233951802089997,
      "grad_norm": 3.177557945251465,
      "learning_rate": 9.9875e-06,
      "loss": 0.2662,
      "step": 800
    },
    {
      "epoch": 2.7916400085306035,
      "grad_norm": 3.9750397205352783,
      "learning_rate": 9.973611111111113e-06,
      "loss": 0.2659,
      "step": 820
    },
    {
      "epoch": 2.859884836852207,
      "grad_norm": 2.99078631401062,
      "learning_rate": 9.945833333333334e-06,
      "loss": 0.2721,
      "step": 840
    },
    {
      "epoch": 2.928129665173811,
      "grad_norm": 6.057601451873779,
      "learning_rate": 9.918055555555557e-06,
      "loss": 0.2656,
      "step": 860
    },
    {
      "epoch": 2.9963744934954146,
      "grad_norm": 5.393702030181885,
      "learning_rate": 9.890277777777778e-06,
      "loss": 0.2563,
      "step": 880
    },
    {
      "epoch": 3.0614203454894433,
      "grad_norm": 4.786610126495361,
      "learning_rate": 9.862500000000001e-06,
      "loss": 0.2392,
      "step": 900
    },
    {
      "epoch": 3.129665173811047,
      "grad_norm": 5.9845170974731445,
      "learning_rate": 9.834722222222222e-06,
      "loss": 0.2392,
      "step": 920
    },
    {
      "epoch": 3.1979100021326508,
      "grad_norm": 4.448172569274902,
      "learning_rate": 9.806944444444445e-06,
      "loss": 0.2384,
      "step": 940
    },
    {
      "epoch": 3.2661548304542545,
      "grad_norm": 3.9855151176452637,
      "learning_rate": 9.779166666666668e-06,
      "loss": 0.2435,
      "step": 960
    },
    {
      "epoch": 3.334399658775858,
      "grad_norm": 3.3169848918914795,
      "learning_rate": 9.751388888888889e-06,
      "loss": 0.2386,
      "step": 980
    },
    {
      "epoch": 3.4026444870974624,
      "grad_norm": 5.063298225402832,
      "learning_rate": 9.723611111111112e-06,
      "loss": 0.2419,
      "step": 1000
    },
    {
      "epoch": 3.4026444870974624,
      "eval_accuracy": 0.9114557278639319,
      "eval_loss": 0.23855580389499664,
      "eval_macro_f1": 0.8983406584787085,
      "eval_macro_precision": 0.9041258741258741,
      "eval_macro_recall": 0.8932656908179029,
      "eval_runtime": 1.4094,
      "eval_samples_per_second": 1418.291,
      "eval_steps_per_second": 44.699,
      "step": 1000
    },
    {
      "epoch": 3.470889315419066,
      "grad_norm": 3.827061891555786,
      "learning_rate": 9.695833333333335e-06,
      "loss": 0.2346,
      "step": 1020
    },
    {
      "epoch": 3.53913414374067,
      "grad_norm": 10.00329875946045,
      "learning_rate": 9.668055555555556e-06,
      "loss": 0.2378,
      "step": 1040
    },
    {
      "epoch": 3.6073789720622735,
      "grad_norm": 3.19516658782959,
      "learning_rate": 9.640277777777779e-06,
      "loss": 0.2381,
      "step": 1060
    },
    {
      "epoch": 3.6756238003838773,
      "grad_norm": 3.625253438949585,
      "learning_rate": 9.612500000000002e-06,
      "loss": 0.2383,
      "step": 1080
    },
    {
      "epoch": 3.743868628705481,
      "grad_norm": 3.612203598022461,
      "learning_rate": 9.584722222222223e-06,
      "loss": 0.2358,
      "step": 1100
    },
    {
      "epoch": 3.8121134570270847,
      "grad_norm": 3.0862245559692383,
      "learning_rate": 9.556944444444446e-06,
      "loss": 0.2339,
      "step": 1120
    },
    {
      "epoch": 3.8803582853486884,
      "grad_norm": 3.7409534454345703,
      "learning_rate": 9.529166666666667e-06,
      "loss": 0.231,
      "step": 1140
    },
    {
      "epoch": 3.948603113670292,
      "grad_norm": 3.6773200035095215,
      "learning_rate": 9.50138888888889e-06,
      "loss": 0.2377,
      "step": 1160
    },
    {
      "epoch": 4.013648965664321,
      "grad_norm": 3.540532112121582,
      "learning_rate": 9.473611111111112e-06,
      "loss": 0.2272,
      "step": 1180
    },
    {
      "epoch": 4.081893793985924,
      "grad_norm": 5.2176923751831055,
      "learning_rate": 9.445833333333334e-06,
      "loss": 0.2203,
      "step": 1200
    },
    {
      "epoch": 4.150138622307528,
      "grad_norm": 5.405241012573242,
      "learning_rate": 9.418055555555557e-06,
      "loss": 0.2128,
      "step": 1220
    },
    {
      "epoch": 4.218383450629132,
      "grad_norm": 3.4214258193969727,
      "learning_rate": 9.39027777777778e-06,
      "loss": 0.2129,
      "step": 1240
    },
    {
      "epoch": 4.286628278950736,
      "grad_norm": 4.0479278564453125,
      "learning_rate": 9.3625e-06,
      "loss": 0.2104,
      "step": 1260
    },
    {
      "epoch": 4.35487310727234,
      "grad_norm": 5.426510334014893,
      "learning_rate": 9.334722222222223e-06,
      "loss": 0.2015,
      "step": 1280
    },
    {
      "epoch": 4.423117935593943,
      "grad_norm": 4.535679817199707,
      "learning_rate": 9.306944444444446e-06,
      "loss": 0.2092,
      "step": 1300
    },
    {
      "epoch": 4.491362763915547,
      "grad_norm": 3.986802101135254,
      "learning_rate": 9.279166666666667e-06,
      "loss": 0.2122,
      "step": 1320
    },
    {
      "epoch": 4.559607592237151,
      "grad_norm": 3.359113931655884,
      "learning_rate": 9.251388888888889e-06,
      "loss": 0.2038,
      "step": 1340
    },
    {
      "epoch": 4.627852420558755,
      "grad_norm": 3.788799524307251,
      "learning_rate": 9.223611111111112e-06,
      "loss": 0.2137,
      "step": 1360
    },
    {
      "epoch": 4.696097248880358,
      "grad_norm": 4.224390506744385,
      "learning_rate": 9.195833333333334e-06,
      "loss": 0.2032,
      "step": 1380
    },
    {
      "epoch": 4.764342077201962,
      "grad_norm": 31.796676635742188,
      "learning_rate": 9.168055555555556e-06,
      "loss": 0.203,
      "step": 1400
    },
    {
      "epoch": 4.8325869055235655,
      "grad_norm": 4.246357440948486,
      "learning_rate": 9.140277777777778e-06,
      "loss": 0.2118,
      "step": 1420
    },
    {
      "epoch": 4.90083173384517,
      "grad_norm": 4.06821346282959,
      "learning_rate": 9.112500000000001e-06,
      "loss": 0.2172,
      "step": 1440
    },
    {
      "epoch": 4.969076562166773,
      "grad_norm": 4.318020343780518,
      "learning_rate": 9.084722222222222e-06,
      "loss": 0.2106,
      "step": 1460
    },
    {
      "epoch": 5.034122414160802,
      "grad_norm": 4.984006404876709,
      "learning_rate": 9.056944444444445e-06,
      "loss": 0.198,
      "step": 1480
    },
    {
      "epoch": 5.102367242482406,
      "grad_norm": 3.972074031829834,
      "learning_rate": 9.029166666666668e-06,
      "loss": 0.1864,
      "step": 1500
    },
    {
      "epoch": 5.102367242482406,
      "eval_accuracy": 0.9159579789894947,
      "eval_loss": 0.250356525182724,
      "eval_macro_f1": 0.9033899475283071,
      "eval_macro_precision": 0.9098497437299734,
      "eval_macro_recall": 0.897784265289318,
      "eval_runtime": 1.4284,
      "eval_samples_per_second": 1399.511,
      "eval_steps_per_second": 44.107,
      "step": 1500
    },
    {
      "epoch": 5.170612070804009,
      "grad_norm": 3.9933793544769287,
      "learning_rate": 9.00138888888889e-06,
      "loss": 0.1848,
      "step": 1520
    },
    {
      "epoch": 5.238856899125613,
      "grad_norm": 4.408804893493652,
      "learning_rate": 8.973611111111112e-06,
      "loss": 0.1867,
      "step": 1540
    },
    {
      "epoch": 5.3071017274472165,
      "grad_norm": 3.3043317794799805,
      "learning_rate": 8.945833333333333e-06,
      "loss": 0.1896,
      "step": 1560
    },
    {
      "epoch": 5.375346555768821,
      "grad_norm": 3.7466795444488525,
      "learning_rate": 8.918055555555556e-06,
      "loss": 0.1858,
      "step": 1580
    },
    {
      "epoch": 5.443591384090425,
      "grad_norm": 4.581419944763184,
      "learning_rate": 8.890277777777777e-06,
      "loss": 0.1873,
      "step": 1600
    },
    {
      "epoch": 5.511836212412028,
      "grad_norm": 4.9938740730285645,
      "learning_rate": 8.8625e-06,
      "loss": 0.1884,
      "step": 1620
    },
    {
      "epoch": 5.580081040733631,
      "grad_norm": 3.7643070220947266,
      "learning_rate": 8.834722222222223e-06,
      "loss": 0.1915,
      "step": 1640
    },
    {
      "epoch": 5.648325869055236,
      "grad_norm": 3.4106128215789795,
      "learning_rate": 8.806944444444446e-06,
      "loss": 0.1913,
      "step": 1660
    },
    {
      "epoch": 5.71657069737684,
      "grad_norm": 3.2618584632873535,
      "learning_rate": 8.779166666666667e-06,
      "loss": 0.1859,
      "step": 1680
    },
    {
      "epoch": 5.784815525698443,
      "grad_norm": 4.3591203689575195,
      "learning_rate": 8.75138888888889e-06,
      "loss": 0.1867,
      "step": 1700
    },
    {
      "epoch": 5.853060354020047,
      "grad_norm": 3.9039251804351807,
      "learning_rate": 8.723611111111113e-06,
      "loss": 0.1801,
      "step": 1720
    },
    {
      "epoch": 5.9213051823416505,
      "grad_norm": 3.476314067840576,
      "learning_rate": 8.695833333333334e-06,
      "loss": 0.1902,
      "step": 1740
    },
    {
      "epoch": 5.989550010663255,
      "grad_norm": 4.053734302520752,
      "learning_rate": 8.668055555555557e-06,
      "loss": 0.182,
      "step": 1760
    },
    {
      "epoch": 6.054595862657283,
      "grad_norm": 4.896427154541016,
      "learning_rate": 8.640277777777778e-06,
      "loss": 0.1676,
      "step": 1780
    },
    {
      "epoch": 6.122840690978887,
      "grad_norm": 4.3695197105407715,
      "learning_rate": 8.612500000000001e-06,
      "loss": 0.1645,
      "step": 1800
    },
    {
      "epoch": 6.191085519300491,
      "grad_norm": 3.6107914447784424,
      "learning_rate": 8.584722222222222e-06,
      "loss": 0.169,
      "step": 1820
    },
    {
      "epoch": 6.259330347622094,
      "grad_norm": 4.4239349365234375,
      "learning_rate": 8.556944444444445e-06,
      "loss": 0.1596,
      "step": 1840
    },
    {
      "epoch": 6.327575175943698,
      "grad_norm": 3.941694498062134,
      "learning_rate": 8.529166666666668e-06,
      "loss": 0.164,
      "step": 1860
    },
    {
      "epoch": 6.3958200042653015,
      "grad_norm": 4.966579437255859,
      "learning_rate": 8.501388888888889e-06,
      "loss": 0.1626,
      "step": 1880
    },
    {
      "epoch": 6.464064832586906,
      "grad_norm": 3.821866512298584,
      "learning_rate": 8.473611111111112e-06,
      "loss": 0.1616,
      "step": 1900
    },
    {
      "epoch": 6.532309660908509,
      "grad_norm": 3.621548652648926,
      "learning_rate": 8.445833333333335e-06,
      "loss": 0.1687,
      "step": 1920
    },
    {
      "epoch": 6.600554489230113,
      "grad_norm": 4.270162582397461,
      "learning_rate": 8.418055555555556e-06,
      "loss": 0.16,
      "step": 1940
    },
    {
      "epoch": 6.668799317551716,
      "grad_norm": 6.776479244232178,
      "learning_rate": 8.390277777777779e-06,
      "loss": 0.1712,
      "step": 1960
    },
    {
      "epoch": 6.737044145873321,
      "grad_norm": 4.498440265655518,
      "learning_rate": 8.362500000000002e-06,
      "loss": 0.1622,
      "step": 1980
    },
    {
      "epoch": 6.805288974194925,
      "grad_norm": 3.7078778743743896,
      "learning_rate": 8.334722222222223e-06,
      "loss": 0.1583,
      "step": 2000
    },
    {
      "epoch": 6.805288974194925,
      "eval_accuracy": 0.9169584792396198,
      "eval_loss": 0.256711483001709,
      "eval_macro_f1": 0.9038951214539254,
      "eval_macro_precision": 0.9137964357280908,
      "eval_macro_recall": 0.8958102244089219,
      "eval_runtime": 1.4165,
      "eval_samples_per_second": 1411.196,
      "eval_steps_per_second": 44.475,
      "step": 2000
    },
    {
      "epoch": 6.873533802516528,
      "grad_norm": 4.241532802581787,
      "learning_rate": 8.306944444444444e-06,
      "loss": 0.166,
      "step": 2020
    },
    {
      "epoch": 6.941778630838132,
      "grad_norm": 3.9160573482513428,
      "learning_rate": 8.279166666666667e-06,
      "loss": 0.1714,
      "step": 2040
    },
    {
      "epoch": 7.00682448283216,
      "grad_norm": 3.1729848384857178,
      "learning_rate": 8.25138888888889e-06,
      "loss": 0.1682,
      "step": 2060
    },
    {
      "epoch": 7.075069311153764,
      "grad_norm": 4.5266618728637695,
      "learning_rate": 8.223611111111111e-06,
      "loss": 0.1449,
      "step": 2080
    },
    {
      "epoch": 7.143314139475368,
      "grad_norm": 3.6916897296905518,
      "learning_rate": 8.195833333333334e-06,
      "loss": 0.143,
      "step": 2100
    },
    {
      "epoch": 7.211558967796972,
      "grad_norm": 3.2381067276000977,
      "learning_rate": 8.168055555555557e-06,
      "loss": 0.1438,
      "step": 2120
    },
    {
      "epoch": 7.279803796118576,
      "grad_norm": 4.369308948516846,
      "learning_rate": 8.140277777777778e-06,
      "loss": 0.1514,
      "step": 2140
    },
    {
      "epoch": 7.348048624440179,
      "grad_norm": 4.058343410491943,
      "learning_rate": 8.1125e-06,
      "loss": 0.1459,
      "step": 2160
    },
    {
      "epoch": 7.416293452761783,
      "grad_norm": 5.170927047729492,
      "learning_rate": 8.084722222222224e-06,
      "loss": 0.1433,
      "step": 2180
    },
    {
      "epoch": 7.4845382810833865,
      "grad_norm": 3.678720712661743,
      "learning_rate": 8.056944444444446e-06,
      "loss": 0.145,
      "step": 2200
    },
    {
      "epoch": 7.552783109404991,
      "grad_norm": 4.261829376220703,
      "learning_rate": 8.029166666666668e-06,
      "loss": 0.1483,
      "step": 2220
    },
    {
      "epoch": 7.621027937726594,
      "grad_norm": 5.597457408905029,
      "learning_rate": 8.001388888888889e-06,
      "loss": 0.1457,
      "step": 2240
    },
    {
      "epoch": 7.689272766048198,
      "grad_norm": 4.783130168914795,
      "learning_rate": 7.973611111111112e-06,
      "loss": 0.1514,
      "step": 2260
    },
    {
      "epoch": 7.757517594369801,
      "grad_norm": 4.854778289794922,
      "learning_rate": 7.945833333333334e-06,
      "loss": 0.1498,
      "step": 2280
    },
    {
      "epoch": 7.8257624226914055,
      "grad_norm": 4.360438346862793,
      "learning_rate": 7.918055555555556e-06,
      "loss": 0.1532,
      "step": 2300
    },
    {
      "epoch": 7.894007251013009,
      "grad_norm": 3.831707715988159,
      "learning_rate": 7.890277777777779e-06,
      "loss": 0.1416,
      "step": 2320
    },
    {
      "epoch": 7.962252079334613,
      "grad_norm": 4.567569732666016,
      "learning_rate": 7.862500000000001e-06,
      "loss": 0.1498,
      "step": 2340
    },
    {
      "epoch": 8.027297931328642,
      "grad_norm": 4.425360202789307,
      "learning_rate": 7.834722222222223e-06,
      "loss": 0.1384,
      "step": 2360
    },
    {
      "epoch": 8.095542759650245,
      "grad_norm": 5.512426376342773,
      "learning_rate": 7.806944444444445e-06,
      "loss": 0.1263,
      "step": 2380
    },
    {
      "epoch": 8.163787587971848,
      "grad_norm": 4.902756690979004,
      "learning_rate": 7.779166666666668e-06,
      "loss": 0.1207,
      "step": 2400
    },
    {
      "epoch": 8.232032416293453,
      "grad_norm": 3.9367856979370117,
      "learning_rate": 7.75138888888889e-06,
      "loss": 0.1291,
      "step": 2420
    },
    {
      "epoch": 8.300277244615057,
      "grad_norm": 4.645631790161133,
      "learning_rate": 7.723611111111112e-06,
      "loss": 0.1286,
      "step": 2440
    },
    {
      "epoch": 8.36852207293666,
      "grad_norm": 4.726120471954346,
      "learning_rate": 7.695833333333333e-06,
      "loss": 0.1318,
      "step": 2460
    },
    {
      "epoch": 8.436766901258263,
      "grad_norm": 4.769508361816406,
      "learning_rate": 7.668055555555556e-06,
      "loss": 0.1276,
      "step": 2480
    },
    {
      "epoch": 8.505011729579868,
      "grad_norm": 4.960451602935791,
      "learning_rate": 7.640277777777778e-06,
      "loss": 0.1333,
      "step": 2500
    },
    {
      "epoch": 8.505011729579868,
      "eval_accuracy": 0.9129564782391195,
      "eval_loss": 0.2688034176826477,
      "eval_macro_f1": 0.9013756473334286,
      "eval_macro_precision": 0.9013756473334286,
      "eval_macro_recall": 0.9013756473334286,
      "eval_runtime": 1.4494,
      "eval_samples_per_second": 1379.202,
      "eval_steps_per_second": 43.467,
      "step": 2500
    },
    {
      "epoch": 8.573256557901471,
      "grad_norm": 4.912074089050293,
      "learning_rate": 7.6125e-06,
      "loss": 0.1308,
      "step": 2520
    },
    {
      "epoch": 8.641501386223075,
      "grad_norm": 4.894683361053467,
      "learning_rate": 7.584722222222222e-06,
      "loss": 0.1305,
      "step": 2540
    },
    {
      "epoch": 8.70974621454468,
      "grad_norm": 4.504294395446777,
      "learning_rate": 7.556944444444445e-06,
      "loss": 0.1314,
      "step": 2560
    },
    {
      "epoch": 8.777991042866283,
      "grad_norm": 5.289912223815918,
      "learning_rate": 7.529166666666667e-06,
      "loss": 0.1328,
      "step": 2580
    },
    {
      "epoch": 8.846235871187886,
      "grad_norm": 3.6101064682006836,
      "learning_rate": 7.501388888888889e-06,
      "loss": 0.1321,
      "step": 2600
    },
    {
      "epoch": 8.91448069950949,
      "grad_norm": 4.533703327178955,
      "learning_rate": 7.473611111111112e-06,
      "loss": 0.1315,
      "step": 2620
    },
    {
      "epoch": 8.982725527831095,
      "grad_norm": 4.261584281921387,
      "learning_rate": 7.445833333333334e-06,
      "loss": 0.1336,
      "step": 2640
    },
    {
      "epoch": 9.047771379825123,
      "grad_norm": 3.670870780944824,
      "learning_rate": 7.418055555555557e-06,
      "loss": 0.1186,
      "step": 2660
    },
    {
      "epoch": 9.116016208146727,
      "grad_norm": 4.423170566558838,
      "learning_rate": 7.390277777777778e-06,
      "loss": 0.114,
      "step": 2680
    },
    {
      "epoch": 9.18426103646833,
      "grad_norm": 4.100703239440918,
      "learning_rate": 7.3625e-06,
      "loss": 0.1109,
      "step": 2700
    },
    {
      "epoch": 9.252505864789933,
      "grad_norm": 5.2762675285339355,
      "learning_rate": 7.334722222222222e-06,
      "loss": 0.1176,
      "step": 2720
    },
    {
      "epoch": 9.320750693111538,
      "grad_norm": 4.627791404724121,
      "learning_rate": 7.306944444444445e-06,
      "loss": 0.1171,
      "step": 2740
    },
    {
      "epoch": 9.388995521433142,
      "grad_norm": 5.1249847412109375,
      "learning_rate": 7.279166666666667e-06,
      "loss": 0.1143,
      "step": 2760
    },
    {
      "epoch": 9.457240349754745,
      "grad_norm": 4.806126594543457,
      "learning_rate": 7.251388888888889e-06,
      "loss": 0.1219,
      "step": 2780
    },
    {
      "epoch": 9.525485178076348,
      "grad_norm": 4.402298927307129,
      "learning_rate": 7.223611111111112e-06,
      "loss": 0.115,
      "step": 2800
    },
    {
      "epoch": 9.593730006397953,
      "grad_norm": 4.615245342254639,
      "learning_rate": 7.195833333333334e-06,
      "loss": 0.1114,
      "step": 2820
    },
    {
      "epoch": 9.661974834719556,
      "grad_norm": 4.432644367218018,
      "learning_rate": 7.168055555555556e-06,
      "loss": 0.1154,
      "step": 2840
    },
    {
      "epoch": 9.73021966304116,
      "grad_norm": 3.7581989765167236,
      "learning_rate": 7.140277777777779e-06,
      "loss": 0.1159,
      "step": 2860
    },
    {
      "epoch": 9.798464491362765,
      "grad_norm": 4.727720737457275,
      "learning_rate": 7.112500000000001e-06,
      "loss": 0.1224,
      "step": 2880
    },
    {
      "epoch": 9.866709319684368,
      "grad_norm": 5.337357997894287,
      "learning_rate": 7.084722222222222e-06,
      "loss": 0.1134,
      "step": 2900
    },
    {
      "epoch": 9.934954148005971,
      "grad_norm": 4.717106342315674,
      "learning_rate": 7.056944444444445e-06,
      "loss": 0.119,
      "step": 2920
    },
    {
      "epoch": 10.0,
      "grad_norm": 20.922042846679688,
      "learning_rate": 7.029166666666667e-06,
      "loss": 0.1053,
      "step": 2940
    },
    {
      "epoch": 10.068244828321603,
      "grad_norm": 5.187588214874268,
      "learning_rate": 7.001388888888889e-06,
      "loss": 0.1005,
      "step": 2960
    },
    {
      "epoch": 10.136489656643208,
      "grad_norm": 4.167693138122559,
      "learning_rate": 6.973611111111112e-06,
      "loss": 0.1061,
      "step": 2980
    },
    {
      "epoch": 10.204734484964812,
      "grad_norm": 4.281857967376709,
      "learning_rate": 6.945833333333334e-06,
      "loss": 0.0999,
      "step": 3000
    },
    {
      "epoch": 10.204734484964812,
      "eval_accuracy": 0.9134567283641821,
      "eval_loss": 0.3054823577404022,
      "eval_macro_f1": 0.9008013011283043,
      "eval_macro_precision": 0.9058407318104318,
      "eval_macro_recall": 0.8963098308483441,
      "eval_runtime": 1.4176,
      "eval_samples_per_second": 1410.172,
      "eval_steps_per_second": 44.443,
      "step": 3000
    },
    {
      "epoch": 10.272979313286415,
      "grad_norm": 5.762639045715332,
      "learning_rate": 6.918055555555556e-06,
      "loss": 0.1017,
      "step": 3020
    },
    {
      "epoch": 10.341224141608018,
      "grad_norm": 4.038492679595947,
      "learning_rate": 6.890277777777779e-06,
      "loss": 0.1024,
      "step": 3040
    },
    {
      "epoch": 10.409468969929623,
      "grad_norm": 4.392915725708008,
      "learning_rate": 6.862500000000001e-06,
      "loss": 0.0982,
      "step": 3060
    },
    {
      "epoch": 10.477713798251227,
      "grad_norm": 4.123185634613037,
      "learning_rate": 6.834722222222223e-06,
      "loss": 0.1,
      "step": 3080
    },
    {
      "epoch": 10.54595862657283,
      "grad_norm": 3.720217227935791,
      "learning_rate": 6.806944444444446e-06,
      "loss": 0.1022,
      "step": 3100
    },
    {
      "epoch": 10.614203454894433,
      "grad_norm": 4.220370769500732,
      "learning_rate": 6.779166666666667e-06,
      "loss": 0.1059,
      "step": 3120
    },
    {
      "epoch": 10.682448283216038,
      "grad_norm": 3.898287534713745,
      "learning_rate": 6.751388888888889e-06,
      "loss": 0.0954,
      "step": 3140
    },
    {
      "epoch": 10.750693111537641,
      "grad_norm": 4.539979934692383,
      "learning_rate": 6.723611111111112e-06,
      "loss": 0.103,
      "step": 3160
    },
    {
      "epoch": 10.818937939859245,
      "grad_norm": 4.738539695739746,
      "learning_rate": 6.695833333333334e-06,
      "loss": 0.105,
      "step": 3180
    },
    {
      "epoch": 10.88718276818085,
      "grad_norm": 4.302069187164307,
      "learning_rate": 6.668055555555556e-06,
      "loss": 0.1018,
      "step": 3200
    },
    {
      "epoch": 10.955427596502453,
      "grad_norm": 4.7486982345581055,
      "learning_rate": 6.640277777777779e-06,
      "loss": 0.1041,
      "step": 3220
    },
    {
      "epoch": 11.020473448496482,
      "grad_norm": 3.7071642875671387,
      "learning_rate": 6.612500000000001e-06,
      "loss": 0.102,
      "step": 3240
    },
    {
      "epoch": 11.088718276818085,
      "grad_norm": 3.713987350463867,
      "learning_rate": 6.584722222222223e-06,
      "loss": 0.0918,
      "step": 3260
    },
    {
      "epoch": 11.156963105139688,
      "grad_norm": 5.575456619262695,
      "learning_rate": 6.5569444444444455e-06,
      "loss": 0.0862,
      "step": 3280
    },
    {
      "epoch": 11.225207933461292,
      "grad_norm": 4.564228534698486,
      "learning_rate": 6.5291666666666675e-06,
      "loss": 0.0954,
      "step": 3300
    },
    {
      "epoch": 11.293452761782897,
      "grad_norm": 4.775580406188965,
      "learning_rate": 6.5013888888888896e-06,
      "loss": 0.0859,
      "step": 3320
    },
    {
      "epoch": 11.3616975901045,
      "grad_norm": 4.978163719177246,
      "learning_rate": 6.473611111111112e-06,
      "loss": 0.0943,
      "step": 3340
    },
    {
      "epoch": 11.429942418426103,
      "grad_norm": 4.721282482147217,
      "learning_rate": 6.445833333333334e-06,
      "loss": 0.0915,
      "step": 3360
    },
    {
      "epoch": 11.498187246747708,
      "grad_norm": 5.464110851287842,
      "learning_rate": 6.418055555555556e-06,
      "loss": 0.089,
      "step": 3380
    },
    {
      "epoch": 11.566432075069311,
      "grad_norm": 5.382199287414551,
      "learning_rate": 6.390277777777778e-06,
      "loss": 0.084,
      "step": 3400
    },
    {
      "epoch": 11.634676903390915,
      "grad_norm": 5.774059772491455,
      "learning_rate": 6.3625000000000005e-06,
      "loss": 0.0893,
      "step": 3420
    },
    {
      "epoch": 11.702921731712518,
      "grad_norm": 4.047942161560059,
      "learning_rate": 6.3347222222222225e-06,
      "loss": 0.0951,
      "step": 3440
    },
    {
      "epoch": 11.771166560034123,
      "grad_norm": 4.369022846221924,
      "learning_rate": 6.306944444444445e-06,
      "loss": 0.0914,
      "step": 3460
    },
    {
      "epoch": 11.839411388355726,
      "grad_norm": 5.973501205444336,
      "learning_rate": 6.279166666666667e-06,
      "loss": 0.0938,
      "step": 3480
    },
    {
      "epoch": 11.90765621667733,
      "grad_norm": 5.651524066925049,
      "learning_rate": 6.251388888888889e-06,
      "loss": 0.088,
      "step": 3500
    },
    {
      "epoch": 11.90765621667733,
      "eval_accuracy": 0.9064532266133066,
      "eval_loss": 0.32942596077919006,
      "eval_macro_f1": 0.8933790948719067,
      "eval_macro_precision": 0.8958073632279377,
      "eval_macro_recall": 0.8910937354683144,
      "eval_runtime": 1.4801,
      "eval_samples_per_second": 1350.558,
      "eval_steps_per_second": 42.564,
      "step": 3500
    },
    {
      "epoch": 11.975901044998933,
      "grad_norm": 4.101532936096191,
      "learning_rate": 6.223611111111112e-06,
      "loss": 0.0907,
      "step": 3520
    },
    {
      "epoch": 12.040946896992962,
      "grad_norm": 7.262320518493652,
      "learning_rate": 6.195833333333334e-06,
      "loss": 0.082,
      "step": 3540
    },
    {
      "epoch": 12.109191725314567,
      "grad_norm": 6.128721237182617,
      "learning_rate": 6.168055555555556e-06,
      "loss": 0.0771,
      "step": 3560
    },
    {
      "epoch": 12.17743655363617,
      "grad_norm": 5.268158435821533,
      "learning_rate": 6.1402777777777775e-06,
      "loss": 0.0841,
      "step": 3580
    },
    {
      "epoch": 12.245681381957773,
      "grad_norm": 5.067440986633301,
      "learning_rate": 6.1125e-06,
      "loss": 0.0815,
      "step": 3600
    },
    {
      "epoch": 12.313926210279377,
      "grad_norm": 3.9277994632720947,
      "learning_rate": 6.084722222222222e-06,
      "loss": 0.0817,
      "step": 3620
    },
    {
      "epoch": 12.382171038600982,
      "grad_norm": 5.399319171905518,
      "learning_rate": 6.056944444444444e-06,
      "loss": 0.0774,
      "step": 3640
    },
    {
      "epoch": 12.450415866922585,
      "grad_norm": 3.945708990097046,
      "learning_rate": 6.029166666666667e-06,
      "loss": 0.0768,
      "step": 3660
    },
    {
      "epoch": 12.518660695244188,
      "grad_norm": 4.4936909675598145,
      "learning_rate": 6.001388888888889e-06,
      "loss": 0.0783,
      "step": 3680
    },
    {
      "epoch": 12.586905523565793,
      "grad_norm": 4.723905086517334,
      "learning_rate": 5.973611111111111e-06,
      "loss": 0.0821,
      "step": 3700
    },
    {
      "epoch": 12.655150351887396,
      "grad_norm": 6.208759307861328,
      "learning_rate": 5.945833333333334e-06,
      "loss": 0.0806,
      "step": 3720
    },
    {
      "epoch": 12.723395180209,
      "grad_norm": 4.492892742156982,
      "learning_rate": 5.918055555555556e-06,
      "loss": 0.0811,
      "step": 3740
    },
    {
      "epoch": 12.791640008530603,
      "grad_norm": 5.422558307647705,
      "learning_rate": 5.890277777777779e-06,
      "loss": 0.0808,
      "step": 3760
    },
    {
      "epoch": 12.859884836852208,
      "grad_norm": 4.833296775817871,
      "learning_rate": 5.862500000000001e-06,
      "loss": 0.0818,
      "step": 3780
    },
    {
      "epoch": 12.928129665173811,
      "grad_norm": 4.474386692047119,
      "learning_rate": 5.834722222222222e-06,
      "loss": 0.0854,
      "step": 3800
    },
    {
      "epoch": 12.996374493495415,
      "grad_norm": 5.790529251098633,
      "learning_rate": 5.806944444444444e-06,
      "loss": 0.0754,
      "step": 3820
    },
    {
      "epoch": 13.061420345489443,
      "grad_norm": 4.672941207885742,
      "learning_rate": 5.779166666666667e-06,
      "loss": 0.0729,
      "step": 3840
    },
    {
      "epoch": 13.129665173811047,
      "grad_norm": 5.900380611419678,
      "learning_rate": 5.751388888888889e-06,
      "loss": 0.0749,
      "step": 3860
    },
    {
      "epoch": 13.197910002132652,
      "grad_norm": 4.635248184204102,
      "learning_rate": 5.723611111111111e-06,
      "loss": 0.0724,
      "step": 3880
    },
    {
      "epoch": 13.266154830454255,
      "grad_norm": 6.130173206329346,
      "learning_rate": 5.695833333333334e-06,
      "loss": 0.074,
      "step": 3900
    },
    {
      "epoch": 13.334399658775858,
      "grad_norm": 6.089526176452637,
      "learning_rate": 5.668055555555556e-06,
      "loss": 0.0712,
      "step": 3920
    },
    {
      "epoch": 13.402644487097461,
      "grad_norm": 5.671406269073486,
      "learning_rate": 5.640277777777778e-06,
      "loss": 0.0729,
      "step": 3940
    },
    {
      "epoch": 13.470889315419067,
      "grad_norm": 4.026220321655273,
      "learning_rate": 5.612500000000001e-06,
      "loss": 0.0675,
      "step": 3960
    },
    {
      "epoch": 13.53913414374067,
      "grad_norm": 5.205732822418213,
      "learning_rate": 5.584722222222223e-06,
      "loss": 0.0758,
      "step": 3980
    },
    {
      "epoch": 13.607378972062273,
      "grad_norm": 5.125048637390137,
      "learning_rate": 5.556944444444446e-06,
      "loss": 0.0745,
      "step": 4000
    },
    {
      "epoch": 13.607378972062273,
      "eval_accuracy": 0.9164582291145573,
      "eval_loss": 0.35248470306396484,
      "eval_macro_f1": 0.9047061869486643,
      "eval_macro_precision": 0.9075578320456218,
      "eval_macro_recall": 0.9020414111925452,
      "eval_runtime": 1.4571,
      "eval_samples_per_second": 1371.916,
      "eval_steps_per_second": 43.237,
      "step": 4000
    }
  ],
  "logging_steps": 20,
  "max_steps": 8000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 28,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 8.463747167023153e+17,
  "train_batch_size": 64,
  "trial_name": null,
  "trial_params": null
}
