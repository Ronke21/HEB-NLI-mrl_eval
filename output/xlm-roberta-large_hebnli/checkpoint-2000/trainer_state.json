{
  "best_global_step": 2000,
  "best_metric": 0.9069088248012507,
  "best_model_checkpoint": "output/xlm-roberta-large_hebnli/checkpoint-2000",
  "epoch": 6.805288974194925,
  "eval_steps": 500,
  "global_step": 2000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.06824482832160375,
      "grad_norm": 11.00940990447998,
      "learning_rate": 2.3750000000000003e-07,
      "loss": 1.0384,
      "step": 20
    },
    {
      "epoch": 0.1364896566432075,
      "grad_norm": 11.179853439331055,
      "learning_rate": 4.875000000000001e-07,
      "loss": 0.9949,
      "step": 40
    },
    {
      "epoch": 0.20473448496481125,
      "grad_norm": 6.895035266876221,
      "learning_rate": 7.375e-07,
      "loss": 0.8973,
      "step": 60
    },
    {
      "epoch": 0.272979313286415,
      "grad_norm": 6.414916515350342,
      "learning_rate": 9.875e-07,
      "loss": 0.786,
      "step": 80
    },
    {
      "epoch": 0.3412241416080188,
      "grad_norm": 3.0077974796295166,
      "learning_rate": 1.2375e-06,
      "loss": 0.6973,
      "step": 100
    },
    {
      "epoch": 0.4094689699296225,
      "grad_norm": 2.458232879638672,
      "learning_rate": 1.4875000000000002e-06,
      "loss": 0.6592,
      "step": 120
    },
    {
      "epoch": 0.4777137982512263,
      "grad_norm": 2.2534964084625244,
      "learning_rate": 1.7375e-06,
      "loss": 0.6477,
      "step": 140
    },
    {
      "epoch": 0.54595862657283,
      "grad_norm": 2.695919990539551,
      "learning_rate": 1.9875000000000005e-06,
      "loss": 0.6432,
      "step": 160
    },
    {
      "epoch": 0.6142034548944337,
      "grad_norm": 2.167755365371704,
      "learning_rate": 2.2375e-06,
      "loss": 0.629,
      "step": 180
    },
    {
      "epoch": 0.6824482832160376,
      "grad_norm": 5.372771263122559,
      "learning_rate": 2.4875000000000003e-06,
      "loss": 0.5734,
      "step": 200
    },
    {
      "epoch": 0.7506931115376413,
      "grad_norm": 5.432171821594238,
      "learning_rate": 2.7375e-06,
      "loss": 0.5005,
      "step": 220
    },
    {
      "epoch": 0.818937939859245,
      "grad_norm": 7.5378875732421875,
      "learning_rate": 2.9875e-06,
      "loss": 0.4507,
      "step": 240
    },
    {
      "epoch": 0.8871827681808488,
      "grad_norm": 7.985943794250488,
      "learning_rate": 3.2375e-06,
      "loss": 0.4021,
      "step": 260
    },
    {
      "epoch": 0.9554275965024526,
      "grad_norm": 7.240444183349609,
      "learning_rate": 3.4875000000000005e-06,
      "loss": 0.3656,
      "step": 280
    },
    {
      "epoch": 1.020473448496481,
      "grad_norm": 10.606550216674805,
      "learning_rate": 3.7375000000000006e-06,
      "loss": 0.3461,
      "step": 300
    },
    {
      "epoch": 1.088718276818085,
      "grad_norm": 10.045287132263184,
      "learning_rate": 3.9875e-06,
      "loss": 0.3341,
      "step": 320
    },
    {
      "epoch": 1.1569631051396887,
      "grad_norm": 6.5651116371154785,
      "learning_rate": 4.2375000000000005e-06,
      "loss": 0.3215,
      "step": 340
    },
    {
      "epoch": 1.2252079334612924,
      "grad_norm": 5.078037738800049,
      "learning_rate": 4.4875e-06,
      "loss": 0.3169,
      "step": 360
    },
    {
      "epoch": 1.2934527617828961,
      "grad_norm": 7.71006441116333,
      "learning_rate": 4.737500000000001e-06,
      "loss": 0.3168,
      "step": 380
    },
    {
      "epoch": 1.3616975901044999,
      "grad_norm": 6.600442409515381,
      "learning_rate": 4.987500000000001e-06,
      "loss": 0.2989,
      "step": 400
    },
    {
      "epoch": 1.4299424184261036,
      "grad_norm": 6.330490589141846,
      "learning_rate": 5.237500000000001e-06,
      "loss": 0.3004,
      "step": 420
    },
    {
      "epoch": 1.4981872467477073,
      "grad_norm": 4.409477233886719,
      "learning_rate": 5.4875e-06,
      "loss": 0.2897,
      "step": 440
    },
    {
      "epoch": 1.566432075069311,
      "grad_norm": 8.02309513092041,
      "learning_rate": 5.7375000000000005e-06,
      "loss": 0.282,
      "step": 460
    },
    {
      "epoch": 1.6346769033909148,
      "grad_norm": 7.750760078430176,
      "learning_rate": 5.987500000000001e-06,
      "loss": 0.2823,
      "step": 480
    },
    {
      "epoch": 1.7029217317125187,
      "grad_norm": 4.843262672424316,
      "learning_rate": 6.237500000000001e-06,
      "loss": 0.2809,
      "step": 500
    },
    {
      "epoch": 1.7029217317125187,
      "eval_accuracy": 0.8859429714857429,
      "eval_loss": 0.2751270532608032,
      "eval_macro_f1": 0.8707680896093202,
      "eval_macro_precision": 0.8707680896093202,
      "eval_macro_recall": 0.8707680896093202,
      "eval_runtime": 1.4331,
      "eval_samples_per_second": 1394.846,
      "eval_steps_per_second": 43.96,
      "step": 500
    },
    {
      "epoch": 1.7711665600341224,
      "grad_norm": 8.007478713989258,
      "learning_rate": 6.487500000000001e-06,
      "loss": 0.2797,
      "step": 520
    },
    {
      "epoch": 1.8394113883557262,
      "grad_norm": 3.534803628921509,
      "learning_rate": 6.7375e-06,
      "loss": 0.2805,
      "step": 540
    },
    {
      "epoch": 1.9076562166773299,
      "grad_norm": 6.552062034606934,
      "learning_rate": 6.9875000000000004e-06,
      "loss": 0.2753,
      "step": 560
    },
    {
      "epoch": 1.9759010449989338,
      "grad_norm": 15.184134483337402,
      "learning_rate": 7.237500000000001e-06,
      "loss": 0.27,
      "step": 580
    },
    {
      "epoch": 2.040946896992962,
      "grad_norm": 6.743217945098877,
      "learning_rate": 7.487500000000001e-06,
      "loss": 0.2605,
      "step": 600
    },
    {
      "epoch": 2.109191725314566,
      "grad_norm": 3.689117908477783,
      "learning_rate": 7.737500000000002e-06,
      "loss": 0.2537,
      "step": 620
    },
    {
      "epoch": 2.17743655363617,
      "grad_norm": 6.313772201538086,
      "learning_rate": 7.987500000000001e-06,
      "loss": 0.2578,
      "step": 640
    },
    {
      "epoch": 2.2456813819577737,
      "grad_norm": 7.130761623382568,
      "learning_rate": 8.2375e-06,
      "loss": 0.2485,
      "step": 660
    },
    {
      "epoch": 2.3139262102793774,
      "grad_norm": 4.737888336181641,
      "learning_rate": 8.487500000000001e-06,
      "loss": 0.2583,
      "step": 680
    },
    {
      "epoch": 2.382171038600981,
      "grad_norm": 4.260260581970215,
      "learning_rate": 8.7375e-06,
      "loss": 0.2536,
      "step": 700
    },
    {
      "epoch": 2.450415866922585,
      "grad_norm": 4.497675895690918,
      "learning_rate": 8.987500000000002e-06,
      "loss": 0.2518,
      "step": 720
    },
    {
      "epoch": 2.5186606952441886,
      "grad_norm": 4.5175909996032715,
      "learning_rate": 9.237500000000001e-06,
      "loss": 0.2391,
      "step": 740
    },
    {
      "epoch": 2.5869055235657923,
      "grad_norm": 4.328146457672119,
      "learning_rate": 9.4875e-06,
      "loss": 0.2474,
      "step": 760
    },
    {
      "epoch": 2.655150351887396,
      "grad_norm": 5.945594787597656,
      "learning_rate": 9.737500000000001e-06,
      "loss": 0.2512,
      "step": 780
    },
    {
      "epoch": 2.7233951802089997,
      "grad_norm": 4.235509872436523,
      "learning_rate": 9.9875e-06,
      "loss": 0.2422,
      "step": 800
    },
    {
      "epoch": 2.7916400085306035,
      "grad_norm": 5.1783037185668945,
      "learning_rate": 9.973611111111113e-06,
      "loss": 0.2408,
      "step": 820
    },
    {
      "epoch": 2.859884836852207,
      "grad_norm": 4.521084785461426,
      "learning_rate": 9.945833333333334e-06,
      "loss": 0.2447,
      "step": 840
    },
    {
      "epoch": 2.928129665173811,
      "grad_norm": 5.813595771789551,
      "learning_rate": 9.918055555555557e-06,
      "loss": 0.2469,
      "step": 860
    },
    {
      "epoch": 2.9963744934954146,
      "grad_norm": 4.391115665435791,
      "learning_rate": 9.890277777777778e-06,
      "loss": 0.2336,
      "step": 880
    },
    {
      "epoch": 3.0614203454894433,
      "grad_norm": 3.578157424926758,
      "learning_rate": 9.862500000000001e-06,
      "loss": 0.2159,
      "step": 900
    },
    {
      "epoch": 3.129665173811047,
      "grad_norm": 7.32731294631958,
      "learning_rate": 9.834722222222222e-06,
      "loss": 0.2175,
      "step": 920
    },
    {
      "epoch": 3.1979100021326508,
      "grad_norm": 3.788149356842041,
      "learning_rate": 9.806944444444445e-06,
      "loss": 0.213,
      "step": 940
    },
    {
      "epoch": 3.2661548304542545,
      "grad_norm": 3.395658254623413,
      "learning_rate": 9.779166666666668e-06,
      "loss": 0.2178,
      "step": 960
    },
    {
      "epoch": 3.334399658775858,
      "grad_norm": 3.7128210067749023,
      "learning_rate": 9.751388888888889e-06,
      "loss": 0.2166,
      "step": 980
    },
    {
      "epoch": 3.4026444870974624,
      "grad_norm": 3.6492626667022705,
      "learning_rate": 9.723611111111112e-06,
      "loss": 0.2152,
      "step": 1000
    },
    {
      "epoch": 3.4026444870974624,
      "eval_accuracy": 0.8969484742371185,
      "eval_loss": 0.2618758976459503,
      "eval_macro_f1": 0.8842976274767637,
      "eval_macro_precision": 0.8810433420458622,
      "eval_macro_recall": 0.8878993165429276,
      "eval_runtime": 1.4332,
      "eval_samples_per_second": 1394.741,
      "eval_steps_per_second": 43.956,
      "step": 1000
    },
    {
      "epoch": 3.470889315419066,
      "grad_norm": 6.230752944946289,
      "learning_rate": 9.695833333333335e-06,
      "loss": 0.2145,
      "step": 1020
    },
    {
      "epoch": 3.53913414374067,
      "grad_norm": 4.145622253417969,
      "learning_rate": 9.668055555555556e-06,
      "loss": 0.2179,
      "step": 1040
    },
    {
      "epoch": 3.6073789720622735,
      "grad_norm": 3.0579476356506348,
      "learning_rate": 9.640277777777779e-06,
      "loss": 0.2119,
      "step": 1060
    },
    {
      "epoch": 3.6756238003838773,
      "grad_norm": 3.5516700744628906,
      "learning_rate": 9.612500000000002e-06,
      "loss": 0.2165,
      "step": 1080
    },
    {
      "epoch": 3.743868628705481,
      "grad_norm": 3.855407476425171,
      "learning_rate": 9.584722222222223e-06,
      "loss": 0.2092,
      "step": 1100
    },
    {
      "epoch": 3.8121134570270847,
      "grad_norm": 3.18847393989563,
      "learning_rate": 9.556944444444446e-06,
      "loss": 0.2114,
      "step": 1120
    },
    {
      "epoch": 3.8803582853486884,
      "grad_norm": 4.565018653869629,
      "learning_rate": 9.529166666666667e-06,
      "loss": 0.2079,
      "step": 1140
    },
    {
      "epoch": 3.948603113670292,
      "grad_norm": 3.8664145469665527,
      "learning_rate": 9.50138888888889e-06,
      "loss": 0.214,
      "step": 1160
    },
    {
      "epoch": 4.013648965664321,
      "grad_norm": 3.5041515827178955,
      "learning_rate": 9.473611111111112e-06,
      "loss": 0.1979,
      "step": 1180
    },
    {
      "epoch": 4.081893793985924,
      "grad_norm": 6.000615119934082,
      "learning_rate": 9.445833333333334e-06,
      "loss": 0.1869,
      "step": 1200
    },
    {
      "epoch": 4.150138622307528,
      "grad_norm": 4.371553897857666,
      "learning_rate": 9.418055555555557e-06,
      "loss": 0.1839,
      "step": 1220
    },
    {
      "epoch": 4.218383450629132,
      "grad_norm": 8.248231887817383,
      "learning_rate": 9.39027777777778e-06,
      "loss": 0.1833,
      "step": 1240
    },
    {
      "epoch": 4.286628278950736,
      "grad_norm": 3.9171245098114014,
      "learning_rate": 9.3625e-06,
      "loss": 0.186,
      "step": 1260
    },
    {
      "epoch": 4.35487310727234,
      "grad_norm": 3.4793283939361572,
      "learning_rate": 9.334722222222223e-06,
      "loss": 0.1755,
      "step": 1280
    },
    {
      "epoch": 4.423117935593943,
      "grad_norm": 7.878885269165039,
      "learning_rate": 9.306944444444446e-06,
      "loss": 0.1783,
      "step": 1300
    },
    {
      "epoch": 4.491362763915547,
      "grad_norm": 4.393846035003662,
      "learning_rate": 9.279166666666667e-06,
      "loss": 0.1874,
      "step": 1320
    },
    {
      "epoch": 4.559607592237151,
      "grad_norm": 4.124290943145752,
      "learning_rate": 9.251388888888889e-06,
      "loss": 0.1805,
      "step": 1340
    },
    {
      "epoch": 4.627852420558755,
      "grad_norm": 4.857876777648926,
      "learning_rate": 9.223611111111112e-06,
      "loss": 0.1863,
      "step": 1360
    },
    {
      "epoch": 4.696097248880358,
      "grad_norm": 4.413712978363037,
      "learning_rate": 9.195833333333334e-06,
      "loss": 0.1763,
      "step": 1380
    },
    {
      "epoch": 4.764342077201962,
      "grad_norm": 4.726840496063232,
      "learning_rate": 9.168055555555556e-06,
      "loss": 0.1752,
      "step": 1400
    },
    {
      "epoch": 4.8325869055235655,
      "grad_norm": 4.19461727142334,
      "learning_rate": 9.140277777777778e-06,
      "loss": 0.1823,
      "step": 1420
    },
    {
      "epoch": 4.90083173384517,
      "grad_norm": 4.707530975341797,
      "learning_rate": 9.112500000000001e-06,
      "loss": 0.189,
      "step": 1440
    },
    {
      "epoch": 4.969076562166773,
      "grad_norm": 5.049172401428223,
      "learning_rate": 9.084722222222222e-06,
      "loss": 0.1842,
      "step": 1460
    },
    {
      "epoch": 5.034122414160802,
      "grad_norm": 5.509632587432861,
      "learning_rate": 9.056944444444445e-06,
      "loss": 0.168,
      "step": 1480
    },
    {
      "epoch": 5.102367242482406,
      "grad_norm": 3.8714957237243652,
      "learning_rate": 9.029166666666668e-06,
      "loss": 0.1631,
      "step": 1500
    },
    {
      "epoch": 5.102367242482406,
      "eval_accuracy": 0.9134567283641821,
      "eval_loss": 0.254062682390213,
      "eval_macro_f1": 0.9012824571384367,
      "eval_macro_precision": 0.9041103292862399,
      "eval_macro_recall": 0.8986405714454221,
      "eval_runtime": 1.4441,
      "eval_samples_per_second": 1384.271,
      "eval_steps_per_second": 43.626,
      "step": 1500
    },
    {
      "epoch": 5.170612070804009,
      "grad_norm": 4.1053643226623535,
      "learning_rate": 9.00138888888889e-06,
      "loss": 0.1491,
      "step": 1520
    },
    {
      "epoch": 5.238856899125613,
      "grad_norm": 3.2158660888671875,
      "learning_rate": 8.973611111111112e-06,
      "loss": 0.1563,
      "step": 1540
    },
    {
      "epoch": 5.3071017274472165,
      "grad_norm": 3.396588087081909,
      "learning_rate": 8.945833333333333e-06,
      "loss": 0.1568,
      "step": 1560
    },
    {
      "epoch": 5.375346555768821,
      "grad_norm": 4.356389045715332,
      "learning_rate": 8.918055555555556e-06,
      "loss": 0.1534,
      "step": 1580
    },
    {
      "epoch": 5.443591384090425,
      "grad_norm": 4.059852600097656,
      "learning_rate": 8.890277777777777e-06,
      "loss": 0.1595,
      "step": 1600
    },
    {
      "epoch": 5.511836212412028,
      "grad_norm": 5.105983734130859,
      "learning_rate": 8.8625e-06,
      "loss": 0.1517,
      "step": 1620
    },
    {
      "epoch": 5.580081040733631,
      "grad_norm": 4.1315999031066895,
      "learning_rate": 8.834722222222223e-06,
      "loss": 0.1627,
      "step": 1640
    },
    {
      "epoch": 5.648325869055236,
      "grad_norm": 10.039505958557129,
      "learning_rate": 8.806944444444446e-06,
      "loss": 0.1606,
      "step": 1660
    },
    {
      "epoch": 5.71657069737684,
      "grad_norm": 4.642844200134277,
      "learning_rate": 8.779166666666667e-06,
      "loss": 0.1585,
      "step": 1680
    },
    {
      "epoch": 5.784815525698443,
      "grad_norm": 7.059525489807129,
      "learning_rate": 8.75138888888889e-06,
      "loss": 0.1603,
      "step": 1700
    },
    {
      "epoch": 5.853060354020047,
      "grad_norm": 3.7070629596710205,
      "learning_rate": 8.723611111111113e-06,
      "loss": 0.1522,
      "step": 1720
    },
    {
      "epoch": 5.9213051823416505,
      "grad_norm": 3.899528741836548,
      "learning_rate": 8.695833333333334e-06,
      "loss": 0.1609,
      "step": 1740
    },
    {
      "epoch": 5.989550010663255,
      "grad_norm": 4.26993465423584,
      "learning_rate": 8.668055555555557e-06,
      "loss": 0.1576,
      "step": 1760
    },
    {
      "epoch": 6.054595862657283,
      "grad_norm": 6.123539447784424,
      "learning_rate": 8.640277777777778e-06,
      "loss": 0.138,
      "step": 1780
    },
    {
      "epoch": 6.122840690978887,
      "grad_norm": 4.462007522583008,
      "learning_rate": 8.612500000000001e-06,
      "loss": 0.1365,
      "step": 1800
    },
    {
      "epoch": 6.191085519300491,
      "grad_norm": 4.8214569091796875,
      "learning_rate": 8.584722222222222e-06,
      "loss": 0.1332,
      "step": 1820
    },
    {
      "epoch": 6.259330347622094,
      "grad_norm": 4.261445045471191,
      "learning_rate": 8.556944444444445e-06,
      "loss": 0.1301,
      "step": 1840
    },
    {
      "epoch": 6.327575175943698,
      "grad_norm": 3.7162721157073975,
      "learning_rate": 8.529166666666668e-06,
      "loss": 0.1391,
      "step": 1860
    },
    {
      "epoch": 6.3958200042653015,
      "grad_norm": 6.362778663635254,
      "learning_rate": 8.501388888888889e-06,
      "loss": 0.1385,
      "step": 1880
    },
    {
      "epoch": 6.464064832586906,
      "grad_norm": 4.8666205406188965,
      "learning_rate": 8.473611111111112e-06,
      "loss": 0.1363,
      "step": 1900
    },
    {
      "epoch": 6.532309660908509,
      "grad_norm": 7.178747653961182,
      "learning_rate": 8.445833333333335e-06,
      "loss": 0.1478,
      "step": 1920
    },
    {
      "epoch": 6.600554489230113,
      "grad_norm": 4.3745927810668945,
      "learning_rate": 8.418055555555556e-06,
      "loss": 0.1324,
      "step": 1940
    },
    {
      "epoch": 6.668799317551716,
      "grad_norm": 6.255326271057129,
      "learning_rate": 8.390277777777779e-06,
      "loss": 0.1438,
      "step": 1960
    },
    {
      "epoch": 6.737044145873321,
      "grad_norm": 5.401108264923096,
      "learning_rate": 8.362500000000002e-06,
      "loss": 0.1366,
      "step": 1980
    },
    {
      "epoch": 6.805288974194925,
      "grad_norm": 6.416804313659668,
      "learning_rate": 8.334722222222223e-06,
      "loss": 0.135,
      "step": 2000
    },
    {
      "epoch": 6.805288974194925,
      "eval_accuracy": 0.9194597298649325,
      "eval_loss": 0.26233309507369995,
      "eval_macro_f1": 0.9069088248012507,
      "eval_macro_precision": 0.9162047372204776,
      "eval_macro_recall": 0.899226942680794,
      "eval_runtime": 1.4294,
      "eval_samples_per_second": 1398.525,
      "eval_steps_per_second": 44.076,
      "step": 2000
    }
  ],
  "logging_steps": 20,
  "max_steps": 8000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 28,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4.232159598217754e+17,
  "train_batch_size": 64,
  "trial_name": null,
  "trial_params": null
}
