{
  "best_global_step": 3000,
  "best_metric": 0.8222430119483475,
  "best_model_checkpoint": "output/bert-base-multilingual-cased_hebnli/checkpoint-3000",
  "epoch": 10.204734484964812,
  "eval_steps": 500,
  "global_step": 3000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.06824482832160375,
      "grad_norm": 4.238726615905762,
      "learning_rate": 2.3750000000000003e-07,
      "loss": 1.1058,
      "step": 20
    },
    {
      "epoch": 0.1364896566432075,
      "grad_norm": 4.0209550857543945,
      "learning_rate": 4.875000000000001e-07,
      "loss": 1.077,
      "step": 40
    },
    {
      "epoch": 0.20473448496481125,
      "grad_norm": 3.7930264472961426,
      "learning_rate": 7.375e-07,
      "loss": 1.0126,
      "step": 60
    },
    {
      "epoch": 0.272979313286415,
      "grad_norm": 2.448258399963379,
      "learning_rate": 9.875e-07,
      "loss": 0.8758,
      "step": 80
    },
    {
      "epoch": 0.3412241416080188,
      "grad_norm": 1.2724642753601074,
      "learning_rate": 1.2375e-06,
      "loss": 0.7427,
      "step": 100
    },
    {
      "epoch": 0.4094689699296225,
      "grad_norm": 1.0473136901855469,
      "learning_rate": 1.4875000000000002e-06,
      "loss": 0.6778,
      "step": 120
    },
    {
      "epoch": 0.4777137982512263,
      "grad_norm": 0.8543972969055176,
      "learning_rate": 1.7375e-06,
      "loss": 0.6504,
      "step": 140
    },
    {
      "epoch": 0.54595862657283,
      "grad_norm": 1.0305250883102417,
      "learning_rate": 1.9875000000000005e-06,
      "loss": 0.6226,
      "step": 160
    },
    {
      "epoch": 0.6142034548944337,
      "grad_norm": 0.9232239723205566,
      "learning_rate": 2.2375e-06,
      "loss": 0.5963,
      "step": 180
    },
    {
      "epoch": 0.6824482832160376,
      "grad_norm": 0.9515542387962341,
      "learning_rate": 2.4875000000000003e-06,
      "loss": 0.5877,
      "step": 200
    },
    {
      "epoch": 0.7506931115376413,
      "grad_norm": 1.4449692964553833,
      "learning_rate": 2.7375e-06,
      "loss": 0.5726,
      "step": 220
    },
    {
      "epoch": 0.818937939859245,
      "grad_norm": 0.9561119079589844,
      "learning_rate": 2.9875e-06,
      "loss": 0.5559,
      "step": 240
    },
    {
      "epoch": 0.8871827681808488,
      "grad_norm": 1.0364538431167603,
      "learning_rate": 3.2375e-06,
      "loss": 0.5455,
      "step": 260
    },
    {
      "epoch": 0.9554275965024526,
      "grad_norm": 1.1005817651748657,
      "learning_rate": 3.4875000000000005e-06,
      "loss": 0.5391,
      "step": 280
    },
    {
      "epoch": 1.020473448496481,
      "grad_norm": 1.0922209024429321,
      "learning_rate": 3.7375000000000006e-06,
      "loss": 0.5283,
      "step": 300
    },
    {
      "epoch": 1.088718276818085,
      "grad_norm": 1.3905435800552368,
      "learning_rate": 3.9875e-06,
      "loss": 0.5194,
      "step": 320
    },
    {
      "epoch": 1.1569631051396887,
      "grad_norm": 1.3158761262893677,
      "learning_rate": 4.2375000000000005e-06,
      "loss": 0.5094,
      "step": 340
    },
    {
      "epoch": 1.2252079334612924,
      "grad_norm": 1.5506861209869385,
      "learning_rate": 4.4875e-06,
      "loss": 0.509,
      "step": 360
    },
    {
      "epoch": 1.2934527617828961,
      "grad_norm": 1.985611915588379,
      "learning_rate": 4.737500000000001e-06,
      "loss": 0.5051,
      "step": 380
    },
    {
      "epoch": 1.3616975901044999,
      "grad_norm": 1.461193561553955,
      "learning_rate": 4.987500000000001e-06,
      "loss": 0.4943,
      "step": 400
    },
    {
      "epoch": 1.4299424184261036,
      "grad_norm": 1.3440146446228027,
      "learning_rate": 5.237500000000001e-06,
      "loss": 0.4829,
      "step": 420
    },
    {
      "epoch": 1.4981872467477073,
      "grad_norm": 1.897265911102295,
      "learning_rate": 5.4875e-06,
      "loss": 0.4778,
      "step": 440
    },
    {
      "epoch": 1.566432075069311,
      "grad_norm": 2.339411735534668,
      "learning_rate": 5.7375000000000005e-06,
      "loss": 0.4804,
      "step": 460
    },
    {
      "epoch": 1.6346769033909148,
      "grad_norm": 1.350225567817688,
      "learning_rate": 5.987500000000001e-06,
      "loss": 0.4814,
      "step": 480
    },
    {
      "epoch": 1.7029217317125187,
      "grad_norm": 1.5185205936431885,
      "learning_rate": 6.237500000000001e-06,
      "loss": 0.4732,
      "step": 500
    },
    {
      "epoch": 1.7029217317125187,
      "eval_accuracy": 0.7818909454727364,
      "eval_loss": 0.45727935433387756,
      "eval_macro_f1": 0.7456012534822194,
      "eval_macro_precision": 0.7545334224534412,
      "eval_macro_recall": 0.7392763248927632,
      "eval_runtime": 1.7208,
      "eval_samples_per_second": 1161.688,
      "eval_steps_per_second": 36.611,
      "step": 500
    },
    {
      "epoch": 1.7711665600341224,
      "grad_norm": 1.5732786655426025,
      "learning_rate": 6.487500000000001e-06,
      "loss": 0.4714,
      "step": 520
    },
    {
      "epoch": 1.8394113883557262,
      "grad_norm": 1.5108362436294556,
      "learning_rate": 6.7375e-06,
      "loss": 0.4697,
      "step": 540
    },
    {
      "epoch": 1.9076562166773299,
      "grad_norm": 3.3093442916870117,
      "learning_rate": 6.9875000000000004e-06,
      "loss": 0.464,
      "step": 560
    },
    {
      "epoch": 1.9759010449989338,
      "grad_norm": 1.5924838781356812,
      "learning_rate": 7.237500000000001e-06,
      "loss": 0.4646,
      "step": 580
    },
    {
      "epoch": 2.040946896992962,
      "grad_norm": 1.4280611276626587,
      "learning_rate": 7.487500000000001e-06,
      "loss": 0.4549,
      "step": 600
    },
    {
      "epoch": 2.109191725314566,
      "grad_norm": 2.3034093379974365,
      "learning_rate": 7.737500000000002e-06,
      "loss": 0.4509,
      "step": 620
    },
    {
      "epoch": 2.17743655363617,
      "grad_norm": 1.4939898252487183,
      "learning_rate": 7.987500000000001e-06,
      "loss": 0.4532,
      "step": 640
    },
    {
      "epoch": 2.2456813819577737,
      "grad_norm": 2.534858226776123,
      "learning_rate": 8.2375e-06,
      "loss": 0.4392,
      "step": 660
    },
    {
      "epoch": 2.3139262102793774,
      "grad_norm": 1.317044734954834,
      "learning_rate": 8.487500000000001e-06,
      "loss": 0.4496,
      "step": 680
    },
    {
      "epoch": 2.382171038600981,
      "grad_norm": 1.9060896635055542,
      "learning_rate": 8.7375e-06,
      "loss": 0.445,
      "step": 700
    },
    {
      "epoch": 2.450415866922585,
      "grad_norm": 1.2968693971633911,
      "learning_rate": 8.987500000000002e-06,
      "loss": 0.443,
      "step": 720
    },
    {
      "epoch": 2.5186606952441886,
      "grad_norm": 1.2930861711502075,
      "learning_rate": 9.237500000000001e-06,
      "loss": 0.4349,
      "step": 740
    },
    {
      "epoch": 2.5869055235657923,
      "grad_norm": 3.3298768997192383,
      "learning_rate": 9.4875e-06,
      "loss": 0.442,
      "step": 760
    },
    {
      "epoch": 2.655150351887396,
      "grad_norm": 1.3247339725494385,
      "learning_rate": 9.737500000000001e-06,
      "loss": 0.4368,
      "step": 780
    },
    {
      "epoch": 2.7233951802089997,
      "grad_norm": 1.4808156490325928,
      "learning_rate": 9.9875e-06,
      "loss": 0.4321,
      "step": 800
    },
    {
      "epoch": 2.7916400085306035,
      "grad_norm": 2.0284481048583984,
      "learning_rate": 9.973611111111113e-06,
      "loss": 0.4304,
      "step": 820
    },
    {
      "epoch": 2.859884836852207,
      "grad_norm": 1.4073902368545532,
      "learning_rate": 9.945833333333334e-06,
      "loss": 0.4276,
      "step": 840
    },
    {
      "epoch": 2.928129665173811,
      "grad_norm": 1.7538739442825317,
      "learning_rate": 9.918055555555557e-06,
      "loss": 0.4277,
      "step": 860
    },
    {
      "epoch": 2.9963744934954146,
      "grad_norm": 1.7719919681549072,
      "learning_rate": 9.890277777777778e-06,
      "loss": 0.422,
      "step": 880
    },
    {
      "epoch": 3.0614203454894433,
      "grad_norm": 1.446956992149353,
      "learning_rate": 9.862500000000001e-06,
      "loss": 0.4036,
      "step": 900
    },
    {
      "epoch": 3.129665173811047,
      "grad_norm": 1.3877074718475342,
      "learning_rate": 9.834722222222222e-06,
      "loss": 0.4119,
      "step": 920
    },
    {
      "epoch": 3.1979100021326508,
      "grad_norm": 2.0906527042388916,
      "learning_rate": 9.806944444444445e-06,
      "loss": 0.4101,
      "step": 940
    },
    {
      "epoch": 3.2661548304542545,
      "grad_norm": 1.6399444341659546,
      "learning_rate": 9.779166666666668e-06,
      "loss": 0.4111,
      "step": 960
    },
    {
      "epoch": 3.334399658775858,
      "grad_norm": 2.130391836166382,
      "learning_rate": 9.751388888888889e-06,
      "loss": 0.4068,
      "step": 980
    },
    {
      "epoch": 3.4026444870974624,
      "grad_norm": 2.765031576156616,
      "learning_rate": 9.723611111111112e-06,
      "loss": 0.4123,
      "step": 1000
    },
    {
      "epoch": 3.4026444870974624,
      "eval_accuracy": 0.8129064532266133,
      "eval_loss": 0.40928345918655396,
      "eval_macro_f1": 0.7880143224293235,
      "eval_macro_precision": 0.7880143224293235,
      "eval_macro_recall": 0.7880143224293235,
      "eval_runtime": 0.8066,
      "eval_samples_per_second": 2478.237,
      "eval_steps_per_second": 78.104,
      "step": 1000
    },
    {
      "epoch": 3.470889315419066,
      "grad_norm": 1.3866723775863647,
      "learning_rate": 9.695833333333335e-06,
      "loss": 0.4093,
      "step": 1020
    },
    {
      "epoch": 3.53913414374067,
      "grad_norm": 1.8722583055496216,
      "learning_rate": 9.668055555555556e-06,
      "loss": 0.4061,
      "step": 1040
    },
    {
      "epoch": 3.6073789720622735,
      "grad_norm": 2.3272416591644287,
      "learning_rate": 9.640277777777779e-06,
      "loss": 0.4056,
      "step": 1060
    },
    {
      "epoch": 3.6756238003838773,
      "grad_norm": 1.913228154182434,
      "learning_rate": 9.612500000000002e-06,
      "loss": 0.4093,
      "step": 1080
    },
    {
      "epoch": 3.743868628705481,
      "grad_norm": 1.691199779510498,
      "learning_rate": 9.584722222222223e-06,
      "loss": 0.4028,
      "step": 1100
    },
    {
      "epoch": 3.8121134570270847,
      "grad_norm": 2.632054328918457,
      "learning_rate": 9.556944444444446e-06,
      "loss": 0.399,
      "step": 1120
    },
    {
      "epoch": 3.8803582853486884,
      "grad_norm": 2.3423655033111572,
      "learning_rate": 9.529166666666667e-06,
      "loss": 0.3982,
      "step": 1140
    },
    {
      "epoch": 3.948603113670292,
      "grad_norm": 1.7554192543029785,
      "learning_rate": 9.50138888888889e-06,
      "loss": 0.4024,
      "step": 1160
    },
    {
      "epoch": 4.013648965664321,
      "grad_norm": 1.7169402837753296,
      "learning_rate": 9.473611111111112e-06,
      "loss": 0.3927,
      "step": 1180
    },
    {
      "epoch": 4.081893793985924,
      "grad_norm": 3.241577625274658,
      "learning_rate": 9.445833333333334e-06,
      "loss": 0.3982,
      "step": 1200
    },
    {
      "epoch": 4.150138622307528,
      "grad_norm": 1.8109190464019775,
      "learning_rate": 9.418055555555557e-06,
      "loss": 0.386,
      "step": 1220
    },
    {
      "epoch": 4.218383450629132,
      "grad_norm": 1.5739351511001587,
      "learning_rate": 9.39027777777778e-06,
      "loss": 0.3882,
      "step": 1240
    },
    {
      "epoch": 4.286628278950736,
      "grad_norm": 2.2713332176208496,
      "learning_rate": 9.3625e-06,
      "loss": 0.3905,
      "step": 1260
    },
    {
      "epoch": 4.35487310727234,
      "grad_norm": 1.5617871284484863,
      "learning_rate": 9.334722222222223e-06,
      "loss": 0.3706,
      "step": 1280
    },
    {
      "epoch": 4.423117935593943,
      "grad_norm": 1.7254172563552856,
      "learning_rate": 9.306944444444446e-06,
      "loss": 0.381,
      "step": 1300
    },
    {
      "epoch": 4.491362763915547,
      "grad_norm": 1.760470986366272,
      "learning_rate": 9.279166666666667e-06,
      "loss": 0.3798,
      "step": 1320
    },
    {
      "epoch": 4.559607592237151,
      "grad_norm": 1.6931596994400024,
      "learning_rate": 9.251388888888889e-06,
      "loss": 0.3644,
      "step": 1340
    },
    {
      "epoch": 4.627852420558755,
      "grad_norm": 2.2630832195281982,
      "learning_rate": 9.223611111111112e-06,
      "loss": 0.3837,
      "step": 1360
    },
    {
      "epoch": 4.696097248880358,
      "grad_norm": 1.468401551246643,
      "learning_rate": 9.195833333333334e-06,
      "loss": 0.3813,
      "step": 1380
    },
    {
      "epoch": 4.764342077201962,
      "grad_norm": 1.9099924564361572,
      "learning_rate": 9.168055555555556e-06,
      "loss": 0.376,
      "step": 1400
    },
    {
      "epoch": 4.8325869055235655,
      "grad_norm": 2.038067102432251,
      "learning_rate": 9.140277777777778e-06,
      "loss": 0.3762,
      "step": 1420
    },
    {
      "epoch": 4.90083173384517,
      "grad_norm": 1.3601075410842896,
      "learning_rate": 9.112500000000001e-06,
      "loss": 0.3875,
      "step": 1440
    },
    {
      "epoch": 4.969076562166773,
      "grad_norm": 1.5320748090744019,
      "learning_rate": 9.084722222222222e-06,
      "loss": 0.378,
      "step": 1460
    },
    {
      "epoch": 5.034122414160802,
      "grad_norm": 2.4824743270874023,
      "learning_rate": 9.056944444444445e-06,
      "loss": 0.3751,
      "step": 1480
    },
    {
      "epoch": 5.102367242482406,
      "grad_norm": 1.6322612762451172,
      "learning_rate": 9.029166666666668e-06,
      "loss": 0.3687,
      "step": 1500
    },
    {
      "epoch": 5.102367242482406,
      "eval_accuracy": 0.8299149574787393,
      "eval_loss": 0.38590145111083984,
      "eval_macro_f1": 0.8051227881840745,
      "eval_macro_precision": 0.8088403560279902,
      "eval_macro_recall": 0.8018473529365063,
      "eval_runtime": 0.8085,
      "eval_samples_per_second": 2472.396,
      "eval_steps_per_second": 77.919,
      "step": 1500
    },
    {
      "epoch": 5.170612070804009,
      "grad_norm": 2.1137516498565674,
      "learning_rate": 9.00138888888889e-06,
      "loss": 0.3584,
      "step": 1520
    },
    {
      "epoch": 5.238856899125613,
      "grad_norm": 2.0885958671569824,
      "learning_rate": 8.973611111111112e-06,
      "loss": 0.3617,
      "step": 1540
    },
    {
      "epoch": 5.3071017274472165,
      "grad_norm": 1.981590986251831,
      "learning_rate": 8.945833333333333e-06,
      "loss": 0.3607,
      "step": 1560
    },
    {
      "epoch": 5.375346555768821,
      "grad_norm": 1.575046181678772,
      "learning_rate": 8.918055555555556e-06,
      "loss": 0.3582,
      "step": 1580
    },
    {
      "epoch": 5.443591384090425,
      "grad_norm": 2.2486586570739746,
      "learning_rate": 8.890277777777777e-06,
      "loss": 0.363,
      "step": 1600
    },
    {
      "epoch": 5.511836212412028,
      "grad_norm": 2.057607412338257,
      "learning_rate": 8.8625e-06,
      "loss": 0.3618,
      "step": 1620
    },
    {
      "epoch": 5.580081040733631,
      "grad_norm": 1.5199756622314453,
      "learning_rate": 8.834722222222223e-06,
      "loss": 0.37,
      "step": 1640
    },
    {
      "epoch": 5.648325869055236,
      "grad_norm": 1.7663795948028564,
      "learning_rate": 8.806944444444446e-06,
      "loss": 0.3665,
      "step": 1660
    },
    {
      "epoch": 5.71657069737684,
      "grad_norm": 1.7566401958465576,
      "learning_rate": 8.779166666666667e-06,
      "loss": 0.3631,
      "step": 1680
    },
    {
      "epoch": 5.784815525698443,
      "grad_norm": 1.8996009826660156,
      "learning_rate": 8.75138888888889e-06,
      "loss": 0.3597,
      "step": 1700
    },
    {
      "epoch": 5.853060354020047,
      "grad_norm": 2.099503517150879,
      "learning_rate": 8.723611111111113e-06,
      "loss": 0.3582,
      "step": 1720
    },
    {
      "epoch": 5.9213051823416505,
      "grad_norm": 1.690809726715088,
      "learning_rate": 8.695833333333334e-06,
      "loss": 0.3587,
      "step": 1740
    },
    {
      "epoch": 5.989550010663255,
      "grad_norm": 1.6150052547454834,
      "learning_rate": 8.668055555555557e-06,
      "loss": 0.3563,
      "step": 1760
    },
    {
      "epoch": 6.054595862657283,
      "grad_norm": 2.4723961353302,
      "learning_rate": 8.640277777777778e-06,
      "loss": 0.337,
      "step": 1780
    },
    {
      "epoch": 6.122840690978887,
      "grad_norm": 1.8031606674194336,
      "learning_rate": 8.612500000000001e-06,
      "loss": 0.345,
      "step": 1800
    },
    {
      "epoch": 6.191085519300491,
      "grad_norm": 2.4128880500793457,
      "learning_rate": 8.584722222222222e-06,
      "loss": 0.3463,
      "step": 1820
    },
    {
      "epoch": 6.259330347622094,
      "grad_norm": 1.9069716930389404,
      "learning_rate": 8.556944444444445e-06,
      "loss": 0.3447,
      "step": 1840
    },
    {
      "epoch": 6.327575175943698,
      "grad_norm": 2.0255472660064697,
      "learning_rate": 8.529166666666668e-06,
      "loss": 0.3477,
      "step": 1860
    },
    {
      "epoch": 6.3958200042653015,
      "grad_norm": 2.237671375274658,
      "learning_rate": 8.501388888888889e-06,
      "loss": 0.3499,
      "step": 1880
    },
    {
      "epoch": 6.464064832586906,
      "grad_norm": 1.9280164241790771,
      "learning_rate": 8.473611111111112e-06,
      "loss": 0.3475,
      "step": 1900
    },
    {
      "epoch": 6.532309660908509,
      "grad_norm": 2.225080728530884,
      "learning_rate": 8.445833333333335e-06,
      "loss": 0.3443,
      "step": 1920
    },
    {
      "epoch": 6.600554489230113,
      "grad_norm": 2.4958479404449463,
      "learning_rate": 8.418055555555556e-06,
      "loss": 0.3355,
      "step": 1940
    },
    {
      "epoch": 6.668799317551716,
      "grad_norm": 1.876584768295288,
      "learning_rate": 8.390277777777779e-06,
      "loss": 0.3504,
      "step": 1960
    },
    {
      "epoch": 6.737044145873321,
      "grad_norm": 2.3804609775543213,
      "learning_rate": 8.362500000000002e-06,
      "loss": 0.3478,
      "step": 1980
    },
    {
      "epoch": 6.805288974194925,
      "grad_norm": 2.1689975261688232,
      "learning_rate": 8.334722222222223e-06,
      "loss": 0.3363,
      "step": 2000
    },
    {
      "epoch": 6.805288974194925,
      "eval_accuracy": 0.8389194597298649,
      "eval_loss": 0.3806750774383545,
      "eval_macro_f1": 0.8117856558048142,
      "eval_macro_precision": 0.8237382620577376,
      "eval_macro_recall": 0.8031153665557438,
      "eval_runtime": 0.8177,
      "eval_samples_per_second": 2444.596,
      "eval_steps_per_second": 77.043,
      "step": 2000
    },
    {
      "epoch": 6.873533802516528,
      "grad_norm": 1.903191328048706,
      "learning_rate": 8.306944444444444e-06,
      "loss": 0.3398,
      "step": 2020
    },
    {
      "epoch": 6.941778630838132,
      "grad_norm": 3.0612809658050537,
      "learning_rate": 8.279166666666667e-06,
      "loss": 0.3525,
      "step": 2040
    },
    {
      "epoch": 7.00682448283216,
      "grad_norm": 1.7657408714294434,
      "learning_rate": 8.25138888888889e-06,
      "loss": 0.3449,
      "step": 2060
    },
    {
      "epoch": 7.075069311153764,
      "grad_norm": 1.9476031064987183,
      "learning_rate": 8.223611111111111e-06,
      "loss": 0.3329,
      "step": 2080
    },
    {
      "epoch": 7.143314139475368,
      "grad_norm": 2.2910470962524414,
      "learning_rate": 8.195833333333334e-06,
      "loss": 0.3227,
      "step": 2100
    },
    {
      "epoch": 7.211558967796972,
      "grad_norm": 1.9702738523483276,
      "learning_rate": 8.168055555555557e-06,
      "loss": 0.3303,
      "step": 2120
    },
    {
      "epoch": 7.279803796118576,
      "grad_norm": 1.9978903532028198,
      "learning_rate": 8.140277777777778e-06,
      "loss": 0.3307,
      "step": 2140
    },
    {
      "epoch": 7.348048624440179,
      "grad_norm": 2.56121826171875,
      "learning_rate": 8.1125e-06,
      "loss": 0.3284,
      "step": 2160
    },
    {
      "epoch": 7.416293452761783,
      "grad_norm": 2.1393775939941406,
      "learning_rate": 8.084722222222224e-06,
      "loss": 0.3279,
      "step": 2180
    },
    {
      "epoch": 7.4845382810833865,
      "grad_norm": 2.07291316986084,
      "learning_rate": 8.056944444444446e-06,
      "loss": 0.3285,
      "step": 2200
    },
    {
      "epoch": 7.552783109404991,
      "grad_norm": 2.357668399810791,
      "learning_rate": 8.029166666666668e-06,
      "loss": 0.3297,
      "step": 2220
    },
    {
      "epoch": 7.621027937726594,
      "grad_norm": 1.9861778020858765,
      "learning_rate": 8.001388888888889e-06,
      "loss": 0.3292,
      "step": 2240
    },
    {
      "epoch": 7.689272766048198,
      "grad_norm": 3.018803119659424,
      "learning_rate": 7.973611111111112e-06,
      "loss": 0.3342,
      "step": 2260
    },
    {
      "epoch": 7.757517594369801,
      "grad_norm": 3.280634641647339,
      "learning_rate": 7.945833333333334e-06,
      "loss": 0.3303,
      "step": 2280
    },
    {
      "epoch": 7.8257624226914055,
      "grad_norm": 2.2558510303497314,
      "learning_rate": 7.918055555555556e-06,
      "loss": 0.3332,
      "step": 2300
    },
    {
      "epoch": 7.894007251013009,
      "grad_norm": 1.8158224821090698,
      "learning_rate": 7.890277777777779e-06,
      "loss": 0.3246,
      "step": 2320
    },
    {
      "epoch": 7.962252079334613,
      "grad_norm": 2.13370418548584,
      "learning_rate": 7.862500000000001e-06,
      "loss": 0.3319,
      "step": 2340
    },
    {
      "epoch": 8.027297931328642,
      "grad_norm": 2.405484676361084,
      "learning_rate": 7.834722222222223e-06,
      "loss": 0.3242,
      "step": 2360
    },
    {
      "epoch": 8.095542759650245,
      "grad_norm": 2.2277023792266846,
      "learning_rate": 7.806944444444445e-06,
      "loss": 0.3103,
      "step": 2380
    },
    {
      "epoch": 8.163787587971848,
      "grad_norm": 1.8672759532928467,
      "learning_rate": 7.779166666666668e-06,
      "loss": 0.3126,
      "step": 2400
    },
    {
      "epoch": 8.232032416293453,
      "grad_norm": 1.914949655532837,
      "learning_rate": 7.75138888888889e-06,
      "loss": 0.3144,
      "step": 2420
    },
    {
      "epoch": 8.300277244615057,
      "grad_norm": 2.8184378147125244,
      "learning_rate": 7.723611111111112e-06,
      "loss": 0.3164,
      "step": 2440
    },
    {
      "epoch": 8.36852207293666,
      "grad_norm": 2.1592323780059814,
      "learning_rate": 7.695833333333333e-06,
      "loss": 0.3194,
      "step": 2460
    },
    {
      "epoch": 8.436766901258263,
      "grad_norm": 2.0609471797943115,
      "learning_rate": 7.668055555555556e-06,
      "loss": 0.3173,
      "step": 2480
    },
    {
      "epoch": 8.505011729579868,
      "grad_norm": 2.183847427368164,
      "learning_rate": 7.640277777777778e-06,
      "loss": 0.3145,
      "step": 2500
    },
    {
      "epoch": 8.505011729579868,
      "eval_accuracy": 0.8339169584792396,
      "eval_loss": 0.38577815890312195,
      "eval_macro_f1": 0.8119660734968461,
      "eval_macro_precision": 0.8117257038026862,
      "eval_macro_recall": 0.8122086574253653,
      "eval_runtime": 0.824,
      "eval_samples_per_second": 2426.086,
      "eval_steps_per_second": 76.46,
      "step": 2500
    },
    {
      "epoch": 8.573256557901471,
      "grad_norm": 2.176070213317871,
      "learning_rate": 7.6125e-06,
      "loss": 0.3133,
      "step": 2520
    },
    {
      "epoch": 8.641501386223075,
      "grad_norm": 2.2395756244659424,
      "learning_rate": 7.584722222222222e-06,
      "loss": 0.3112,
      "step": 2540
    },
    {
      "epoch": 8.70974621454468,
      "grad_norm": 2.3942174911499023,
      "learning_rate": 7.556944444444445e-06,
      "loss": 0.3122,
      "step": 2560
    },
    {
      "epoch": 8.777991042866283,
      "grad_norm": 2.081547260284424,
      "learning_rate": 7.529166666666667e-06,
      "loss": 0.3247,
      "step": 2580
    },
    {
      "epoch": 8.846235871187886,
      "grad_norm": 1.885301947593689,
      "learning_rate": 7.501388888888889e-06,
      "loss": 0.3141,
      "step": 2600
    },
    {
      "epoch": 8.91448069950949,
      "grad_norm": 2.0867509841918945,
      "learning_rate": 7.473611111111112e-06,
      "loss": 0.3188,
      "step": 2620
    },
    {
      "epoch": 8.982725527831095,
      "grad_norm": 2.4143738746643066,
      "learning_rate": 7.445833333333334e-06,
      "loss": 0.3218,
      "step": 2640
    },
    {
      "epoch": 9.047771379825123,
      "grad_norm": 1.9364920854568481,
      "learning_rate": 7.418055555555557e-06,
      "loss": 0.2995,
      "step": 2660
    },
    {
      "epoch": 9.116016208146727,
      "grad_norm": 2.229085683822632,
      "learning_rate": 7.390277777777778e-06,
      "loss": 0.3051,
      "step": 2680
    },
    {
      "epoch": 9.18426103646833,
      "grad_norm": 2.030240535736084,
      "learning_rate": 7.3625e-06,
      "loss": 0.3007,
      "step": 2700
    },
    {
      "epoch": 9.252505864789933,
      "grad_norm": 3.1082980632781982,
      "learning_rate": 7.334722222222222e-06,
      "loss": 0.2956,
      "step": 2720
    },
    {
      "epoch": 9.320750693111538,
      "grad_norm": 2.7744617462158203,
      "learning_rate": 7.306944444444445e-06,
      "loss": 0.2977,
      "step": 2740
    },
    {
      "epoch": 9.388995521433142,
      "grad_norm": 2.866006374359131,
      "learning_rate": 7.279166666666667e-06,
      "loss": 0.3055,
      "step": 2760
    },
    {
      "epoch": 9.457240349754745,
      "grad_norm": 2.2238340377807617,
      "learning_rate": 7.251388888888889e-06,
      "loss": 0.2981,
      "step": 2780
    },
    {
      "epoch": 9.525485178076348,
      "grad_norm": 2.963663101196289,
      "learning_rate": 7.223611111111112e-06,
      "loss": 0.2951,
      "step": 2800
    },
    {
      "epoch": 9.593730006397953,
      "grad_norm": 2.2953853607177734,
      "learning_rate": 7.195833333333334e-06,
      "loss": 0.2947,
      "step": 2820
    },
    {
      "epoch": 9.661974834719556,
      "grad_norm": 1.9991017580032349,
      "learning_rate": 7.168055555555556e-06,
      "loss": 0.3039,
      "step": 2840
    },
    {
      "epoch": 9.73021966304116,
      "grad_norm": 2.151250123977661,
      "learning_rate": 7.140277777777779e-06,
      "loss": 0.3028,
      "step": 2860
    },
    {
      "epoch": 9.798464491362765,
      "grad_norm": 2.5875425338745117,
      "learning_rate": 7.112500000000001e-06,
      "loss": 0.306,
      "step": 2880
    },
    {
      "epoch": 9.866709319684368,
      "grad_norm": 2.8690295219421387,
      "learning_rate": 7.084722222222222e-06,
      "loss": 0.3042,
      "step": 2900
    },
    {
      "epoch": 9.934954148005971,
      "grad_norm": 2.000572919845581,
      "learning_rate": 7.056944444444445e-06,
      "loss": 0.3042,
      "step": 2920
    },
    {
      "epoch": 10.0,
      "grad_norm": 9.982894897460938,
      "learning_rate": 7.029166666666667e-06,
      "loss": 0.3027,
      "step": 2940
    },
    {
      "epoch": 10.068244828321603,
      "grad_norm": 2.1492815017700195,
      "learning_rate": 7.001388888888889e-06,
      "loss": 0.2928,
      "step": 2960
    },
    {
      "epoch": 10.136489656643208,
      "grad_norm": 2.299164295196533,
      "learning_rate": 6.973611111111112e-06,
      "loss": 0.2842,
      "step": 2980
    },
    {
      "epoch": 10.204734484964812,
      "grad_norm": 1.9375745058059692,
      "learning_rate": 6.945833333333334e-06,
      "loss": 0.2914,
      "step": 3000
    },
    {
      "epoch": 10.204734484964812,
      "eval_accuracy": 0.8434217108554277,
      "eval_loss": 0.3856607973575592,
      "eval_macro_f1": 0.8222430119483475,
      "eval_macro_precision": 0.8228817959473678,
      "eval_macro_recall": 0.8216183846096264,
      "eval_runtime": 0.8129,
      "eval_samples_per_second": 2459.022,
      "eval_steps_per_second": 77.498,
      "step": 3000
    }
  ],
  "logging_steps": 20,
  "max_steps": 8000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 28,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.174793852548447e+17,
  "train_batch_size": 64,
  "trial_name": null,
  "trial_params": null
}
